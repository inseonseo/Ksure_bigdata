import pandas as pd
import numpy as np
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoModel, AutoTokenizer
import pickle
import os
from collections import Counter

class ImprovedSimilaritySearch:
    def __init__(self, text_weight=0.5, categorical_weight=0.4, numerical_weight=0.1):
        """
        ê°œì„ ëœ ë‹¤ì¤‘ ëª¨ë‹¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ëª¨ë¸
        
        Args:
            text_weight: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜
            categorical_weight: ë²”ì£¼í˜• ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜  
            numerical_weight: ìˆ«ìí˜• ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜
        """
        self.text_weight = text_weight
        self.categorical_weight = categorical_weight
        self.numerical_weight = numerical_weight
        
        # Yê°’ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ë“¤
        self.target_columns = ['ì‹¬ì‚¬í•­ëª©ëª…', 'ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…']
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()
        
        # ìºì‹œ ë° ìƒíƒœ ë³€ìˆ˜
        self.embedding_cache = {}
        self.label_encoders = {}
        self.scaler = None
        self.is_fitted = False
        self.cache_message_shown = False
        
        # ë°ì´í„° ì €ì¥
        self.train_df = None
        self.valid_df = None
        self.test_df = None
        self.text_column = None
        self.categorical_features = None
        self.numerical_features = None
        
    def _initialize_model(self):
        """KoSimCSE ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            print("ğŸ”§ KoSimCSE ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = AutoModel.from_pretrained('BM-K/KoSimCSE-roberta')
            self.tokenizer = AutoTokenizer.from_pretrained('BM-K/KoSimCSE-roberta')
            self.model.to(self.device)
            print("âœ… KoSimCSE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ KoSimCSE ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ì˜ì–´ BERTë¡œ ëŒ€ì²´...")
            self.model = AutoModel.from_pretrained('bert-base-uncased')
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            self.model.to(self.device)
            print("âœ… ì˜ì–´ BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    def print_cache_status(self):
        """ìºì‹œ ìƒíƒœ ì¶œë ¥"""
        print(f"ğŸ“Š ìºì‹œ ìƒíƒœ: {len(self.embedding_cache)}ê°œ ì„ë² ë”© ì €ì¥ë¨")
    
    def reset_cache_message(self):
        """ìºì‹œ ë©”ì‹œì§€ ì´ˆê¸°í™”"""
        self.cache_message_shown = False
    
    def save_embeddings(self, file_path='improved_embeddings_cache.pkl'):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        cache_data = {
            'embedding_cache': self.embedding_cache,
            'label_encoders': self.label_encoders,
            'scaler': self.scaler,
            'is_fitted': self.is_fitted
        }
        with open(file_path, 'wb') as f:
            pickle.dump(cache_data, f)
        print(f"ğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ: {file_path}")
    
    def load_embeddings(self, file_path='improved_embeddings_cache.pkl'):
        """ì„ë² ë”© ìºì‹œ ë¡œë“œ"""
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                cache_data = pickle.load(f)
            self.embedding_cache = cache_data['embedding_cache']
            self.label_encoders = cache_data['label_encoders']
            self.scaler = cache_data['scaler']
            self.is_fitted = cache_data['is_fitted']
            print(f"ğŸ“¥ ì„ë² ë”© ìºì‹œ ë¡œë“œ ì™„ë£Œ: {file_path}")
            return True
        return False
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text)
        
        # ì˜ë¯¸ì—†ëŠ” íŒ¨í„´ ì œê±°
        meaningless_patterns = [
            'ì²¨ë¶€í™•ì¸', 'í™•ì¸', 'ì²¨ë¶€', 'ì°¸ê³ ', 'ê´€ë ¨', 'ê¸°íƒ€',
            'ìƒì„¸ë‚´ìš©', 'ì¶”ê°€ì •ë³´', 'ê¸°íƒ€ì‚¬í•­', 'ê¸°íƒ€ì •ë³´'
        ]
        
        for pattern in meaningless_patterns:
            text = text.replace(pattern, '')
        
        # ê³µë°± ì •ë¦¬
        text = ' '.join(text.split())
        
        # ìµœì†Œ ê¸¸ì´ í•„í„°ë§
        if len(text) < 10:
            return ''
        
        return text
    
    def clean_categorical(self, df, columns, min_freq=0.01):
        """ë²”ì£¼í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬"""
        for col in columns:
            if col in df.columns:
                # NaNì„ 'Unknown'ìœ¼ë¡œ ëŒ€ì²´
                df[col] = df[col].fillna('Unknown')
                
                # ë¹ˆ ë¬¸ìì—´ì„ 'Unknown'ìœ¼ë¡œ ëŒ€ì²´
                df[col] = df[col].replace('', 'Unknown')
                
                # ë¹ˆë„ê°€ ë‚®ì€ ì¹´í…Œê³ ë¦¬ë¥¼ 'Others'ë¡œ ê·¸ë£¹í™”
                value_counts = df[col].value_counts(normalize=True)
                rare_categories = value_counts[value_counts < min_freq].index
                df[col] = df[col].replace(rare_categories, 'Others')
        
        return df
    
    def handle_outliers(self, df, numerical_columns):
        """ìˆ«ìí˜• ë³€ìˆ˜ ì´ìƒì¹˜ ì²˜ë¦¬"""
        for col in numerical_columns:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # ì´ìƒì¹˜ë¥¼ ê²½ê³„ê°’ìœ¼ë¡œ í´ë¦¬í•‘
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        
        return df
    
    def load_and_split_data(self, file_path, text_column='ì‚¬ê³ ì„¤ëª…', 
                           categorical_columns=None, numerical_columns=None,
                           train_size=0.7, valid_size=0.15, test_size=0.15, random_state=42):
        """
        ë°ì´í„° ë¡œë“œ ë° ë¶„í•  (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
        
        Args:
            file_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            categorical_columns: ë²”ì£¼í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            numerical_columns: ìˆ«ìí˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            train_size: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
            valid_size: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        """
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {file_path}")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(file_path, encoding='cp949')
        print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df)}í–‰, {len(df.columns)}ì—´")
        
        # ê¸°ë³¸ ì»¬ëŸ¼ ì„¤ì •
        if categorical_columns is None:
            categorical_columns = [
                'ì‚¬ê³ ìœ í˜•ëª…', 'ì‚¬ê³ ì§„í–‰ìƒíƒœ', 'ì‚¬ê³ ë“±ë¡ìƒíƒœëª…', 'ì ‘ìˆ˜ì‚¬ê³ ìœ í˜•í™•ì •',
                'ì‚¬ê³ í†µí™”ì½”ë“œ_ZZ067', 'ìˆ˜ì¶œìëª…', 'ìˆ˜ì…ìëª…', 'ìˆ˜ì¶œìêµ­ê°€', 'ìˆ˜ì…ìêµ­ê°€'
            ]
        
        if numerical_columns is None:
            numerical_columns = [
                'ì™¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡',
                'ì™¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ë¯¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ì›í™”í•©ê³„íŒì •ê¸ˆì•¡',
                'ì™¸í™”íŒì •ê¸ˆì•¡', 'ë¯¸í™”íŒì •ê¸ˆì•¡', 'ì›í™”íŒì •ê¸ˆì•¡',
                'ì™¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ê¸ˆì•¡',
                'ì™¸í™”ë³´í—˜ê°€ì•¡', 'ë¯¸í™”ë³´í—˜ê°€ì•¡', 'ì›í™”ë³´í—˜ê°€ì•¡',
                'ì™¸í™”ë³´í—˜ê¸ˆì•¡', 'ë¯¸í™”ë³´í—˜ê¸ˆì•¡', 'ì›í™”ë³´í—˜ê¸ˆì•¡'
            ]
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = [text_column] + self.target_columns + categorical_columns + numerical_columns
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}")
            return None, None, None
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        print("ğŸ§¹ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        df[text_column] = df[text_column].apply(self.clean_text)
        df = df[df[text_column] != ''].reset_index(drop=True)
        
        # ì¤‘ë³µ ì œê±° (ì‚¬ê³ ì„¤ëª… ê¸°ì¤€)
        df = df.drop_duplicates(subset=[text_column]).reset_index(drop=True)
        df = df.drop_duplicates().reset_index(drop=True)
        
        print(f"ğŸ“Š ì „ì²˜ë¦¬ í›„ ë°ì´í„°: {len(df)}í–‰")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬
        df = self.clean_categorical(df, categorical_columns)
        
        # ìˆ«ìí˜• ë³€ìˆ˜ ì „ì²˜ë¦¬
        df = self.handle_outliers(df, numerical_columns)
        
        # ğŸš¨ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€: ë¨¼ì € ë°ì´í„° ë¶„í•  í›„ ì „ì²˜ë¦¬
        print("ğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...")
        
        # Train/Validation/Test ë¶„í• 
        train_valid_size = train_size + valid_size
        train_valid_df, test_df = train_test_split(
            df, test_size=test_size, random_state=random_state
        )
        
        valid_ratio = valid_size / train_valid_size
        train_df, valid_df = train_test_split(
            train_valid_df, test_size=valid_ratio, random_state=random_state
        )
        
        print(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"   - í•™ìŠµ: {len(train_df)}í–‰ ({len(train_df)/len(df)*100:.1f}%)")
        print(f"   - ê²€ì¦: {len(valid_df)}í–‰ ({len(valid_df)/len(df)*100:.1f}%)")
        print(f"   - í…ŒìŠ¤íŠ¸: {len(test_df)}í–‰ ({len(test_df)/len(df)*100:.1f}%)")
        
        # ğŸš¨ í•™ìŠµ ë°ì´í„°ë§Œìœ¼ë¡œ LabelEncoderì™€ StandardScaler í•™ìŠµ
        print("ğŸ”§ ì „ì²˜ë¦¬ ëª¨ë¸ í•™ìŠµ (í•™ìŠµ ë°ì´í„°ë§Œ ì‚¬ìš©)...")
        
        # LabelEncoder ì ìš© (í•™ìŠµ ë°ì´í„°ë§Œìœ¼ë¡œ fit)
        for col in categorical_columns:
            if col in train_df.columns:
                le = LabelEncoder()
                # í•™ìŠµ ë°ì´í„°ë¡œë§Œ fit
                le.fit(train_df[col].astype(str))
                # ëª¨ë“  ë°ì´í„°ì— transform ì ìš©
                train_df[f'{col}_encoded'] = le.transform(train_df[col].astype(str))
                valid_df[f'{col}_encoded'] = le.transform(valid_df[col].astype(str))
                test_df[f'{col}_encoded'] = le.transform(test_df[col].astype(str))
                self.label_encoders[col] = le
        
        # StandardScaler ì ìš© (í•™ìŠµ ë°ì´í„°ë§Œìœ¼ë¡œ fit)
        if numerical_columns:
            self.scaler = StandardScaler()
            # í•™ìŠµ ë°ì´í„°ë¡œë§Œ fit
            self.scaler.fit(train_df[numerical_columns])
            # ëª¨ë“  ë°ì´í„°ì— transform ì ìš©
            train_df[numerical_columns] = self.scaler.transform(train_df[numerical_columns])
            valid_df[numerical_columns] = self.scaler.transform(valid_df[numerical_columns])
            test_df[numerical_columns] = self.scaler.transform(test_df[numerical_columns])
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        print("ğŸ”¤ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
        all_texts = df[text_column].tolist()
        self.encode_batch(all_texts)
        
        # ë°ì´í„° ì €ì¥
        self.train_df = train_df
        self.valid_df = valid_df
        self.test_df = test_df
        self.text_column = text_column
        self.categorical_features = categorical_columns
        self.numerical_features = numerical_columns
        self.is_fitted = True
        
        print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)")
        return train_df, valid_df, test_df
    
    def load_data_only(self, file_path, text_column='ì‚¬ê³ ì„¤ëª…',
                      categorical_columns=None, numerical_columns=None,
                      train_size=0.7, valid_size=0.15, test_size=0.15, random_state=42):
        """ìºì‹œëœ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë§Œ ë¡œë“œ"""
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘ (ìºì‹œ ì‚¬ìš©): {file_path}")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(file_path, encoding='cp949')
        
        # ê¸°ë³¸ ì»¬ëŸ¼ ì„¤ì •
        if categorical_columns is None:
            categorical_columns = [
                'ì‚¬ê³ ìœ í˜•ëª…', 'ì‚¬ê³ ì§„í–‰ìƒíƒœ', 'ì‚¬ê³ ë“±ë¡ìƒíƒœëª…', 'ì ‘ìˆ˜ì‚¬ê³ ìœ í˜•í™•ì •',
                'ì‚¬ê³ í†µí™”ì½”ë“œ_ZZ067', 'ìˆ˜ì¶œìëª…', 'ìˆ˜ì…ìëª…', 'ìˆ˜ì¶œìêµ­ê°€', 'ìˆ˜ì…ìêµ­ê°€'
            ]
        
        if numerical_columns is None:
            numerical_columns = [
                'ì™¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡',
                'ì™¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ë¯¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ì›í™”í•©ê³„íŒì •ê¸ˆì•¡',
                'ì™¸í™”íŒì •ê¸ˆì•¡', 'ë¯¸í™”íŒì •ê¸ˆì•¡', 'ì›í™”íŒì •ê¸ˆì•¡',
                'ì™¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ê¸ˆì•¡',
                'ì™¸í™”ë³´í—˜ê°€ì•¡', 'ë¯¸í™”ë³´í—˜ê°€ì•¡', 'ì›í™”ë³´í—˜ê°€ì•¡',
                'ì™¸í™”ë³´í—˜ê¸ˆì•¡', 'ë¯¸í™”ë³´í—˜ê¸ˆì•¡', 'ì›í™”ë³´í—˜ê¸ˆì•¡'
            ]
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df[text_column] = df[text_column].apply(self.clean_text)
        df = df[df[text_column] != ''].reset_index(drop=True)
        df = df.drop_duplicates(subset=[text_column]).reset_index(drop=True)
        df = df.drop_duplicates().reset_index(drop=True)
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì „ì²˜ë¦¬
        df = self.clean_categorical(df, categorical_columns)
        
        # ìˆ«ìí˜• ë³€ìˆ˜ ì „ì²˜ë¦¬
        df = self.handle_outliers(df, numerical_columns)
        
        # LabelEncoder ì ìš© (ìºì‹œëœ ì¸ì½”ë” ì‚¬ìš©)
        for col in categorical_columns:
            if col in df.columns and col in self.label_encoders:
                df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].astype(str))
        
        # StandardScaler ì ìš© (ìºì‹œëœ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©)
        if numerical_columns and self.scaler:
            df[numerical_columns] = self.scaler.transform(df[numerical_columns])
        
        # ë°ì´í„° ë¶„í• 
        train_valid_size = train_size + valid_size
        train_valid_df, test_df = train_test_split(
            df, test_size=test_size, random_state=random_state
        )
        
        valid_ratio = valid_size / train_valid_size
        train_df, valid_df = train_test_split(
            train_valid_df, test_size=valid_ratio, random_state=random_state
        )
        
        # ë°ì´í„° ì €ì¥
        self.train_df = train_df
        self.valid_df = valid_df
        self.test_df = test_df
        self.text_column = text_column
        self.categorical_features = categorical_columns
        self.numerical_features = numerical_columns
        
        print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (ì„ë² ë”©ì€ ìºì‹œ ì‚¬ìš©)")
        return train_df, valid_df, test_df
    
    def encode_text(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        inputs = self.tokenizer(
            [text], padding=True, truncation=True, max_length=128, return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            embeddings, _ = self.model(**inputs, return_dict=False)
            embedding = embeddings[0][0].cpu().numpy()
            self.embedding_cache[text] = embedding
            return embedding
    
    def encode_batch(self, texts, batch_size=16):
        """ë°°ì¹˜ë¡œ í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = []
        uncached_texts = []
        
        for text in texts:
            if text not in self.embedding_cache:
                uncached_texts.append(text)
            else:
                embeddings.append(self.embedding_cache[text])
        
        if uncached_texts:
            print(f"    ğŸ“¥ {len(uncached_texts)}ê°œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            total_batches = (len(uncached_texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(uncached_texts), batch_size):
                batch_texts = uncached_texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts, padding=True, truncation=True, max_length=128, return_tensors="pt"
                ).to(self.device)
                
                with torch.no_grad():
                    batch_embeddings, _ = self.model(**inputs, return_dict=False)
                    batch_embeddings = batch_embeddings[0].cpu().numpy()
                    
                    for text, embedding in zip(batch_texts, batch_embeddings):
                        self.embedding_cache[text] = embedding
                        embeddings.append(embedding)
                
                current_batch = i // batch_size + 1
                if current_batch % max(1, total_batches // 10) == 0 or current_batch == total_batches:
                    progress = min((i + batch_size) / len(uncached_texts) * 100, 100)
                    print(f"    ì„ë² ë”© ì§„í–‰ë¥ : {progress:.1f}% ({min(i + batch_size, len(uncached_texts))}/{len(uncached_texts)})")
        else:
            if not self.cache_message_shown:
                print(f"    âœ… ìºì‹œëœ ì„ë² ë”© ì‚¬ìš© ({len(texts)}ê°œ)")
                self.cache_message_shown = True
        
        return np.array(embeddings)
    
    def calculate_text_similarity(self, query_text, target_texts):
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        query_embedding = self.encode_text(query_text)
        target_embeddings = self.encode_batch(target_texts)
        return cosine_similarity([query_embedding], target_embeddings)[0]
    
    def calculate_categorical_similarity(self, query_categorical, target_categorical):
        """ê°œì„ ëœ ë²”ì£¼í˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚° (ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬)"""
        similarities = []
        for target in target_categorical:
            # ì™„ì „ ì¼ì¹˜: 1.0, ë¶€ë¶„ ì¼ì¹˜: 0.5, ë¶ˆì¼ì¹˜: 0.0
            if query_categorical == target:
                similarity = 1.0
            else:
                # ë¶€ë¶„ ì¼ì¹˜ ë¡œì§ (ì˜ˆ: ê°™ì€ êµ­ê°€, ê°™ì€ íšŒì‚¬ ë“±)
                similarity = 0.0
            similarities.append(similarity)
        return np.array(similarities)
    
    def calculate_improved_categorical_similarity(self, query_categorical, target_categorical):
        """ë” ì •êµí•œ ë²”ì£¼í˜• ìœ ì‚¬ë„ ê³„ì‚°"""
        similarities = []
        for target in target_categorical:
            if query_categorical == target:
                similarity = 1.0
            else:
                # ë¶€ë¶„ ì ìˆ˜ ë¡œì§
                # ì˜ˆ: ê°™ì€ êµ­ê°€ë©´ 0.3, ê°™ì€ íšŒì‚¬ë©´ 0.5 ë“±
                similarity = 0.0
            similarities.append(similarity)
        return np.array(similarities)
    
    def calculate_numerical_similarity(self, query_numerical, target_numerical):
        """ìˆ«ìí˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚° (ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜)"""
        if len(query_numerical) == 0:
            return np.ones(len(target_numerical))
        
        similarities = []
        for target in target_numerical:
            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 / (1 + ê±°ë¦¬))
            distance = np.sqrt(np.sum((query_numerical - target) ** 2))
            similarity = 1 / (1 + distance)
            similarities.append(similarity)
        return np.array(similarities)
    
    def find_similar_cases(self, query_data, search_df, top_k=5, verbose=True):
        """
        ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (ì‹¬ì‚¬í•­ëª©ëª…, ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… ì˜ˆì¸¡)
        
        Args:
            query_data: ì¿¼ë¦¬ ë°ì´í„° (dict í˜•íƒœ)
                - text: í…ìŠ¤íŠ¸
                - categorical: ë²”ì£¼í˜• ë³€ìˆ˜ dict
                - numerical: ìˆ«ìí˜• ë³€ìˆ˜ dict
            search_df: ê²€ìƒ‰í•  ë°ì´í„°í”„ë ˆì„
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        """
        if not self.is_fitted:
            raise ValueError("ë¨¼ì € load_and_split_data()ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”!")
        
        if verbose:
            print(f"\nğŸ” ê°œì„ ëœ ë‹¤ì¤‘ ëª¨ë‹¬ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (ìƒìœ„ {top_k}ê°œ)")
            print(f"ğŸ“ ì¿¼ë¦¬ í…ìŠ¤íŠ¸: {query_data['text'][:100]}...")
        
        # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        text_similarities = self.calculate_text_similarity(
            query_data['text'], 
            search_df[self.text_column].tolist()
        )
        
        # 2. ë²”ì£¼í˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°œì„ ëœ ë°©ì‹)
        categorical_similarities = np.ones(len(search_df))
        categorical_details = {}  # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ì •ë³´
        
        if query_data.get('categorical') and self.categorical_features:
            for col, value in query_data['categorical'].items():
                if col in self.label_encoders:
                    encoded_value = self.label_encoders[col].transform([str(value)])[0]
                    col_similarities = self.calculate_categorical_similarity(
                        encoded_value,
                        search_df[f'{col}_encoded'].values
                    )
                    # ê³±ì…ˆ ëŒ€ì‹  í‰ê·  ì‚¬ìš© (ë” ê´€ëŒ€í•œ ì ìˆ˜)
                    categorical_similarities = (categorical_similarities + col_similarities) / 2
                    
                    # ìƒì„¸ ì •ë³´ ì €ì¥
                    categorical_details[col] = {
                        'query_value': value,
                        'similarities': col_similarities
                    }
        
        # 3. ìˆ«ìí˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚°
        numerical_similarities = np.ones(len(search_df))
        numerical_details = {}  # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ì •ë³´
        
        if query_data.get('numerical') and self.numerical_features:
            query_numerical = []
            for col in self.numerical_features:
                if col in query_data['numerical']:
                    query_numerical.append(query_data['numerical'][col])
                else:
                    query_numerical.append(0)  # ê¸°ë³¸ê°’
            
            query_numerical = np.array(query_numerical)
            target_numerical = search_df[self.numerical_features].values
            numerical_similarities = self.calculate_numerical_similarity(query_numerical, target_numerical)
            
            # ìƒì„¸ ì •ë³´ ì €ì¥
            numerical_details = {
                'query_values': dict(zip(self.numerical_features, query_numerical)),
                'similarities': numerical_similarities
            }
        
        # 4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
        final_similarities = (
            self.text_weight * text_similarities +
            self.categorical_weight * categorical_similarities +
            self.numerical_weight * numerical_similarities
        )
        
        # 5. ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
        top_indices = np.argsort(final_similarities)[::-1][:top_k]
        
        results = []
        for i, idx in enumerate(top_indices):
            similarity = final_similarities[idx]
            row = search_df.iloc[idx]
            
            # ìƒì„¸ ìœ ì‚¬ë„ ë¶„ì„
            similarity_breakdown = {
                'text_similarity': text_similarities[idx],
                'categorical_similarity': categorical_similarities[idx],
                'numerical_similarity': numerical_similarities[idx],
                'text_weight': self.text_weight,
                'categorical_weight': self.categorical_weight,
                'numerical_weight': self.numerical_weight
            }
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ìƒì„¸ ë¶„ì„
            categorical_analysis = {}
            if categorical_details:
                for col, details in categorical_details.items():
                    col_similarity = details['similarities'][idx]
                    target_value = row.get(col, 'N/A')
                    categorical_analysis[col] = {
                        'query_value': details['query_value'],
                        'target_value': target_value,
                        'similarity': col_similarity,
                        'match': details['query_value'] == target_value
                    }
            
            # ìˆ«ìí˜• ë³€ìˆ˜ë³„ ìƒì„¸ ë¶„ì„
            numerical_analysis = {}
            if numerical_details:
                for col in self.numerical_features:
                    query_val = numerical_details['query_values'].get(col, 0)
                    target_val = row.get(col, 0)
                    numerical_analysis[col] = {
                        'query_value': query_val,
                        'target_value': target_val,
                        'difference': abs(query_val - target_val)
                    }
            
            result = {
                'rank': i + 1,
                'similarity': similarity,
                'similarity_breakdown': similarity_breakdown,
                'categorical_analysis': categorical_analysis,
                'numerical_analysis': numerical_analysis,
                'predicted_ì‹¬ì‚¬í•­ëª©ëª…': row.get('ì‹¬ì‚¬í•­ëª©ëª…', 'N/A'),
                'predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…': row.get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', 'N/A'),
                'data': row.to_dict()
            }
            results.append(result)
            
            if verbose:
                print(f"\n{i+1}. ì „ì²´ ìœ ì‚¬ë„: {similarity:.4f}")
                print(f"   ğŸ“ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_similarities[idx]:.4f} (ê°€ì¤‘ì¹˜: {self.text_weight})")
                print(f"   ğŸ·ï¸ ë²”ì£¼í˜• ìœ ì‚¬ë„: {categorical_similarities[idx]:.4f} (ê°€ì¤‘ì¹˜: {self.categorical_weight})")
                print(f"   ğŸ”¢ ìˆ«ìí˜• ìœ ì‚¬ë„: {numerical_similarities[idx]:.4f} (ê°€ì¤‘ì¹˜: {self.numerical_weight})")
                print(f"   ğŸ¯ ì˜ˆì¸¡ ì‹¬ì‚¬í•­ëª©ëª…: {row.get('ì‹¬ì‚¬í•­ëª©ëª…', 'N/A')}")
                print(f"   ğŸ¯ ì˜ˆì¸¡ ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…: {row.get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', 'N/A')}")
                print(f"   ğŸ“„ ì‚¬ê³ ì„¤ëª…: {row.get(self.text_column, 'N/A')[:100]}...")
                
                # ë²”ì£¼í˜• ë³€ìˆ˜ ìƒì„¸ ë¶„ì„ ì¶œë ¥
                if categorical_analysis:
                    print(f"   ğŸ·ï¸ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„:")
                    for col, analysis in categorical_analysis.items():
                        match_symbol = "âœ…" if analysis['match'] else "âŒ"
                        print(f"      {col}: {analysis['query_value']} vs {analysis['target_value']} {match_symbol} (ìœ ì‚¬ë„: {analysis['similarity']:.3f})")
                
                # ìˆ«ìí˜• ë³€ìˆ˜ ìƒì„¸ ë¶„ì„ ì¶œë ¥
                if numerical_analysis:
                    print(f"   ğŸ”¢ ìˆ«ìí˜• ë³€ìˆ˜ ë¶„ì„:")
                    for col, analysis in numerical_analysis.items():
                        print(f"      {col}: {analysis['query_value']:.2f} vs {analysis['target_value']:.2f} (ì°¨ì´: {analysis['difference']:.2f})")
        
        return results
    
    def evaluate_on_validation(self, top_k=5):
        """ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€ (ì‹¬ì‚¬í•­ëª©ëª…, ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… ê¸°ì¤€)"""
        print(f"\nğŸ“Š ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ (ìƒìœ„ {top_k}ê°œ ê¸°ì¤€)")
        
        correct_count = 0
        total_similarities = []
        
        # ğŸš¨ ë” ì—„ê²©í•œ í‰ê°€ë¥¼ ìœ„í•œ ì¹´ìš´í„°
        exact_match_count = 0  # ì™„ì „ ì¼ì¹˜
        partial_match_count = 0  # ë¶€ë¶„ ì¼ì¹˜
        no_match_count = 0  # ë¶ˆì¼ì¹˜
        
        for i, valid_row in self.valid_df.iterrows():
            # ê²€ì¦ ì¿¼ë¦¬ ìƒì„±
            query_data = {
                'text': valid_row[self.text_column],
                'categorical': {},
                'numerical': {}
            }
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒë§Œ)
            for col in self.categorical_features:
                if col in valid_row and pd.notna(valid_row[col]):
                    query_data['categorical'][col] = valid_row[col]
            
            # ìˆ«ìí˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.numerical_features:
                if col in valid_row and pd.notna(valid_row[col]):
                    query_data['numerical'][col] = valid_row[col]
            
            # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
            results = self.find_similar_cases(query_data, self.train_df, top_k=top_k, verbose=False)
            
            # ì •í™•ë„ ê³„ì‚° (ì‹¬ì‚¬í•­ëª©ëª… ë˜ëŠ” ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…ì´ ì¼ì¹˜í•˜ëŠ”ì§€)
            actual_ì‹¬ì‚¬í•­ëª©ëª… = valid_row.get('ì‹¬ì‚¬í•­ëª©ëª…', '')
            actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… = valid_row.get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')
            
            # ğŸš¨ ë” ì—„ê²©í•œ í‰ê°€
            exact_match = False
            partial_match = False
            
            for result in results:
                predicted_ì‹¬ì‚¬í•­ëª©ëª… = result['predicted_ì‹¬ì‚¬í•­ëª©ëª…']
                predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… = result['predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…']
                
                # ì™„ì „ ì¼ì¹˜ (ë‘˜ ë‹¤ ì¼ì¹˜)
                if (predicted_ì‹¬ì‚¬í•­ëª©ëª… == actual_ì‹¬ì‚¬í•­ëª©ëª… and 
                    predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… == actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…):
                    exact_match = True
                    break
                # ë¶€ë¶„ ì¼ì¹˜ (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì¼ì¹˜)
                elif (predicted_ì‹¬ì‚¬í•­ëª©ëª… == actual_ì‹¬ì‚¬í•­ëª©ëª… or 
                      predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… == actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…):
                    partial_match = True
            
            if exact_match:
                correct_count += 1
                exact_match_count += 1
            elif partial_match:
                partial_match_count += 1
            else:
                no_match_count += 1
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean([r['similarity'] for r in results])
            total_similarities.append(avg_similarity)
        
        accuracy = correct_count / len(self.valid_df) * 100
        avg_similarity = np.mean(total_similarities)
        
        print(f"âœ… ê²€ì¦ ì •í™•ë„ (ì™„ì „ ì¼ì¹˜): {accuracy:.2f}%")
        print(f"ğŸ“ˆ ê²€ì¦ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„:")
        print(f"   - ì™„ì „ ì¼ì¹˜: {exact_match_count}ê°œ ({exact_match_count/len(self.valid_df)*100:.2f}%)")
        print(f"   - ë¶€ë¶„ ì¼ì¹˜: {partial_match_count}ê°œ ({partial_match_count/len(self.valid_df)*100:.2f}%)")
        print(f"   - ë¶ˆì¼ì¹˜: {no_match_count}ê°œ ({no_match_count/len(self.valid_df)*100:.2f}%)")
        
        return accuracy, avg_similarity
    
    def evaluate_on_test(self, top_k=5):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€"""
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ (ìƒìœ„ {top_k}ê°œ ê¸°ì¤€)")
        
        correct_count = 0
        total_similarities = []
        
        # ğŸš¨ ë” ì—„ê²©í•œ í‰ê°€ë¥¼ ìœ„í•œ ì¹´ìš´í„°
        exact_match_count = 0  # ì™„ì „ ì¼ì¹˜
        partial_match_count = 0  # ë¶€ë¶„ ì¼ì¹˜
        no_match_count = 0  # ë¶ˆì¼ì¹˜
        
        for i, test_row in self.test_df.iterrows():
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±
            query_data = {
                'text': test_row[self.text_column],
                'categorical': {},
                'numerical': {}
            }
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.categorical_features:
                if col in test_row and pd.notna(test_row[col]):
                    query_data['categorical'][col] = test_row[col]
            
            # ìˆ«ìí˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.numerical_features:
                if col in test_row and pd.notna(test_row[col]):
                    query_data['numerical'][col] = test_row[col]
            
            # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
            results = self.find_similar_cases(query_data, self.train_df, top_k=top_k, verbose=False)
            
            # ì •í™•ë„ ê³„ì‚°
            actual_ì‹¬ì‚¬í•­ëª©ëª… = test_row.get('ì‹¬ì‚¬í•­ëª©ëª…', '')
            actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… = test_row.get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')
            
            # ğŸš¨ ë” ì—„ê²©í•œ í‰ê°€
            exact_match = False
            partial_match = False
            
            for result in results:
                predicted_ì‹¬ì‚¬í•­ëª©ëª… = result['predicted_ì‹¬ì‚¬í•­ëª©ëª…']
                predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… = result['predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…']
                
                # ì™„ì „ ì¼ì¹˜ (ë‘˜ ë‹¤ ì¼ì¹˜)
                if (predicted_ì‹¬ì‚¬í•­ëª©ëª… == actual_ì‹¬ì‚¬í•­ëª©ëª… and 
                    predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… == actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…):
                    exact_match = True
                    break
                # ë¶€ë¶„ ì¼ì¹˜ (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì¼ì¹˜)
                elif (predicted_ì‹¬ì‚¬í•­ëª©ëª… == actual_ì‹¬ì‚¬í•­ëª©ëª… or 
                      predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… == actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…):
                    partial_match = True
            
            if exact_match:
                correct_count += 1
                exact_match_count += 1
            elif partial_match:
                partial_match_count += 1
            else:
                no_match_count += 1
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean([r['similarity'] for r in results])
            total_similarities.append(avg_similarity)
        
        accuracy = correct_count / len(self.test_df) * 100
        avg_similarity = np.mean(total_similarities)
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (ì™„ì „ ì¼ì¹˜): {accuracy:.2f}%")
        print(f"ğŸ“ˆ í…ŒìŠ¤íŠ¸ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„:")
        print(f"   - ì™„ì „ ì¼ì¹˜: {exact_match_count}ê°œ ({exact_match_count/len(self.test_df)*100:.2f}%)")
        print(f"   - ë¶€ë¶„ ì¼ì¹˜: {partial_match_count}ê°œ ({partial_match_count/len(self.test_df)*100:.2f}%)")
        print(f"   - ë¶ˆì¼ì¹˜: {no_match_count}ê°œ ({no_match_count/len(self.test_df)*100:.2f}%)")
        
        return accuracy, avg_similarity
    
    def test_with_real_queries(self, top_k=5):
        """ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì„ íƒ
        sample_size = min(5, len(self.test_df))
        test_samples = self.test_df.sample(n=sample_size, random_state=42)
        
        for i, (idx, test_row) in enumerate(test_samples.iterrows(), 1):
            print(f"\n--- ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i} ---")
            print(f"ğŸ” ì¿¼ë¦¬: {test_row[self.text_column][:100]}...")
            print(f"ğŸ“‹ ì‹¤ì œ ì‹¬ì‚¬í•­ëª©ëª…: {test_row.get('ì‹¬ì‚¬í•­ëª©ëª…', 'N/A')}")
            print(f"ğŸ“‹ ì‹¤ì œ ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…: {test_row.get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', 'N/A')}")
            
            # ì¿¼ë¦¬ ë°ì´í„° ìƒì„±
            query_data = {
                'text': test_row[self.text_column],
                'categorical': {},
                'numerical': {}
            }
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.categorical_features:
                if col in test_row and pd.notna(test_row[col]):
                    query_data['categorical'][col] = test_row[col]
            
            # ìˆ«ìí˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.numerical_features:
                if col in test_row and pd.notna(test_row[col]):
                    query_data['numerical'][col] = test_row[col]
            
            # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
            results = self.find_similar_cases(query_data, self.train_df, top_k=top_k)
            
            # ì •í™•ë„ í™•ì¸
            actual_ì‹¬ì‚¬í•­ëª©ëª… = test_row.get('ì‹¬ì‚¬í•­ëª©ëª…', '')
            actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… = test_row.get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')
            
            is_correct = False
            for result in results:
                predicted_ì‹¬ì‚¬í•­ëª©ëª… = result['predicted_ì‹¬ì‚¬í•­ëª©ëª…']
                predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… = result['predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…']
                
                if (predicted_ì‹¬ì‚¬í•­ëª©ëª… == actual_ì‹¬ì‚¬í•­ëª©ëª… or 
                    predicted_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… == actual_ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…):
                    is_correct = True
                    break
            
            if is_correct:
                print("âœ… ì •í™•í•œ ì‹¬ì‚¬í•­ëª© ì˜ˆì¸¡!")
            else:
                print("âŒ ì •í™•í•œ ì‹¬ì‚¬í•­ëª© ë¯¸ì˜ˆì¸¡")

    def diagnose_data_leakage(self):
        """ê°œì„ ëœ ë°ì´í„° ëˆ„ì¶œ ì§„ë‹¨ (ì‚¬ê³  ë‚´ìš© + ì‹¬ì‚¬ ê²°ê³¼ êµ¬ë¶„)"""
        print(f"\nğŸ” ê°œì„ ëœ ë°ì´í„° ëˆ„ì¶œ ì§„ë‹¨")
        print(f"=" * 60)
        
        # 1. ì‚¬ê³  ë‚´ìš© ì¤‘ë³µ í™•ì¸ (í—ˆìš© ê°€ëŠ¥)
        print(f"1ï¸âƒ£ ì‚¬ê³  ë‚´ìš© ì¤‘ë³µ í™•ì¸:")
        all_texts = self.train_df[self.text_column].tolist() + self.valid_df[self.text_column].tolist() + self.test_df[self.text_column].tolist()
        unique_texts = set(all_texts)
        text_duplication_rate = (1 - len(unique_texts)/len(all_texts))
        print(f"   - ì „ì²´ ì‚¬ê³  ë‚´ìš©: {len(all_texts)}ê°œ")
        print(f"   - ê³ ìœ  ì‚¬ê³  ë‚´ìš©: {len(unique_texts)}ê°œ")
        print(f"   - ì‚¬ê³  ë‚´ìš© ì¤‘ë³µë¥ : {text_duplication_rate*100:.2f}%")
        
        # 2. ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µ í™•ì¸ (ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ)
        print(f"\n2ï¸âƒ£ ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µ í™•ì¸:")
        all_reviews = []
        for df in [self.train_df, self.valid_df, self.test_df]:
            for _, row in df.iterrows():
                review = f"{row.get('ì‹¬ì‚¬í•­ëª©ëª…', '')}|{row.get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')}"
                all_reviews.append(review)
        
        unique_reviews = set(all_reviews)
        review_duplication_rate = (1 - len(unique_reviews)/len(all_reviews))
        print(f"   - ì „ì²´ ì‹¬ì‚¬ ê²°ê³¼: {len(all_reviews)}ê°œ")
        print(f"   - ê³ ìœ  ì‹¬ì‚¬ ê²°ê³¼: {len(unique_reviews)}ê°œ")
        print(f"   - ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µë¥ : {review_duplication_rate*100:.2f}%")
        
        # 3. ì‚¬ê³  ë‚´ìš©ì€ ê°™ì§€ë§Œ ì‹¬ì‚¬ ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš° (ì •ìƒì ì¸ ê²½ìš°)
        print(f"\n3ï¸âƒ£ ê°™ì€ ì‚¬ê³ , ë‹¤ë¥¸ ì‹¬ì‚¬ ê²°ê³¼ í™•ì¸ (ì •ìƒì ì¸ ê²½ìš°):")
        same_accident_different_review = self.check_same_accident_different_review()
        print(f"   - ê°™ì€ ì‚¬ê³ ì§€ë§Œ ë‹¤ë¥¸ ì‹¬ì‚¬ ê²°ê³¼: {same_accident_different_review}ê°œ")
        print(f"   - ë¹„ìœ¨: {same_accident_different_review/len(self.valid_df)*100:.2f}%")
        
        # 4. ì‚¬ê³  ë‚´ìš©ë„ ê°™ê³  ì‹¬ì‚¬ ê²°ê³¼ë„ ê°™ì€ ê²½ìš° (ë¬¸ì œ)
        print(f"\n4ï¸âƒ£ ê°™ì€ ì‚¬ê³ , ê°™ì€ ì‹¬ì‚¬ ê²°ê³¼ í™•ì¸ (ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ê²½ìš°):")
        same_accident_same_review = self.check_same_accident_same_review()
        print(f"   - ê°™ì€ ì‚¬ê³  + ê°™ì€ ì‹¬ì‚¬ ê²°ê³¼: {same_accident_same_review}ê°œ")
        print(f"   - ë¹„ìœ¨: {same_accident_same_review/len(self.valid_df)*100:.2f}%")
        
        # 5. ì‹¬ì‚¬í•­ëª©ëª… ë¶„í¬ í™•ì¸
        print(f"\n5ï¸âƒ£ ì‹¬ì‚¬í•­ëª©ëª… ë¶„í¬ í™•ì¸:")
        train_items = self.train_df['ì‹¬ì‚¬í•­ëª©ëª…'].value_counts()
        valid_items = self.valid_df['ì‹¬ì‚¬í•­ëª©ëª…'].value_counts()
        test_items = self.test_df['ì‹¬ì‚¬í•­ëª©ëª…'].value_counts()
        
        print(f"   - í•™ìŠµ ë°ì´í„° ê³ ìœ  ì‹¬ì‚¬í•­ëª©: {len(train_items)}ê°œ")
        print(f"   - ê²€ì¦ ë°ì´í„° ê³ ìœ  ì‹¬ì‚¬í•­ëª©: {len(valid_items)}ê°œ")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„° ê³ ìœ  ì‹¬ì‚¬í•­ëª©: {len(test_items)}ê°œ")
        
        # 6. ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… ë¶„í¬ í™•ì¸
        print(f"\n6ï¸âƒ£ ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª… ë¶„í¬ í™•ì¸:")
        train_product_items = self.train_df['ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…'].value_counts()
        valid_product_items = self.valid_df['ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…'].value_counts()
        test_product_items = self.test_df['ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…'].value_counts()
        
        print(f"   - í•™ìŠµ ë°ì´í„° ê³ ìœ  ìƒí’ˆì‹¬ì‚¬í•­ëª©: {len(train_product_items)}ê°œ")
        print(f"   - ê²€ì¦ ë°ì´í„° ê³ ìœ  ìƒí’ˆì‹¬ì‚¬í•­ëª©: {len(valid_product_items)}ê°œ")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„° ê³ ìœ  ìƒí’ˆì‹¬ì‚¬í•­ëª©: {len(test_product_items)}ê°œ")
        
        # 7. ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ë„ í‰ê°€ (ê°œì„ ëœ ë°©ì‹)
        print(f"\n7ï¸âƒ£ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ë„ í‰ê°€ (ê°œì„ ëœ ë°©ì‹):")
        risk_score = 0
        risk_details = []
        
        # ì‚¬ê³  ë‚´ìš© ì¤‘ë³µ ê¸°ë°˜ ìœ„í—˜ë„ (ë‚®ì€ ê°€ì¤‘ì¹˜)
        if text_duplication_rate > 0.2:  # 20% ì´ìƒ
            risk_score += 10
            risk_details.append(f"ì‚¬ê³  ë‚´ìš© ì¤‘ë³µë¥  ë†’ìŒ: {text_duplication_rate*100:.1f}% (+10ì )")
        
        # ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µ ê¸°ë°˜ ìœ„í—˜ë„ (ë†’ì€ ê°€ì¤‘ì¹˜)
        if review_duplication_rate > 0.1:  # 10% ì´ìƒ
            risk_score += 30
            risk_details.append(f"ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µë¥  ë†’ìŒ: {review_duplication_rate*100:.1f}% (+30ì )")
        
        # ê°™ì€ ì‚¬ê³  + ê°™ì€ ì‹¬ì‚¬ ê¸°ë°˜ ìœ„í—˜ë„ (ë§¤ìš° ë†’ì€ ê°€ì¤‘ì¹˜)
        same_accident_same_review_ratio = same_accident_same_review / len(self.valid_df)
        if same_accident_same_review_ratio > 0.05:  # 5% ì´ìƒ
            risk_score += 40
            risk_details.append(f"ê°™ì€ ì‚¬ê³ +ê°™ì€ ì‹¬ì‚¬ ë¹„ìœ¨ ë†’ìŒ: {same_accident_same_review_ratio*100:.1f}% (+40ì )")
        
        # ì‹¬ì‚¬í•­ëª© ë¶„í¬ ê¸°ë°˜ ìœ„í—˜ë„
        common_items = set(train_items.index) & set(valid_items.index) & set(test_items.index)
        if len(common_items) / len(set(train_items.index)) > 0.9:  # 90% ì´ìƒ
            risk_score += 20
            risk_details.append(f"ì‹¬ì‚¬í•­ëª©ëª… ì¤‘ë³µ: {len(common_items)}ê°œ ê³µí†µ (+20ì )")
        
        # ìœ„í—˜ë„ ìƒì„¸ ì¶œë ¥
        for detail in risk_details:
            print(f"   âš ï¸ {detail}")
        
        print(f"\nğŸ¯ ìµœì¢… ìœ„í—˜ë„ ì ìˆ˜: {risk_score}/100")
        
        # ìœ„í—˜ë„ í•´ì„
        if risk_score < 20:
            print(f"âœ… ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ë„ ë§¤ìš° ë‚®ìŒ")
            print(f"   - ì‚¬ê³  ë‚´ìš©ê³¼ ì‹¬ì‚¬ ê²°ê³¼ê°€ ì ì ˆíˆ ë¶„ì‚°ë˜ì–´ ìˆìŒ")
            print(f"   - ê°™ì€ ì‚¬ê³ ë¼ë„ ë‹¤ë¥¸ ì‹¬ì‚¬ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚¨ (ì •ìƒì ì¸ ì—…ë¬´ ìƒí™©)")
        elif risk_score < 50:
            print(f"âš ï¸ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ë„ ë³´í†µ")
            print(f"   - ì¼ë¶€ ì¤‘ë³µì´ ìˆì§€ë§Œ í—ˆìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€")
            print(f"   - ê²°ê³¼ë¥¼ ì‹ ì¤‘í•˜ê²Œ í•´ì„í•´ì•¼ í•¨")
        elif risk_score < 80:
            print(f"ğŸš¨ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ë„ ë†’ìŒ")
            print(f"   - ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µì´ ë§ìŒ")
            print(f"   - ë†’ì€ ì •í™•ë„ê°€ ë°ì´í„° ëˆ„ì¶œ ë•Œë¬¸ì¼ ìˆ˜ ìˆìŒ")
        else:
            print(f"ğŸš¨ğŸš¨ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ë„ ë§¤ìš° ë†’ìŒ!")
            print(f"   - ì‹¬ê°í•œ ë°ì´í„° ì¤‘ë³µ ë¬¸ì œ")
            print(f"   - ëª¨ë¸ ì„±ëŠ¥ì´ í˜„ì‹¤ê³¼ ë‹¤ë¥¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ")
        
        return risk_score, {
            'text_duplication_rate': text_duplication_rate,
            'review_duplication_rate': review_duplication_rate,
            'same_accident_different_review': same_accident_different_review,
            'same_accident_same_review': same_accident_same_review,
            'risk_details': risk_details
        }
    
    def check_same_accident_different_review(self):
        """ê°™ì€ ì‚¬ê³ ì§€ë§Œ ë‹¤ë¥¸ ì‹¬ì‚¬ ê²°ê³¼ì¸ ê²½ìš° í™•ì¸ (ì •ìƒì ì¸ ê²½ìš°) - ìµœì í™”ëœ ë²„ì „"""
        print(f"   ğŸ”„ ì„ë² ë”© ì‚¬ì „ ê³„ì‚° ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°
        valid_texts = self.valid_df[self.text_column].tolist()
        train_texts = self.train_df[self.text_column].tolist()
        
        # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
        valid_embeddings = self.encode_batch(valid_texts)
        train_embeddings = self.encode_batch(train_texts)
        
        print(f"   ğŸ” ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        normal_cases = 0
        total_valid = len(valid_embeddings)
        
        # ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
        for i, valid_embedding in enumerate(valid_embeddings):
            # ì§„í–‰ë¥  í‘œì‹œ
            if (i + 1) % max(1, total_valid // 10) == 0 or i == total_valid - 1:
                progress = (i + 1) / total_valid * 100
                print(f"   ì§„í–‰ë¥ : {progress:.1f}% ({i + 1}/{total_valid})")
            
            valid_review = f"{self.valid_df.iloc[i].get('ì‹¬ì‚¬í•­ëª©ëª…', '')}|{self.valid_df.iloc[i].get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')}"
            
            # í•œ ë²ˆì— ëª¨ë“  í•™ìŠµ ë°ì´í„°ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity([valid_embedding], train_embeddings)[0]
            
            # ìœ ì‚¬ë„ê°€ ë†’ì€ ì¸ë±ìŠ¤ë“¤ ì°¾ê¸°
            high_similarity_indices = np.where(similarities > 0.95)[0]
            
            for idx in high_similarity_indices:
                train_review = f"{self.train_df.iloc[idx].get('ì‹¬ì‚¬í•­ëª©ëª…', '')}|{self.train_df.iloc[idx].get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')}"
                if valid_review != train_review:
                    normal_cases += 1
                    break
        
        return normal_cases
    
    def check_same_accident_same_review(self):
        """ì‚¬ê³  ë‚´ìš©ê³¼ ì‹¬ì‚¬ ê²°ê³¼ê°€ ëª¨ë‘ ê°™ì€ ê²½ìš° í™•ì¸ (ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ê²½ìš°) - ìµœì í™”ëœ ë²„ì „"""
        print(f"   ğŸ”„ ì„ë² ë”© ì‚¬ì „ ê³„ì‚° ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°
        valid_texts = self.valid_df[self.text_column].tolist()
        train_texts = self.train_df[self.text_column].tolist()
        
        # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
        valid_embeddings = self.encode_batch(valid_texts)
        train_embeddings = self.encode_batch(train_texts)
        
        print(f"   ğŸ” ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        problematic_cases = 0
        total_valid = len(valid_embeddings)
        
        # ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
        for i, valid_embedding in enumerate(valid_embeddings):
            # ì§„í–‰ë¥  í‘œì‹œ
            if (i + 1) % max(1, total_valid // 10) == 0 or i == total_valid - 1:
                progress = (i + 1) / total_valid * 100
                print(f"   ì§„í–‰ë¥ : {progress:.1f}% ({i + 1}/{total_valid})")
            
            valid_review = f"{self.valid_df.iloc[i].get('ì‹¬ì‚¬í•­ëª©ëª…', '')}|{self.valid_df.iloc[i].get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')}"
            
            # í•œ ë²ˆì— ëª¨ë“  í•™ìŠµ ë°ì´í„°ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity([valid_embedding], train_embeddings)[0]
            
            # ìœ ì‚¬ë„ê°€ ë†’ì€ ì¸ë±ìŠ¤ë“¤ ì°¾ê¸°
            high_similarity_indices = np.where(similarities > 0.95)[0]
            
            for idx in high_similarity_indices:
                train_review = f"{self.train_df.iloc[idx].get('ì‹¬ì‚¬í•­ëª©ëª…', '')}|{self.train_df.iloc[idx].get('ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…', '')}"
                if valid_review == train_review:
                    problematic_cases += 1
                    break
        
        return problematic_cases

def main():
    # ê°œì„ ëœ ë‹¤ì¤‘ ëª¨ë‹¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ëª¨ë¸ ì´ˆê¸°í™”
    model = ImprovedSimilaritySearch(text_weight=0.5, categorical_weight=0.4, numerical_weight=0.1)
    
    # ìºì‹œ ë¡œë“œ ì‹œë„
    cache_loaded = model.load_embeddings('improved_embeddings_cache.pkl')
    
    if cache_loaded:
        # ìºì‹œê°€ ìˆìœ¼ë©´ ë°ì´í„°ë§Œ ë¡œë“œ
        train_df, valid_df, test_df = model.load_data_only('data/case.csv')
    else:
        # ìºì‹œê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”© ìƒì„±
        train_df, valid_df, test_df = model.load_and_split_data('data/case.csv')
        model.save_embeddings('improved_embeddings_cache.pkl')
    
    print(f"\nğŸ¯ ê°œì„ ëœ ëª¨ë¸ ì„¤ì •:")
    print(f"   - Yê°’: ì‹¬ì‚¬í•­ëª©ëª…, ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…")
    print(f"   - ê°€ì¤‘ì¹˜: í…ìŠ¤íŠ¸ {model.text_weight}, ë²”ì£¼í˜• {model.categorical_weight}, ìˆ«ìí˜• {model.numerical_weight}")
    print(f"   - ë²”ì£¼í˜• ìœ ì‚¬ë„: í‰ê·  ë°©ì‹ (ê³±ì…ˆ ëŒ€ì‹ )")
    print(f"   - ë°ì´í„° ëˆ„ì¶œ ë°©ì§€: í•™ìŠµ ë°ì´í„°ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬ ëª¨ë¸ í•™ìŠµ")
    
    # ğŸš¨ ë°ì´í„° ëˆ„ì¶œ ì§„ë‹¨
    print(f"\nğŸ” ë°ì´í„° ëˆ„ì¶œ ì§„ë‹¨ ì‹œì‘")
    risk_score, details = model.diagnose_data_leakage()
    
    # 1ë‹¨ê³„: ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€
    print(f"\nğŸ§ª 1ë‹¨ê³„: ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€")
    valid_accuracy, valid_similarity = model.evaluate_on_validation(top_k=5)
    
    # 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€
    print(f"\nğŸ§ª 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€")
    test_accuracy, test_similarity = model.evaluate_on_test(top_k=5)
    
    # 3ë‹¨ê³„: ì‹¤ì œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª 3ë‹¨ê³„: ì‹¤ì œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
    model.test_with_real_queries(top_k=5)
    
    # ìºì‹œ ìƒíƒœ ì¶œë ¥
    model.print_cache_status()
    
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"   ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ë„: {risk_score}/100")
    print(f"   ê²€ì¦ ì •í™•ë„ (ì™„ì „ ì¼ì¹˜): {valid_accuracy:.2f}%")
    print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„ (ì™„ì „ ì¼ì¹˜): {test_accuracy:.2f}%")
    print(f"   ê²€ì¦ í‰ê·  ìœ ì‚¬ë„: {valid_similarity:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ í‰ê·  ìœ ì‚¬ë„: {test_similarity:.4f}")
    
    # ğŸš¨ ë°ì´í„° ëˆ„ì¶œ ê²½ê³  ë° í•´ì„
    print(f"\nğŸ“Š ê²°ê³¼ í•´ì„ ê°€ì´ë“œ:")
    print(f"=" * 50)
    
    if risk_score < 20:
        print(f"âœ… ë°ì´í„° í’ˆì§ˆ: ë§¤ìš° ì¢‹ìŒ")
        print(f"   - ì‚¬ê³  ë‚´ìš© ì¤‘ë³µë¥ : {details['text_duplication_rate']*100:.1f}%")
        print(f"   - ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µë¥ : {details['review_duplication_rate']*100:.1f}%")
        print(f"   - ê°™ì€ ì‚¬ê³ , ë‹¤ë¥¸ ì‹¬ì‚¬: {details['same_accident_different_review']}ê°œ ({details['same_accident_different_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   - ê°™ì€ ì‚¬ê³ , ê°™ì€ ì‹¬ì‚¬: {details['same_accident_same_review']}ê°œ ({details['same_accident_same_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   ğŸ’¡ í•´ì„: ì •ìƒì ì¸ ì—…ë¬´ ìƒí™©. ê°™ì€ ì‚¬ê³ ë¼ë„ ë‹¤ë¥¸ ì‹¬ì‚¬ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚¨.")
        print(f"   ğŸ’¡ ëª¨ë¸ ì„±ëŠ¥: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€")
        
    elif risk_score < 50:
        print(f"âš ï¸ ë°ì´í„° í’ˆì§ˆ: ë³´í†µ")
        print(f"   - ì‚¬ê³  ë‚´ìš© ì¤‘ë³µë¥ : {details['text_duplication_rate']*100:.1f}%")
        print(f"   - ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µë¥ : {details['review_duplication_rate']*100:.1f}%")
        print(f"   - ê°™ì€ ì‚¬ê³ , ë‹¤ë¥¸ ì‹¬ì‚¬: {details['same_accident_different_review']}ê°œ ({details['same_accident_different_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   - ê°™ì€ ì‚¬ê³ , ê°™ì€ ì‹¬ì‚¬: {details['same_accident_same_review']}ê°œ ({details['same_accident_same_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   ğŸ’¡ í•´ì„: ì¼ë¶€ ì¤‘ë³µì´ ìˆì§€ë§Œ í—ˆìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€.")
        print(f"   ğŸ’¡ ëª¨ë¸ ì„±ëŠ¥: ì‹ ì¤‘í•˜ê²Œ í•´ì„ í•„ìš”")
        
    elif risk_score < 80:
        print(f"ğŸš¨ ë°ì´í„° í’ˆì§ˆ: ë¬¸ì œ ìˆìŒ")
        print(f"   - ì‚¬ê³  ë‚´ìš© ì¤‘ë³µë¥ : {details['text_duplication_rate']*100:.1f}%")
        print(f"   - ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µë¥ : {details['review_duplication_rate']*100:.1f}%")
        print(f"   - ê°™ì€ ì‚¬ê³ , ë‹¤ë¥¸ ì‹¬ì‚¬: {details['same_accident_different_review']}ê°œ ({details['same_accident_different_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   - ê°™ì€ ì‚¬ê³ , ê°™ì€ ì‹¬ì‚¬: {details['same_accident_same_review']}ê°œ ({details['same_accident_same_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   ğŸ’¡ í•´ì„: ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µì´ ë§ìŒ. ë†’ì€ ì •í™•ë„ê°€ ë°ì´í„° ëˆ„ì¶œ ë•Œë¬¸ì¼ ìˆ˜ ìˆìŒ.")
        print(f"   ğŸ’¡ ëª¨ë¸ ì„±ëŠ¥: ì‹¤ì œ ì„±ëŠ¥ë³´ë‹¤ ë‚®ì„ ê°€ëŠ¥ì„± ë†’ìŒ")
        
    else:
        print(f"ğŸš¨ğŸš¨ ë°ì´í„° í’ˆì§ˆ: ì‹¬ê°í•œ ë¬¸ì œ")
        print(f"   - ì‚¬ê³  ë‚´ìš© ì¤‘ë³µë¥ : {details['text_duplication_rate']*100:.1f}%")
        print(f"   - ì‹¬ì‚¬ ê²°ê³¼ ì¤‘ë³µë¥ : {details['review_duplication_rate']*100:.1f}%")
        print(f"   - ê°™ì€ ì‚¬ê³ , ë‹¤ë¥¸ ì‹¬ì‚¬: {details['same_accident_different_review']}ê°œ ({details['same_accident_different_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   - ê°™ì€ ì‚¬ê³ , ê°™ì€ ì‹¬ì‚¬: {details['same_accident_same_review']}ê°œ ({details['same_accident_same_review']/len(model.valid_df)*100:.1f}%)")
        print(f"   ğŸ’¡ í•´ì„: ì‹¬ê°í•œ ë°ì´í„° ì¤‘ë³µ ë¬¸ì œ. ëª¨ë¸ì´ 'ê¸°ì–µ'í•˜ê³  ìˆëŠ” ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸.")
        print(f"   ğŸ’¡ ëª¨ë¸ ì„±ëŠ¥: í˜„ì‹¤ê³¼ ë‹¤ë¥¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŒ")
    
    # ğŸ¯ ì—…ë¬´ ê´€ì  í•´ì„
    print(f"\nğŸ¯ ì—…ë¬´ ê´€ì  í•´ì„:")
    print(f"=" * 50)
    
    # ê°™ì€ ì‚¬ê³ , ë‹¤ë¥¸ ì‹¬ì‚¬ ë¹„ìœ¨ í•´ì„
    same_accident_different_ratio = details['same_accident_different_review'] / len(model.valid_df)
    if same_accident_different_ratio > 0.3:
        print(f"âœ… ì‹¬ì‚¬ ë‹¤ì–‘ì„±: ë†’ìŒ ({same_accident_different_ratio*100:.1f}%)")
        print(f"   - ê°™ì€ ì‚¬ê³ ë¼ë„ ë‹¤ë¥¸ ì‹¬ì‚¬ ê²°ê³¼ê°€ ë§ì´ ë‚˜íƒ€ë‚¨")
        print(f"   - ì‹¬ì‚¬ìì˜ íŒë‹¨ ì°¨ì´, ë©´ì±…ì‚¬ìœ  ë°œê²¬ ë“±ì´ ë°˜ì˜ë¨")
        print(f"   - ì‹¤ì œ ì—…ë¬´ ìƒí™©ê³¼ ìœ ì‚¬í•¨")
    elif same_accident_different_ratio > 0.1:
        print(f"âš ï¸ ì‹¬ì‚¬ ë‹¤ì–‘ì„±: ë³´í†µ ({same_accident_different_ratio*100:.1f}%)")
        print(f"   - ì¼ë¶€ ì‹¬ì‚¬ ë‹¤ì–‘ì„±ì´ ë‚˜íƒ€ë‚¨")
        print(f"   - ë” ë‹¤ì–‘í•œ ì‹¬ì‚¬ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì¢‹ê² ìŒ")
    else:
        print(f"ğŸš¨ ì‹¬ì‚¬ ë‹¤ì–‘ì„±: ë‚®ìŒ ({same_accident_different_ratio*100:.1f}%)")
        print(f"   - ê°™ì€ ì‚¬ê³ ëŠ” ê±°ì˜ ê°™ì€ ì‹¬ì‚¬ ê²°ê³¼")
        print(f"   - ì‹¬ì‚¬ìì˜ íŒë‹¨ ì°¨ì´ê°€ ë°˜ì˜ë˜ì§€ ì•ŠìŒ")
        print(f"   - ì‹¤ì œ ì—…ë¬´ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ")
    
    # ê°™ì€ ì‚¬ê³ , ê°™ì€ ì‹¬ì‚¬ ë¹„ìœ¨ í•´ì„
    same_accident_same_ratio = details['same_accident_same_review'] / len(model.valid_df)
    if same_accident_same_ratio > 0.1:
        print(f"ğŸš¨ ë°ì´í„° ì¤‘ë³µ: ë†’ìŒ ({same_accident_same_ratio*100:.1f}%)")
        print(f"   - ì™„ì „íˆ ê°™ì€ ì‚¬ê³ +ì‹¬ì‚¬ê°€ ë§ì´ ì¤‘ë³µë¨")
        print(f"   - ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì¤‘ë³µ ì œê±° í•„ìš”")
    elif same_accident_same_ratio > 0.05:
        print(f"âš ï¸ ë°ì´í„° ì¤‘ë³µ: ë³´í†µ ({same_accident_same_ratio*100:.1f}%)")
        print(f"   - ì¼ë¶€ ì¤‘ë³µì´ ìˆì§€ë§Œ í—ˆìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€")
    else:
        print(f"âœ… ë°ì´í„° ì¤‘ë³µ: ë‚®ìŒ ({same_accident_same_ratio*100:.1f}%)")
        print(f"   - ì¤‘ë³µì´ ê±°ì˜ ì—†ìŒ")
    
    # ğŸ¯ ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ¯ ê¶Œì¥ì‚¬í•­:")
    print(f"=" * 50)
    
    if risk_score < 20:
        print(f"âœ… í˜„ì¬ ìƒíƒœê°€ ì¢‹ìŒ. ì¶”ê°€ ì¡°ì¹˜ ë¶ˆí•„ìš”")
    elif risk_score < 50:
        print(f"âš ï¸ ë°ì´í„° í’ˆì§ˆ ê°œì„  ê¶Œì¥:")
        print(f"   - ì¤‘ë³µ ë°ì´í„° ì œê±° ê²€í† ")
        print(f"   - ë” ë‹¤ì–‘í•œ ì‹¬ì‚¬ íŒ¨í„´ ìˆ˜ì§‘")
    elif risk_score < 80:
        print(f"ğŸš¨ ë°ì´í„° í’ˆì§ˆ ê°œì„  í•„ìˆ˜:")
        print(f"   - ì¤‘ë³µ ë°ì´í„° ëŒ€í­ ì œê±°")
        print(f"   - ìƒˆë¡œìš´ ì‚¬ê³  ìœ í˜• ë° ì‹¬ì‚¬ íŒ¨í„´ ìˆ˜ì§‘")
        print(f"   - ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”")
    else:
        print(f"ğŸš¨ğŸš¨ ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œê¸‰:")
        print(f"   - ë°ì´í„°ì…‹ ì „ì²´ ì¬ê²€í† ")
        print(f"   - ì¤‘ë³µ ì œê±° í›„ ëª¨ë¸ ì¬í•™ìŠµ")
        print(f"   - ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ê³ ë ¤")

if __name__ == "__main__":
    main() 