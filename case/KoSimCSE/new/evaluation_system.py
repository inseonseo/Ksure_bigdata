"""
ë³´í—˜ì‚¬ê³  ìœ ì‚¬ë„ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ Train/Valid/Test ë¶„í•  ë° í‰ê°€ ì‹œìŠ¤í…œ
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pickle
import os
from datetime import datetime

class InsuranceEvaluationSystem:
    def __init__(self, data_path, preserve_labels=True, min_support_for_test=2):
        """
        ì´ˆê¸°í™”
        
        Args:
            data_path: CSV ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            preserve_labels: Trueì´ë©´ ë¼ë²¨(íŒì •êµ¬ë¶„/íŒì •ì‚¬ìœ )ì„ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œë„ í†µí•©/ë³€í™˜í•˜ì§€ ì•ŠìŒ
            min_support_for_test: í…ŒìŠ¤íŠ¸ì— í¬í•¨ì‹œí‚¤ê¸° ìœ„í•œ í´ë˜ìŠ¤ ìµœì†Œ ê±´ìˆ˜(ë¯¸ë§Œì€ testì—ì„œ ì œì™¸)
        """
        self.data_path = data_path
        self.df = None
        self.train_df = None
        self.valid_df = None
        self.test_df = None
        self.label_encoders = {}
        
        # ì¶”ê°€ ì„¤ì •
        self.preserve_labels = preserve_labels
        self.min_support_for_test = min_support_for_test
        self.excluded_classes_from_test = []
        
    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        self.df = pd.read_csv(self.data_path, encoding='cp949')
        print(f"ì „ì²´ ë°ì´í„°: {len(self.df):,}ê±´")
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        self.df = self.df.dropna(subset=['íŒì •êµ¬ë¶„', 'íŒì •ì‚¬ìœ '])
        print(f"ìœ íš¨í•œ íŒì • ë°ì´í„°: {len(self.df):,}ê±´")
        
        if not self.preserve_labels:
            # ë¼ë²¨ í†µí•©ì´ í—ˆìš©ëœ ê²½ìš°ì—ë§Œ ìˆ˜í–‰
            self.df = self._consolidate_judgment_categories()
            self.df = self._consolidate_reason_categories()
        else:
            print("ğŸ”’ ë¼ë²¨ ë³´ì¡´ ëª¨ë“œ: íŒì •êµ¬ë¶„/íŒì •ì‚¬ìœ ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ë¶„í¬ í™•ì¸
        print("\nğŸ“ˆ íŒì •êµ¬ë¶„ ë¶„í¬:")
        print(self.df['íŒì •êµ¬ë¶„'].value_counts())
        print("\nğŸ“ˆ íŒì •ì‚¬ìœ  ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        print(self.df['íŒì •ì‚¬ìœ '].value_counts().head(10))
        
        return self.df
    
    def _consolidate_judgment_categories(self):
        """íŒì •êµ¬ë¶„ ì¹´í…Œê³ ë¦¬ í†µí•© (ìµœì†Œí•œì˜ í†µí•©ë§Œ)"""
        print("\nğŸ”„ íŒì •êµ¬ë¶„ ì¹´í…Œê³ ë¦¬ ì •ë¦¬ ì¤‘...")
        
        # ì›ë³¸ ë¶„í¬
        original_counts = self.df['íŒì •êµ¬ë¶„'].value_counts()
        print("ì›ë³¸ ë¶„í¬:")
        print(original_counts)
        
        # ìµœì†Œí•œì˜ í†µí•©ë§Œ ìˆ˜í–‰ (ì—…ë¬´ì ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” í†µí•©)
        judgment_mapping = {
            'ì§€ê¸‰': 'ì§€ê¸‰',
            'ë©´ì±…': 'ë©´ì±…',
            'ì§€ê¸‰ìœ ì˜ˆ': 'ì§€ê¸‰ìœ ì˜ˆ',
            'ê°€ì§€ê¸‰': 'ì§€ê¸‰',  # ê°€ì§€ê¸‰ì€ ì§€ê¸‰ì˜ ì¼ì¢…
            'ê¸°íƒ€ì§€ê¸‰ê±°ì ˆ': 'ì§€ê¸‰ê±°ì ˆ',  # ì§€ê¸‰ê±°ì ˆë¡œ ëª…í™•í™”
            'ë³´í—˜ê´€ê³„ë¶ˆì„±ë¦½': 'ë©´ì±…'  # ë³´í—˜ê´€ê³„ë¶ˆì„±ë¦½ì€ ë©´ì±…ì˜ ì¼ì¢…
        }
        
        # ë§¤í•‘ ì ìš©
        self.df['íŒì •êµ¬ë¶„_original'] = self.df['íŒì •êµ¬ë¶„'].copy()  # ì›ë³¸ ë³´ì¡´
        self.df['íŒì •êµ¬ë¶„'] = self.df['íŒì •êµ¬ë¶„'].map(judgment_mapping).fillna('ê¸°íƒ€')
        
        # í†µí•© í›„ ë¶„í¬
        consolidated_counts = self.df['íŒì •êµ¬ë¶„'].value_counts()
        print("\nì •ë¦¬ í›„ ë¶„í¬:")
        print(consolidated_counts)
        
        # í†µí•© í†µê³„
        total_consolidated = len(self.df)
        print(f"\nğŸ“Š ì •ë¦¬ ê²°ê³¼:")
        for category, count in consolidated_counts.items():
            percentage = (count / total_consolidated) * 100
            print(f"   - {category}: {count:,}ê±´ ({percentage:.1f}%)")
        
        return self.df
    
    def _consolidate_reason_categories(self):
        """íŒì •ì‚¬ìœ  ì¹´í…Œê³ ë¦¬ ì •ë¦¬ (ìµœì†Œí•œì˜ í†µí•©ë§Œ)"""
        print("\nğŸ”„ íŒì •ì‚¬ìœ  ì¹´í…Œê³ ë¦¬ ì •ë¦¬ ì¤‘...")
        
        # ì›ë³¸ ë¶„í¬ (ìƒìœ„ 20ê°œ)
        original_counts = self.df['íŒì •ì‚¬ìœ '].value_counts()
        print("ì›ë³¸ ë¶„í¬ (ìƒìœ„ 20ê°œ):")
        print(original_counts.head(20))
        
        # ìµœì†Œí•œì˜ í†µí•©ë§Œ ìˆ˜í–‰ (ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ê²ƒë“¤ë§Œ)
        reason_mapping = {
            # ì§€ê¸‰ ê´€ë ¨
            'ì§€ê¸‰ íŒì •': 'ì§€ê¸‰íŒì •',
            'ê¸°íƒ€ ì§€ê¸‰ì‚¬ìœ ì˜ í•´ì†Œ': 'ê¸°íƒ€ì§€ê¸‰ì‚¬ìœ í•´ì†Œ',
            
            # ë©´ì±… ê´€ë ¨ - ì£¼ìš” ì‚¬ìœ ë“¤
            'ë³´í—˜ê³„ì•½ìì˜ ê³ ì˜ ë˜ëŠ” ê³¼ì‹¤ë¡œ ì¸í•˜ì—¬ ë°œìƒí•œ ì†ì‹¤': 'ê³ ì˜ê³¼ì‹¤',
            'ì—°ì†ìˆ˜ì¶œ': 'ì—°ì†ìˆ˜ì¶œ',
            'ë³´ìƒí•œë„ë¥¼ ì´ˆê³¼í•˜ëŠ” ì†ì‹¤': 'ë³´ìƒí•œë„ì´ˆê³¼',
            'ë³´í—˜ê´€ê³„ì˜ ì„±ë¦½': 'ë³´í—˜ê´€ê³„ì„±ë¦½',
            'ì£¼ì˜ì˜ë¬´ í•´íƒœë¡œ ì¸í•œ ì†ì‹¤ê°€ì¤‘': 'ì£¼ì˜ì˜ë¬´í•´íƒœ',
            'ì‹ ìš©ë³´ì¦ì¡°ê±´ ìœ„ë°˜': 'ì‹ ìš©ë³´ì¦ì¡°ê±´ìœ„ë°˜',
            'ì‹ ìš©ë³´ì¦ê´€ê³„ì˜ ì„±ë¦½': 'ì‹ ìš©ë³´ì¦ê´€ê³„ì„±ë¦½',
            'ê¶Œë¦¬ë³´ì „ì˜ë¬´ í•´íƒœë¡œ ì¸í•œ ì†ì‹¤ê°€ì¤‘': 'ê¶Œë¦¬ë³´ì „ì˜ë¬´í•´íƒœ',
            'ë³´í—˜ê³„ì•½ì˜ í•´ì§€(ê³ ì§€ì˜ë¬´, ë‚´ìš©ë³€ê²½, ë³´í—˜ë£Œ ë¯¸ë‚©)': 'ë³´í—˜ê³„ì•½í•´ì§€',
            'ë³´í—˜ê³„ì•½ì´ ê³µì‚¬ê°€ ì±…ì„ ì§€ìš¸ ìˆ˜ ì—†ëŠ” ì‚¬ìœ ë¡œ ë¬´íš¨ ì‹¤íš¨ í•´ì œ í•´ì§€': 'ë³´í—˜ê³„ì•½ë¬´íš¨',
            'ë³´í—˜ì¦ê¶Œ ìƒ íŠ¹ì•½ì‚¬í•­ í•´íƒœë¡œ ì¸í•œ ì†ì‹¤ê°€ì¤‘': 'íŠ¹ì•½ì‚¬í•­í•´íƒœ',
            
            # ì§€ê¸‰ìœ ì˜ˆ ê´€ë ¨
            'ì§€ê¸‰ìœ ì˜ˆ íŒì •': 'ì§€ê¸‰ìœ ì˜ˆíŒì •',
            'ì§€ê¸‰í•  ë³´í—˜ê¸ˆì„ ì‚°ì •í•˜ê¸° ìœ„í•˜ì—¬ ì¥ê¸°ê°„ì´ ì†Œìš”ë˜ëŠ” ê²½ìš°': 'ì¥ê¸°ê°„ì†Œìš”',
            'ì‚¬ê³ ì›ì¸ì˜ ì¡°ì‚¬ì— ì¥ê¸°ê°„ì´ ì†Œìš”ë˜ëŠ” ê²½ìš°': 'ì‚¬ê³ ì›ì¸ì¡°ì‚¬ì¥ê¸°ê°„',
            
            # ê¸°íƒ€ ì£¼ìš” ì‚¬ìœ ë“¤
            'ì ìš©ëŒ€ìƒ ìˆ˜ì¶œê±°ë˜': 'ì ìš©ëŒ€ìƒìˆ˜ì¶œê±°ë˜',
            'ë³€ì œì¶©ë‹¹': 'ë³€ì œì¶©ë‹¹',
            'ë³´í—˜ê³„ì•½ìì™€ ìˆ˜ì¶œê³„ì•½ìƒëŒ€ë°©ê°„ì— ë¶„ìŸë°œìƒ': 'ë¶„ìŸë°œìƒ',
            'ë³´ì¦ì±„ë¬´ì˜ ë²”ìœ„': 'ë³´ì¦ì±„ë¬´ë²”ìœ„',
            'ì‚¬ê³ ë°œìƒí†µì§€ ì˜ë¬´': 'ì‚¬ê³ ë°œìƒí†µì§€ì˜ë¬´',
            'ì‹ ìš©ë³´ì¦ëŒ€ìƒ ìˆ˜ì¶œê±°ë˜': 'ì‹ ìš©ë³´ì¦ëŒ€ìƒìˆ˜ì¶œê±°ë˜',
            'ì†ì‹¤ë°©ì§€ ê²½ê°ì˜ë¬´': 'ì†ì‹¤ë°©ì§€ê²½ê°ì˜ë¬´',
            'ë¬¼í’ˆì˜ ë©¸ì‹¤, í›¼ì† ë˜ëŠ” ê¸°íƒ€ ë¬¼í’ˆì— ëŒ€í•´ ë°œìƒí•œ ì†ì‹¤': 'ë¬¼í’ˆì†ì‹¤',
            'ë³¸ì§€ì‚¬ê±°ë˜ì—ì„œ ì‹ ìš©ìœ„í—˜ìœ¼ë¡œ ì¸í•˜ì—¬ ë°œìƒí•œ ì†ì‹¤': 'ë³¸ì§€ì‚¬ê±°ë˜ì†ì‹¤',
            'ë¬´ì‹ ìš©ì¥ë°©ì‹ ê±°ë˜ì—ì„œ ìˆ˜ì¶œê³„ì•½ì˜ ì£¼ìš” ì‚¬í•­ ìœ„ë°˜': 'ìˆ˜ì¶œê³„ì•½ìœ„ë°˜',
            'ë³´í—˜ì±…ì„ ê°œì‹œì¼ì „ì— ë°œìƒí•œ ì†ì‹¤': 'ì±…ì„ê°œì‹œì¼ì „ì†ì‹¤',
            'ê¸ˆìœµê³„ì•½ìƒì˜ ì˜ë¬´ì‚¬í•­ ë¶ˆì´í–‰': 'ê¸ˆìœµê³„ì•½ì˜ë¬´ë¶ˆì´í–‰',
            'ì§€ì‹œì— ë”°ë¥¼ ì˜ë¬´': 'ì§€ì‹œë”°ë¥¼ì˜ë¬´',
            'ìˆ˜ì¶œì±„ê¶Œ ê°ì†Œ(ìƒê³„, ì±„ë¬´ë©´ì œ ë“±)': 'ìˆ˜ì¶œì±„ê¶Œê°ì†Œ',
            'ë¬´ì‹ ìš©ì¥ë°©ì‹ê±°ë˜ì—ì„œ ìˆ˜ì…ìì˜ ì‹ ìš©ìƒíƒœì•…í™”ë¥¼ ì¸ì§€í•œ ì´í›„ ìˆ˜ì¶œê±°ë˜': 'ìˆ˜ì…ìì‹ ìš©ì•…í™”',
            'ì¡°ì‚¬ì— ë”°ë¥¼ ì˜ë¬´': 'ì¡°ì‚¬ë”°ë¥¼ì˜ë¬´',
            'ìƒê³„ì²˜ë¦¬': 'ìƒê³„ì²˜ë¦¬',
            'ì‚¬ìœ  ì—†ìŒ': 'ì‚¬ìœ ì—†ìŒ',
            'ë²•ë ¹ì„ ìœ„ë°˜í•˜ì—¬ ì·¨ë“í•œ ì±„ê¶Œ': 'ë²•ë ¹ìœ„ë°˜ì±„ê¶Œ',
            'ì±…ì„ê¸ˆì•¡ì„ ì´ˆê³¼í•˜ëŠ” ì†ì‹¤': 'ì±…ì„ê¸ˆì•¡ì´ˆê³¼',
            'ë³´í—˜ì‚¬ê³  ê´€ë ¨ ìˆ˜ì¶œë¬¼í’ˆì´ ì²˜ë¶„ë˜ì§€ ì•ŠëŠ” ê²½ìš°': 'ìˆ˜ì¶œë¬¼í’ˆë¯¸ì²˜ë¶„',
            'ì‹ ìš©ë³´ì¦ë¶€ëŒ€ì¶œì˜ ì‹¤í–‰ê¸ˆì§€': 'ì‹ ìš©ë³´ì¦ë¶€ëŒ€ì¶œì‹¤í–‰ê¸ˆì§€',
            'ì‹ ìš©ì¥ë°©ì‹ ë“±ì˜ ìˆ˜ì¶œê±°ë˜ì˜ ì •ì˜ì— ìœ„ë°°ë˜ëŠ” ì‹ ìš©ì¥ê±°ë˜': 'ì‹ ìš©ì¥ê±°ë˜ìœ„ë°°',
            'ì‹ ìš©ì¥ë°©ì‹ ê±°ë˜ì—ì„œ ì‹ ìš©ì¥ì¡°ê±´ ìœ„ë°˜': 'ì‹ ìš©ì¥ì¡°ê±´ìœ„ë°˜',
            'ì‹ ìš©ë³´ì¦ë¶€ëŒ€ì¶œì—ì˜ ìš°ì„ ì¶©ë‹¹': 'ì‹ ìš©ë³´ì¦ë¶€ëŒ€ì¶œìš°ì„ ì¶©ë‹¹',
            'ì ìš©ëŒ€ìƒê±°ë˜ ë° ë³´í—˜ê³„ì•½ì˜ ì„±ë¦½(ë¶€ë³´ëŒ€ìƒê±°ë˜, ì¡°ê±´ë¶€ ì‹ ìš©ì¥, ëŒ€ê¸ˆì§€ê¸‰ì±…ì„ ë©´ì œ ë“±)': 'ì ìš©ëŒ€ìƒê±°ë˜ì„±ë¦½',
            'ë³´í—˜ê³„ì•½ìì™€ ì¬íŒë§¤ê³„ì•½ìƒëŒ€ë°©ê°„ì— ë¶„ìŸë°œìƒ': 'ì¬íŒë§¤ê³„ì•½ë¶„ìŸ'
        }
        
        # ë§¤í•‘ ì ìš©
        self.df['íŒì •ì‚¬ìœ _original'] = self.df['íŒì •ì‚¬ìœ '].copy()  # ì›ë³¸ ë³´ì¡´
        self.df['íŒì •ì‚¬ìœ '] = self.df['íŒì •ì‚¬ìœ '].map(reason_mapping).fillna('ê¸°íƒ€ì‚¬ìœ ')
        
        # ì •ë¦¬ í›„ ë¶„í¬ (ìƒìœ„ 15ê°œ)
        consolidated_counts = self.df['íŒì •ì‚¬ìœ '].value_counts()
        print("\nì •ë¦¬ í›„ ë¶„í¬ (ìƒìœ„ 15ê°œ):")
        print(consolidated_counts.head(15))
        
        # ì •ë¦¬ í†µê³„
        total_consolidated = len(self.df)
        print(f"\nğŸ“Š íŒì •ì‚¬ìœ  ì •ë¦¬ ê²°ê³¼:")
        for category, count in consolidated_counts.head(10).items():
            percentage = (count / total_consolidated) * 100
            print(f"   - {category}: {count:,}ê±´ ({percentage:.1f}%)")
        
        return self.df
    
    def create_train_valid_test_split(self, test_size=0.2, valid_size=0.1, random_state=42):
        """
        ë°ì´í„°ë¥¼ Train/Valid/Testë¡œ ë¶„í•  (ê°œì„ ëœ ì¸µí™” ìƒ˜í”Œë§)
        """
        print(f"\nğŸ”„ ë°ì´í„° ë¶„í•  ì¤‘... (Train: {100-test_size*100-valid_size*100:.0f}% / Valid: {valid_size*100:.0f}% / Test: {test_size*100:.0f}%)")
        
        # íŒì •êµ¬ë¶„ ê¸°ì¤€ ì¸µí™” íƒ€ê²Ÿ ìƒì„±
        self.df = self._create_balanced_stratification_target()
        
        # í¬ì†Œ í´ë˜ìŠ¤(íŒì •êµ¬ë¶„) ì²˜ë¦¬: í…ŒìŠ¤íŠ¸ì—ì„œ ì œì™¸í•˜ê³  í•™ìŠµì—ëŠ” í¬í•¨
        counts = self.df['íŒì •êµ¬ë¶„'].value_counts()
        rare_classes = counts[counts < self.min_support_for_test].index.tolist()
        self.excluded_classes_from_test = rare_classes
        if rare_classes:
            print(f"âš ï¸ í¬ì†Œ í´ë˜ìŠ¤(í…ŒìŠ¤íŠ¸ ì œì™¸): {rare_classes}  (min_support_for_test={self.min_support_for_test})")
        
        base_df = self.df[~self.df['íŒì •êµ¬ë¶„'].isin(rare_classes)].copy()
        rare_df = self.df[self.df['íŒì •êµ¬ë¶„'].isin(rare_classes)].copy()
        
        # base_dfë§Œ stratifyë¡œ ë¶„í• 
        train_base, temp = train_test_split(
            base_df,
            test_size=(test_size + valid_size),
            random_state=random_state,
            stratify=base_df['stratify_target']
        )
        test_ratio = test_size / (test_size + valid_size)
        valid_base, test_base = train_test_split(
            temp,
            test_size=test_ratio,
            random_state=random_state,
            stratify=temp['stratify_target']
        )
        
        # í¬ì†Œ í´ë˜ìŠ¤ëŠ” trainìœ¼ë¡œë§Œ ë„£ìŒ(ë˜ëŠ” í•„ìš” ì‹œ validì— ì¼ë¶€ ë¶„ë°° ê°€ëŠ¥)
        self.train_df = pd.concat([train_base, rare_df], ignore_index=True)
        self.valid_df = valid_base.copy()
        self.test_df = test_base.copy()  # testì—ëŠ” í¬ì†Œ í´ë˜ìŠ¤ ì—†ìŒ
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ… ë¶„í•  ì™„ë£Œ:")
        print(f"   - Train: {len(self.train_df):,}ê±´ ({len(self.train_df)/len(self.df)*100:.1f}%)")
        print(f"   - Valid: {len(self.valid_df):,}ê±´ ({len(self.valid_df)/len(self.df)*100:.1f}%)")
        print(f"   - Test:  {len(self.test_df):,}ê±´ ({len(self.test_df)/len(self.df)*100:.1f}%)")
        
        print("\nğŸ“Š ì„¸íŠ¸ë³„ íŒì •êµ¬ë¶„ ë¶„í¬:")
        for name, dataset in [('Train', self.train_df), ('Valid', self.valid_df), ('Test', self.test_df)]:
            dist = dataset['íŒì •êµ¬ë¶„'].value_counts(normalize=True)
            print(f"{name}: {dict(dist.round(3))}")
        
        if self.excluded_classes_from_test:
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì œì™¸ í´ë˜ìŠ¤(í•™ìŠµì—ëŠ” í¬í•¨): {self.excluded_classes_from_test}")
        
        return self.train_df, self.valid_df, self.test_df
    
    def _create_balanced_stratification_target(self):
        """ê· í˜•ì¡íŒ ì¸µí™” ìƒ˜í”Œë§ì„ ìœ„í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
        print("\nğŸ¯ ê· í˜•ì¡íŒ ì¸µí™” ìƒ˜í”Œë§ íƒ€ê²Ÿ ìƒì„± ì¤‘...")
        
        # 1. íŒì •êµ¬ë¶„ë³„ ë¶„í¬ í™•ì¸
        judgment_counts = self.df['íŒì •êµ¬ë¶„'].value_counts()
        print("íŒì •êµ¬ë¶„ë³„ ë¶„í¬:")
        print(judgment_counts)
        
        # 2. ì£¼ìš” íŒì •ì‚¬ìœ ë³„ ë¶„í¬ í™•ì¸ (ìƒìœ„ 10ê°œ)
        reason_counts = self.df['íŒì •ì‚¬ìœ '].value_counts()
        print("\nì£¼ìš” íŒì •ì‚¬ìœ ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        print(reason_counts.head(10))
        
        # 3. ê· í˜•ì¡íŒ ì¸µí™” íƒ€ê²Ÿ ìƒì„± (íŒì •êµ¬ë¶„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”)
        # íŒì •êµ¬ë¶„ì´ ì—…ë¬´ì ìœ¼ë¡œ ë” ì¤‘ìš”í•˜ë¯€ë¡œ ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¸µí™”
        self.df['stratify_target'] = self.df['íŒì •êµ¬ë¶„']
        
        # 4. ì¸µí™” íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
        final_counts = self.df['stratify_target'].value_counts()
        print(f"\nğŸ“Š ì¸µí™” íƒ€ê²Ÿ ë¶„í¬ (íŒì •êµ¬ë¶„ ê¸°ë°˜):")
        for category, count in final_counts.items():
            percentage = (count / len(self.df)) * 100
            print(f"   - {category}: {count:,}ê±´ ({percentage:.1f}%)")
        
        # 5. ê· í˜•ë„ í‰ê°€
        min_count = final_counts.min()
        max_count = final_counts.max()
        balance_ratio = min_count / max_count if max_count > 0 else 0
        print(f"\nâœ… ì¸µí™” íƒ€ê²Ÿ ê· í˜•ë„: {balance_ratio:.3f} (ìµœì†Œ: {min_count}, ìµœëŒ€: {max_count})")
        
        return self.df
    
    def prepare_features_for_modeling(self):
        """ëª¨ë¸ë§ì„ ìœ„í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        print("\nğŸ› ï¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        categorical_features = ['ìˆ˜ì…êµ­', 'ì‚¬ê³ ìœ í˜•ëª…', 'ë³´í—˜ì¢…ëª©', 'ìƒí’ˆë¶„ë¥˜ëª…', 'ê²°ì œë°©ë²•']
        
        # Train ë°ì´í„°ë¡œ Label Encoder í•™ìŠµ
        for feature in categorical_features:
            if feature in self.train_df.columns:
                le = LabelEncoder()
                
                # ê²°ì¸¡ê°’ ì²˜ë¦¬
                train_values = self.train_df[feature].fillna('Unknown').astype(str)
                le.fit(train_values)
                self.label_encoders[feature] = le
                
                # ëª¨ë“  ë°ì´í„°ì…‹ì— ì ìš© (ë¯¸ì§€ ì¹´í…Œê³ ë¦¬ëŠ” 'Unknown' ì½”ë“œë¡œ ëŒ€ì²´)
                for df_name, df in [('train', self.train_df), ('valid', self.valid_df), ('test', self.test_df)]:
                    values = df[feature].fillna('Unknown').astype(str)

                    # LabelEncoderì— 'Unknown' í´ë˜ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ (ì •ë ¬ ë³´ì¥)
                    if 'Unknown' not in le.classes_:
                        le.classes_ = np.sort(np.append(le.classes_, 'Unknown'))

                    # í´ë°± ì½”ë“œ(Unknown)
                    unknown_code = le.transform(['Unknown'])[0]

                    # ì•Œë ¤ì§„ ê°’ ë§ˆìŠ¤í¬
                    known_mask = values.isin(le.classes_)

                    # ì „ë¶€ Unknown ì½”ë“œë¡œ ì±„ìš´ í›„, ì•Œë ¤ì§„ ê°’ë§Œ ë³€í™˜í•´ì„œ ë®ì–´ì”€
                    encoded_values = np.full(len(values), unknown_code)
                    if known_mask.any():
                        encoded_values[known_mask] = le.transform(values[known_mask])

                    df[f'{feature}_encoded'] = encoded_values
                    
                print(f"   - {feature}: {len(le.classes_)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        # ìˆ«ìí˜• í”¼ì²˜ ì •ê·œí™”
        numeric_features = ['ì›í™”ì‚¬ê³ ê¸ˆì•¡', 'ë¶€ë³´ìœ¨']
        for feature in numeric_features:
            if feature in self.train_df.columns:
                # Train ë°ì´í„°ë¡œ ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚°
                train_mean = self.train_df[feature].mean()
                train_std = self.train_df[feature].std()

                # ë¶„ì‚° 0 ë°©ì–´ (ì •ê·œí™” ë¶ˆê°€ ì‹œ 0ìœ¼ë¡œ ì„¤ì •)
                if pd.isna(train_std) or train_std == 0:
                    for df_name, df in [('train', self.train_df), ('valid', self.valid_df), ('test', self.test_df)]:
                        df[f'{feature}_normalized'] = 0.0
                    print(f"   - {feature}: í‘œì¤€í¸ì°¨ 0 â†’ ì •ê·œí™” ìƒëµ(0ìœ¼ë¡œ ì„¤ì •)")
                    continue

                # ëª¨ë“  ë°ì´í„°ì…‹ì— ì ìš©
                for df_name, df in [('train', self.train_df), ('valid', self.valid_df), ('test', self.test_df)]:
                    df[f'{feature}_normalized'] = (df[feature] - train_mean) / train_std

                print(f"   - {feature}: ì •ê·œí™” ì™„ë£Œ (í‰ê· : {train_mean:.2f}, í‘œì¤€í¸ì°¨: {train_std:.2f})")
    
    def evaluate_similarity_system(self, similarity_system, sample_size=300):
        """
        ìœ ì‚¬ë„ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ (ë‹¤ì¤‘ íƒ€ê²Ÿ í‰ê°€)
        
        Args:
            similarity_system: í‰ê°€í•  ìœ ì‚¬ë„ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
            sample_size: í‰ê°€ì— ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜
        """
        print(f"\nğŸ” ìœ ì‚¬ë„ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ (ìƒ˜í”Œ í¬ê¸°: {sample_size})")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ ìƒ˜í”Œ ì¶”ì¶œ
        test_sample = self._get_stratified_test_sample(sample_size)
        
        # ë‹¤ì¤‘ íƒ€ê²Ÿ í‰ê°€ë¥¼ ìœ„í•œ ê²°ê³¼ ì €ì¥
        results = {
            'judgment': {'predictions': [], 'actuals': [], 'similarity_scores': [], 'confidence_scores': []},
            'reason': {'predictions': [], 'actuals': [], 'similarity_scores': [], 'confidence_scores': []}
        }
        
        for idx, test_case in test_sample.iterrows():
            try:
                # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì¿¼ë¦¬ë¡œ ë³€í™˜
                case_data = {
                    'ìˆ˜ì…êµ­': test_case['ìˆ˜ì…êµ­'],
                    'ë³´í—˜ì¢…ëª©': test_case['ë³´í—˜ì¢…ëª©'],
                    'ì‚¬ê³ ìœ í˜•ëª…': test_case['ì‚¬ê³ ìœ í˜•ëª…'],
                    'ì›í™”ì‚¬ê³ ê¸ˆì•¡': test_case['ì›í™”ì‚¬ê³ ê¸ˆì•¡'],
                    'ì‚¬ê³ ì„¤ëª…': test_case['ì‚¬ê³ ì„¤ëª…']
                }
                
                # Train ë°ì´í„°ì—ì„œ ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰ (ìê¸° ìì‹  ì œì™¸)
                search_df = self.train_df[self.train_df.index != idx].copy()
                
                # ìœ ì‚¬ë„ ê³„ì‚°
                similarities = similarity_system.calculate_similarity_scores(case_data, search_df.head(1000))
                
                if similarities:
                    # ìƒìœ„ 5ê°œ ìœ ì‚¬ì‚¬ë¡€ ë¶„ì„
                    top_5 = similarities[:5]

                    # ê°€ì¤‘ ë‹¤ìˆ˜ê²°: ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ í•©ì‚°í•˜ì—¬ ìµœë‹¤ ê°€ì¤‘ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡
                    def weighted_vote(label_list, weights):
                        scores = {}
                        for lbl, w in zip(label_list, weights):
                            scores[lbl] = scores.get(lbl, 0.0) + float(w)
                        # ìµœëŒ€ ê°€ì¤‘ì¹˜ ë¼ë²¨ ë°˜í™˜
                        return max(scores.items(), key=lambda x: x[1])[0], scores

                    top_scores = [case[0] for case in top_5]
                    judgment_labels = [case[3]['íŒì •êµ¬ë¶„'] for case in top_5]
                    reason_labels = [case[3]['íŒì •ì‚¬ìœ '] for case in top_5]

                    pred_judgment, judgment_score_map = weighted_vote(judgment_labels, top_scores)
                    pred_reason, reason_score_map = weighted_vote(reason_labels, top_scores)

                    # ë©´ì±… ì˜¤ë²„ë¼ì´ë“œ: ìµœìƒìœ„ 1ê±´ì´ ë©´ì±…ì´ê³  ì¢…í•©ìœ ì‚¬ë„ ì„ê³„ì¹˜ ì´ìƒì´ë©´ ë©´ì±…ìœ¼ë¡œ ê³ ì •
                    top1_score, _, _, top1_case = top_5[0]
                    if top1_case['íŒì •êµ¬ë¶„'] == 'ë©´ì±…' and top1_score >= 0.65:
                        pred_judgment = 'ë©´ì±…'

                    # ì‹ ë¢°ë„: ê°€ì¤‘ í•© ì¤‘ ì˜ˆì¸¡ ë¼ë²¨ ë¹„ìœ¨
                    sum_w = sum(top_scores) if top_scores else 1.0
                    judgment_confidence = (judgment_score_map.get(pred_judgment, 0.0) / sum_w) if sum_w else 0.0
                    reason_confidence = (reason_score_map.get(pred_reason, 0.0) / sum_w) if sum_w else 0.0

                    # í‰ê·  ìœ ì‚¬ë„
                    avg_similarity = np.mean(top_scores) if top_scores else 0.0
                    
                    # ê²°ê³¼ ì €ì¥
                    results['judgment']['predictions'].append(pred_judgment)
                    results['judgment']['actuals'].append(test_case['íŒì •êµ¬ë¶„'])
                    results['judgment']['similarity_scores'].append(avg_similarity)
                    results['judgment']['confidence_scores'].append(judgment_confidence)
                    
                    results['reason']['predictions'].append(pred_reason)
                    results['reason']['actuals'].append(test_case['íŒì •ì‚¬ìœ '])
                    results['reason']['similarity_scores'].append(avg_similarity)
                    results['reason']['confidence_scores'].append(reason_confidence)
                    
            except Exception as e:
                print(f"   âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {e}")
                continue
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        if results['judgment']['predictions'] and results['reason']['predictions']:
            final_results = self._calculate_multi_target_metrics(results)
            self._print_multi_target_results(final_results)
            return final_results
        else:
            print("âŒ í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def _calculate_multi_target_metrics(self, results):
        """ë‹¤ì¤‘ íƒ€ê²Ÿ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, f1_score
        from sklearn.metrics import precision_recall_fscore_support
        
        final_results = {}
        
        for target_type in ['judgment', 'reason']:
            predictions = results[target_type]['predictions']
            actuals = results[target_type]['actuals']
            similarity_scores = results[target_type]['similarity_scores']
            confidence_scores = results[target_type]['confidence_scores']
            
            # ê¸°ë³¸ ì •í™•ë„
            accuracy = sum(p == a for p, a in zip(predictions, actuals)) / len(predictions)
            
            # ê· í˜•ì¡íŒ ì •í™•ë„
            balanced_acc = balanced_accuracy_score(actuals, predictions)
            
            # F1 ìŠ¤ì½”ì–´
            f1_macro = f1_score(actuals, predictions, average='macro', zero_division=0)
            f1_micro = f1_score(actuals, predictions, average='micro', zero_division=0)
            
            # í˜¼ë™ í–‰ë ¬
            unique_labels = sorted(list(set(actuals + predictions)))
            cm = confusion_matrix(actuals, predictions, labels=unique_labels)
            
            # ë¶„ë¥˜ ë¦¬í¬íŠ¸
            report = classification_report(actuals, predictions, target_names=unique_labels, zero_division=0, output_dict=True)
            
            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
            class_performance = {}
            for label in unique_labels:
                if label in report:
                    class_performance[label] = {
                        'precision': report[label]['precision'],
                        'recall': report[label]['recall'],
                        'f1-score': report[label]['f1-score'],
                        'support': report[label]['support']
                    }
            
            # í†µê³„
            similarity_stats = {
                'mean': np.mean(similarity_scores),
                'std': np.std(similarity_scores),
                'min': np.min(similarity_scores),
                'max': np.max(similarity_scores)
            }
            
            confidence_stats = {
                'mean': np.mean(confidence_scores),
                'std': np.std(confidence_scores),
                'min': np.min(confidence_scores),
                'max': np.max(confidence_scores)
            }

            # ë©´ì±… ì¤‘ì‹¬ ë°”ì´ë„ˆë¦¬ ì§€í‘œ (íŒì •êµ¬ë¶„ì¼ ë•Œë§Œ ê³„ì‚°)
            exemption_metrics = None
            if target_type == 'judgment' and predictions and actuals:
                y_true = [1 if a == 'ë©´ì±…' else 0 for a in actuals]
                y_pred = [1 if p == 'ë©´ì±…' else 0 for p in predictions]
                prec, rec, f1_bin, support_pos = precision_recall_fscore_support(
                    y_true, y_pred, average='binary', zero_division=0
                )
                exemption_metrics = {
                    'precision': float(prec),
                    'recall': float(rec),
                    'f1': float(f1_bin),
                    'positive_support': int(sum(y_true))
                }
            
            final_results[target_type] = {
                'accuracy': accuracy,
                'balanced_accuracy': balanced_acc,
                'f1_macro': f1_macro,
                'f1_micro': f1_micro,
                'predictions': predictions,
                'actuals': actuals,
                'similarity_scores': similarity_scores,
                'confidence_scores': confidence_scores,
                'confusion_matrix': cm,
                'classification_report': report,
                'class_performance': class_performance,
                'similarity_stats': similarity_stats,
                'confidence_stats': confidence_stats,
                'exemption_metrics': exemption_metrics,
                'sample_size': len(predictions)
            }
        
        return final_results
    
    def _print_multi_target_results(self, results):
        """ë‹¤ì¤‘ íƒ€ê²Ÿ í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ“ˆ ë‹¤ì¤‘ íƒ€ê²Ÿ ì„±ëŠ¥ ê²°ê³¼:")
        print("=" * 60)
        
        # 1. íŒì •êµ¬ë¶„ í‰ê°€ ê²°ê³¼
        print(f"\nğŸ¯ íŒì •êµ¬ë¶„ ì˜ˆì¸¡ ì„±ëŠ¥:")
        judgment = results['judgment']
        print(f"   - ì •í™•ë„: {judgment['accuracy']:.3f} ({judgment['accuracy']*100:.1f}%)")
        print(f"   - ê· í˜•ì¡íŒ ì •í™•ë„: {judgment['balanced_accuracy']:.3f} ({judgment['balanced_accuracy']*100:.1f}%)")
        print(f"   - F1 ìŠ¤ì½”ì–´ (Macro): {judgment['f1_macro']:.3f}")
        print(f"   - F1 ìŠ¤ì½”ì–´ (Micro): {judgment['f1_micro']:.3f}")
        print(f"   - í‰ê°€ ìƒ˜í”Œ ìˆ˜: {judgment['sample_size']}ê°œ")
        print(f"   - í‰ê·  ìœ ì‚¬ë„: {judgment['similarity_stats']['mean']:.3f} Â± {judgment['similarity_stats']['std']:.3f}")
        print(f"   - í‰ê·  ì‹ ë¢°ë„: {judgment['confidence_stats']['mean']:.3f} Â± {judgment['confidence_stats']['std']:.3f}")
        if judgment.get('exemption_metrics'):
            em = judgment['exemption_metrics']
            print(f"   - ë©´ì±… íƒì§€(ì´ì§„) Precision: {em['precision']:.3f}, Recall: {em['recall']:.3f}, F1: {em['f1']:.3f} (ë©´ì±… ìˆ˜: {em['positive_support']})")
        
        # íŒì •êµ¬ë¶„ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        print(f"\nğŸ“Š íŒì •êµ¬ë¶„ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        for label, perf in judgment['class_performance'].items():
            print(f"   - {label}:")
            print(f"     â€¢ Precision: {perf['precision']:.3f}")
            print(f"     â€¢ Recall: {perf['recall']:.3f}")
            print(f"     â€¢ F1-Score: {perf['f1-score']:.3f}")
            print(f"     â€¢ Support: {perf['support']}ê±´")
        
        # 2. íŒì •ì‚¬ìœ  í‰ê°€ ê²°ê³¼
        print(f"\nğŸ“‹ íŒì •ì‚¬ìœ  ì˜ˆì¸¡ ì„±ëŠ¥:")
        reason = results['reason']
        print(f"   - ì •í™•ë„: {reason['accuracy']:.3f} ({reason['accuracy']*100:.1f}%)")
        print(f"   - ê· í˜•ì¡íŒ ì •í™•ë„: {reason['balanced_accuracy']:.3f} ({reason['balanced_accuracy']*100:.1f}%)")
        print(f"   - F1 ìŠ¤ì½”ì–´ (Macro): {reason['f1_macro']:.3f}")
        print(f"   - F1 ìŠ¤ì½”ì–´ (Micro): {reason['f1_micro']:.3f}")
        print(f"   - í‰ê°€ ìƒ˜í”Œ ìˆ˜: {reason['sample_size']}ê°œ")
        print(f"   - í‰ê·  ìœ ì‚¬ë„: {reason['similarity_stats']['mean']:.3f} Â± {reason['similarity_stats']['std']:.3f}")
        print(f"   - í‰ê·  ì‹ ë¢°ë„: {reason['confidence_stats']['mean']:.3f} Â± {reason['confidence_stats']['std']:.3f}")
        
        # íŒì •ì‚¬ìœ  í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (ìƒìœ„ 10ê°œë§Œ)
        print(f"\nğŸ“Š íŒì •ì‚¬ìœ  í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (ìƒìœ„ 10ê°œ):")
        sorted_reasons = sorted(reason['class_performance'].items(), 
                              key=lambda x: x[1]['support'], reverse=True)[:10]
        for label, perf in sorted_reasons:
            print(f"   - {label}:")
            print(f"     â€¢ Precision: {perf['precision']:.3f}")
            print(f"     â€¢ Recall: {perf['recall']:.3f}")
            print(f"     â€¢ F1-Score: {perf['f1-score']:.3f}")
            print(f"     â€¢ Support: {perf['support']}ê±´")
        
        # 3. ì¢…í•© í‰ê°€
        print(f"\nğŸ¯ ì¢…í•© í‰ê°€:")
        print(f"   - íŒì •êµ¬ë¶„ ì˜ˆì¸¡ ì •í™•ë„: {judgment['accuracy']:.3f}")
        print(f"   - íŒì •ì‚¬ìœ  ì˜ˆì¸¡ ì •í™•ë„: {reason['accuracy']:.3f}")
        print(f"   - í‰ê·  ì •í™•ë„: {(judgment['accuracy'] + reason['accuracy']) / 2:.3f}")
        
        # í˜¼ë™ í–‰ë ¬ (íŒì •êµ¬ë¶„ë§Œ í‘œì‹œ - íŒì •ì‚¬ìœ ëŠ” ë„ˆë¬´ ë§ìŒ)
        print(f"\nğŸ“Š íŒì •êµ¬ë¶„ í˜¼ë™ í–‰ë ¬:")
        unique_labels = sorted(list(set(judgment['actuals'] + judgment['predictions'])))
        cm_df = pd.DataFrame(judgment['confusion_matrix'], index=unique_labels, columns=unique_labels)
        print(cm_df)
    
    def _get_stratified_test_sample(self, sample_size):
        """ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì¶”ì¶œ"""
        print(f"ğŸ“Š ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...")
        
        # íŒì •êµ¬ë¶„ë³„ ë¶„í¬ í™•ì¸
        judgment_counts = self.test_df['íŒì •êµ¬ë¶„'].value_counts()
        print("í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒì •êµ¬ë¶„ ë¶„í¬:")
        print(judgment_counts)
        
        # ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ ê· í˜•ì¡íŒ ìƒ˜í”Œ ì¶”ì¶œ
        stratified_sample = []
        
        for judgment in judgment_counts.index:
            judgment_data = self.test_df[self.test_df['íŒì •êµ¬ë¶„'] == judgment]
            judgment_count = len(judgment_data)
            
            # ê° íŒì •êµ¬ë¶„ë³„ë¡œ ë¹„ë¡€í•˜ì—¬ ìƒ˜í”Œ ì¶”ì¶œ
            if judgment_count > 0:
                # ìµœì†Œ 5ê°œ, ìµœëŒ€ ì „ì²´ì˜ 50%ê¹Œì§€ ì¶”ì¶œ
                min_samples = min(5, judgment_count)
                max_samples = min(int(judgment_count * 0.5), judgment_count)
                target_samples = min(max_samples, max(min_samples, int(sample_size * judgment_count / len(self.test_df))))
                
                if target_samples > 0:
                    sampled = judgment_data.sample(n=target_samples, random_state=42)
                    stratified_sample.append(sampled)
                    print(f"   - {judgment}: {target_samples}ê°œ ì¶”ì¶œ (ì „ì²´: {judgment_count}ê°œ)")
        
        if stratified_sample:
            final_sample = pd.concat(stratified_sample, ignore_index=True)
            print(f"âœ… ì´ {len(final_sample)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ")
            return final_sample
        else:
            print("âš ï¸ ì¸µí™” ìƒ˜í”Œë§ ì‹¤íŒ¨, ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´")
            return self.test_df.sample(n=min(sample_size, len(self.test_df)), random_state=42)
    
    def _calculate_comprehensive_metrics(self, predictions, actuals, similarity_scores, confidence_scores):
        """í¬ê´„ì ì¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, f1_score
        
        # ê¸°ë³¸ ì •í™•ë„
        accuracy = sum(p == a for p, a in zip(predictions, actuals)) / len(predictions)
        
        # ê· í˜•ì¡íŒ ì •í™•ë„ (ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©)
        balanced_acc = balanced_accuracy_score(actuals, predictions)
        
        # F1 ìŠ¤ì½”ì–´ (ë§ˆí¬ë¡œ í‰ê· )
        f1_macro = f1_score(actuals, predictions, average='macro', zero_division=0)
        
        # F1 ìŠ¤ì½”ì–´ (ë§ˆì´í¬ë¡œ í‰ê· )
        f1_micro = f1_score(actuals, predictions, average='micro', zero_division=0)
        
        # í˜¼ë™ í–‰ë ¬
        unique_labels = sorted(list(set(actuals + predictions)))
        cm = confusion_matrix(actuals, predictions, labels=unique_labels)
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        report = classification_report(actuals, predictions, target_names=unique_labels, zero_division=0, output_dict=True)
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        class_performance = {}
        for label in unique_labels:
            if label in report:
                class_performance[label] = {
                    'precision': report[label]['precision'],
                    'recall': report[label]['recall'],
                    'f1-score': report[label]['f1-score'],
                    'support': report[label]['support']
                }
        
        # ìœ ì‚¬ë„ ë° ì‹ ë¢°ë„ í†µê³„
        similarity_stats = {
            'mean': np.mean(similarity_scores),
            'std': np.std(similarity_scores),
            'min': np.min(similarity_scores),
            'max': np.max(similarity_scores)
        }
        
        confidence_stats = {
            'mean': np.mean(confidence_scores),
            'std': np.std(confidence_scores),
            'min': np.min(confidence_scores),
            'max': np.max(confidence_scores)
        }
        
        return {
            'accuracy': accuracy,
            'balanced_accuracy': balanced_acc,
            'f1_macro': f1_macro,
            'f1_micro': f1_micro,
            'predictions': predictions,
            'actuals': actuals,
            'similarity_scores': similarity_scores,
            'confidence_scores': confidence_scores,
            'confusion_matrix': cm,
            'classification_report': report,
            'class_performance': class_performance,
            'similarity_stats': similarity_stats,
            'confidence_stats': confidence_stats,
            'sample_size': len(predictions)
        }
    
    def _print_evaluation_results(self, results):
        """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ê²°ê³¼:")
        print(f"   - ì •í™•ë„: {results['accuracy']:.3f} ({results['accuracy']*100:.1f}%)")
        print(f"   - ê· í˜•ì¡íŒ ì •í™•ë„: {results['balanced_accuracy']:.3f} ({results['balanced_accuracy']*100:.1f}%)")
        print(f"   - F1 ìŠ¤ì½”ì–´ (Macro): {results['f1_macro']:.3f}")
        print(f"   - F1 ìŠ¤ì½”ì–´ (Micro): {results['f1_micro']:.3f}")
        print(f"   - í‰ê°€ ìƒ˜í”Œ ìˆ˜: {results['sample_size']}ê°œ")
        print(f"   - í‰ê·  ìœ ì‚¬ë„: {results['similarity_stats']['mean']:.3f} Â± {results['similarity_stats']['std']:.3f}")
        print(f"   - í‰ê·  ì‹ ë¢°ë„: {results['confidence_stats']['mean']:.3f} Â± {results['confidence_stats']['std']:.3f}")
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        print(f"\nğŸ“Š í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        for label, perf in results['class_performance'].items():
            print(f"   - {label}:")
            print(f"     â€¢ Precision: {perf['precision']:.3f}")
            print(f"     â€¢ Recall: {perf['recall']:.3f}")
            print(f"     â€¢ F1-Score: {perf['f1-score']:.3f}")
            print(f"     â€¢ Support: {perf['support']}ê±´")
        
        # í˜¼ë™ í–‰ë ¬
        print(f"\nğŸ“Š í˜¼ë™ í–‰ë ¬:")
        unique_labels = sorted(list(set(results['actuals'] + results['predictions'])))
        cm_df = pd.DataFrame(results['confusion_matrix'], index=unique_labels, columns=unique_labels)
        print(cm_df)
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        report_df = pd.DataFrame(results['classification_report']).transpose()
        print(report_df.round(3))
    
    def save_splits(self, save_dir='evaluation_data'):
        """ë¶„í• ëœ ë°ì´í„°ì…‹ ì €ì¥"""
        print(f"\nğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘... ({save_dir})")
        
        os.makedirs(save_dir, exist_ok=True)
        
        # ë°ì´í„°ì…‹ ì €ì¥
        self.train_df.to_csv(f'{save_dir}/train_data.csv', index=False, encoding='utf-8-sig')
        self.valid_df.to_csv(f'{save_dir}/valid_data.csv', index=False, encoding='utf-8-sig')
        self.test_df.to_csv(f'{save_dir}/test_data.csv', index=False, encoding='utf-8-sig')
        
        # Label Encoder ì €ì¥
        with open(f'{save_dir}/label_encoders.pkl', 'wb') as f:
            pickle.dump(self.label_encoders, f)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'creation_date': datetime.now().isoformat(),
            'total_samples': len(self.df),
            'train_samples': len(self.train_df),
            'valid_samples': len(self.valid_df),
            'test_samples': len(self.test_df),
            'features': list(self.label_encoders.keys())
        }
        
        with open(f'{save_dir}/metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ:")
        print(f"   - Train: {save_dir}/train_data.csv")
        print(f"   - Valid: {save_dir}/valid_data.csv")
        print(f"   - Test: {save_dir}/test_data.csv")
        print(f"   - Encoders: {save_dir}/label_encoders.pkl")
        print(f"   - Metadata: {save_dir}/metadata.pkl")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë³´í—˜ì‚¬ê³  ìœ ì‚¬ë„ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
    
    # í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    eval_system = InsuranceEvaluationSystem('data/design.csv')
    
    # ë°ì´í„° ë¡œë“œ
    eval_system.load_and_prepare_data()
    
    # Train/Valid/Test ë¶„í• 
    eval_system.create_train_valid_test_split()
    
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    eval_system.prepare_features_for_modeling()
    
    # ë¶„í• ëœ ë°ì´í„° ì €ì¥
    eval_system.save_splits()
    
    print("\nâœ… í‰ê°€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("ğŸ’¡ ì´ì œ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print("   1. train_data.csvë¡œ ëª¨ë¸ í•™ìŠµ")
    print("   2. valid_data.csvë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("   3. test_data.csvë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€")
    print("\nğŸ”§ ìœ ì‚¬ë„ ì‹œìŠ¤í…œ í‰ê°€ ì˜ˆì‹œ:")
    print("   eval_result = eval_system.evaluate_similarity_system(your_similarity_system)")

if __name__ == "__main__":
    main()