import numpy as np
# NumPy 2.0 compatibility aliases for removed dtypes
if not hasattr(np, "unicode_"):
    np.unicode_ = np.str_
if not hasattr(np, "string_"):
    np.string_ = np.bytes_
try:
    np.bool
except AttributeError:
    np.bool = np.bool_
try:
    np.object
except AttributeError:
    np.object = np.object_
try:
    np.int
except AttributeError:
    np.int = int
try:
    np.float
except AttributeError:
    np.float = float

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import torch
from collections import Counter
from scipy import stats
import pickle
import warnings
import re
import time
warnings.filterwarnings('ignore')

# ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï
st.set_page_config(
    page_title=" ÏÇ¨Í≥†Î≥Ñ ÌåêÏ†ïÏã¨ÏÇ¨ ÏÇ¨Î°Ä Î∂ÑÏÑù",
    page_icon="üìã",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ÍπîÎÅîÌïú CSS Ïä§ÌÉÄÏùº
st.markdown("""
<style>
    .main-header {
        background-color: #f8f9fa;
        padding: 2rem;
        border-radius: 10px;
        border-left: 5px solid #007bff;
        margin-bottom: 2rem;
    }
    
    .metric-box {
        background-color: white;
        padding: 1.5rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #007bff;
        text-align: center;
    }
    
    .confidence-box {
        background-color: #e3f2fd;
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px solid #2196f3;
        margin: 1rem 0;
    }
    
    .success-box {
        background-color: #e8f5e8;
        border: 2px solid #4caf50;
    }
    
    .error-box {
        background-color: #ffebee;
        border: 2px solid #f44336;
    }
    
    .warning-box {
        background-color: #fff3e0;
        border: 2px solid #ff9800;
    }
</style>
""", unsafe_allow_html=True)

class CountryProcessor:
    """Í∞úÏÑ†Îêú Íµ≠Í∞Ä Ï≤òÎ¶¨ ÌÅ¥ÎûòÏä§"""
    
    def __init__(self):
        # Í∞úÎ≥Ñ Ïú†ÏßÄ Íµ≠Í∞Ä (ÏÉÅÏúÑ 16Í∞ú)
        self.individual_countries = [
            'ÎØ∏Íµ≠', 'Ï§ëÍµ≠', 'Î∏åÎùºÏßà', 'ÏùºÎ≥∏', 'ÎèÖÏùº', 'ÏòÅÍµ≠', 'Îü¨ÏãúÏïÑ', 'Ïù∏ÎèÑ',
            'Î≤†Ìä∏ÎÇ®', 'Ïù¥ÌÉàÎ¶¨ÏïÑ', 'ÌôçÏΩ©', 'ÏïÑÎûçÏóêÎØ∏Î¶¨Ìä∏Ïó∞Ìï©', 'ÌäÄÎ•¥ÌÇ§Ïòà', 
            'Ïù∏ÎèÑÎÑ§ÏãúÏïÑ', 'Ïä§ÌéòÏù∏', 'ÎåÄÎßå'
        ]
        
        # ÏßÄÏó≠Î≥Ñ Î∂ÑÎ•ò
        self.regions = {
            'asia': {
                'Í∞úÎ≥Ñ': ['Ï§ëÍµ≠', 'ÏùºÎ≥∏', 'Ïù∏ÎèÑ', 'Î≤†Ìä∏ÎÇ®', 'ÌôçÏΩ©', 'ÎåÄÎßå', 'Ïù∏ÎèÑÎÑ§ÏãúÏïÑ'],
                'Í∏∞ÌÉÄ': ['ÌÉúÍµ≠', 'ÎßêÎ†àÏù¥ÏãúÏïÑ', 'ÌïÑÎ¶¨ÌïÄ', 'Ïã±Í∞ÄÌè¨Î•¥', 'ÎØ∏ÏñÄÎßà', 'Ï∫ÑÎ≥¥ÎîîÏïÑ', 'Î™ΩÍ≥®', 'Î∂ÄÌÉÑ']
            },
            'europe': {
                'Í∞úÎ≥Ñ': ['ÎèÖÏùº', 'ÏòÅÍµ≠', 'Ïù¥ÌÉàÎ¶¨ÏïÑ', 'Ïä§ÌéòÏù∏'],
                'Í∏∞ÌÉÄ': ['ÌîÑÎûëÏä§', 'ÎÑ§ÎçúÎûÄÎìú', 'Î≤®Í∏∞ÏóÑ', 'Ïä§ÏúÑÏä§', 'Ïò§Ïä§Ìä∏Î¶¨ÏïÑ', 'Í∑∏Î¶¨Ïä§', 'Î™∞ÌÉÄ', 'ÌóùÍ∞ÄÎ¶¨', 'Ï≤¥ÏΩî', 'Estonia']
            },
            'americas': {
                'Í∞úÎ≥Ñ': ['ÎØ∏Íµ≠', 'Î∏åÎùºÏßà'],
                'Í∏∞ÌÉÄ': ['Î©ïÏãúÏΩî', 'ÏΩúÎ°¨ÎπÑÏïÑ', 'ÏïÑÎ•¥Ìó®Ìã∞ÎÇò', 'ÌéòÎ£®', 'Ïπ†Î†à', 'Í≥ºÌÖåÎßêÎùº', 'Î≥ºÎ¶¨ÎπÑÏïÑ', 'Ïò®ÎëêÎùºÏä§', 'ÌååÎÇòÎßà', 'ÏûêÎ©îÏù¥Ïπ¥']
            },
            'middle_east': {
                'Í∞úÎ≥Ñ': ['ÏïÑÎûçÏóêÎØ∏Î¶¨Ìä∏Ïó∞Ìï©', 'ÌäÄÎ•¥ÌÇ§Ïòà'],
                'Í∏∞ÌÉÄ': ['ÏÇ¨Ïö∞ÎîîÏïÑÎùºÎπÑÏïÑ', 'Ïπ¥ÌÉÄÎ•¥', 'Ïø†Ïõ®Ïù¥Ìä∏', 'Î∞îÎ†àÏù∏', 'Ïù¥Ïä§ÎùºÏóò']
            },
            'africa': {
                'Í∞úÎ≥Ñ': [],
                'Í∏∞ÌÉÄ': ['Í∞ÄÎÇò', 'ÏºÄÎÉê', 'ÏÑ∏ÎÑ§Í∞à', 'ÏóêÌã∞Ïò§ÌîºÏïÑ', 'Ïö∞Í∞ÑÎã§', 'Î•¥ÏôÑÎã§', 'Í∞ÄÎ¥â', 'Í∞êÎπÑÏïÑ', 'ÎùºÏù¥Î≤†Î¶¨ÏïÑ', 'ÎßàÎã§Í∞ÄÏä§Ïπ¥Î•¥', 'ÎßêÎùºÏúÑ', 'Î™®Î°úÏΩî', 'Î≤†ÎÉâ', 'ÏãúÏóêÎùºÎ¶¨Ïò®', 'ÌÜ†Í≥†']
            },
            'oceania': {
                'Í∞úÎ≥Ñ': [],
                'Í∏∞ÌÉÄ': ['Ìò∏Ï£º', 'Îâ¥ÏßàÎûúÎìú']
            },
            'other': {
                'Í∞úÎ≥Ñ': ['Îü¨ÏãúÏïÑ'],
                'Í∏∞ÌÉÄ': ['ÎùºÌä∏ÎπÑÏïÑ', 'Î™∞ÎèÑÎ∞î', 'ÎßàÏºÄÎèÑÎãàÏïÑ', 'Î≥¥Ïä§ÎãàÏïÑÌó§Î•¥Ï≤¥Í≥†ÎπÑÎÇò', 'ÏÑ∏Î•¥ÎπÑÏïÑ', 'Ïä¨Î°úÎ∞îÌÇ§ÏïÑ', 'Ïö∞Ï¶àÎ≤†ÌÇ§Ïä§ÌÉÑ', 'Ï°∞ÏßÄÏïÑ']
            }
        }
    
    def get_country_region(self, country):
        """Íµ≠Í∞ÄÏùò ÏßÄÏó≠ Î∞òÌôò"""
        for region, countries in self.regions.items():
            if country in countries['Í∞úÎ≥Ñ'] or country in countries['Í∏∞ÌÉÄ']:
                return region
        return 'other'
    
    def is_individual_country(self, country):
        """Í∞úÎ≥Ñ Ïú†ÏßÄ Íµ≠Í∞ÄÏù∏ÏßÄ ÌôïÏù∏"""
        return country in self.individual_countries
    
    def is_minor_country(self, country):
        """ÏÜåÍ∑úÎ™® Íµ≠Í∞ÄÏù∏ÏßÄ ÌôïÏù∏"""
        for region, countries in self.regions.items():
            if country in countries['Í∏∞ÌÉÄ']:
                return True
        return False
    
    def preprocess_country(self, country):
        """Íµ≠Í∞Ä Ï†ÑÏ≤òÎ¶¨"""
        if pd.isna(country):
            return 'Ï†ïÎ≥¥ÏóÜÏùå'
        
        # 1. Í∞úÎ≥Ñ Ïú†ÏßÄ Íµ≠Í∞Ä
        if country in self.individual_countries:
            return country
        
        # 2. ÏßÄÏó≠Î≥Ñ Í∑∏Î£πÌôî
        region = self.get_country_region(country)
        if region != 'other':
            return f'{region}_Í∏∞ÌÉÄ'
        
        # 3. ÏôÑÏ†ÑÌûà Ïïå Ïàò ÏóÜÎäî Íµ≠Í∞Ä
        return 'Í∏∞ÌÉÄÍµ≠Í∞Ä'
    
    def calculate_country_similarity(self, country1, country2):
        """Í≥ÑÏ∏µÏ†Å Íµ≠Í∞Ä Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        if pd.isna(country1) or pd.isna(country2):
            return 0.0
        
        # ÏôÑÏ†Ñ ÏùºÏπò
        if country1 == country2:
            return 1.0
        
        region1 = self.get_country_region(country1)
        region2 = self.get_country_region(country2)
        
        is_individual1 = self.is_individual_country(country1)
        is_individual2 = self.is_individual_country(country2)
        is_minor1 = self.is_minor_country(country1)
        is_minor2 = self.is_minor_country(country2)
        
        # Îëò Îã§ Í∞úÎ≥Ñ Íµ≠Í∞ÄÏù∏ Í≤ΩÏö∞
        if is_individual1 and is_individual2:
            return 0.6 if region1 == region2 else 0.3
        
        # Îëò Îã§ ÏÜåÍ∑úÎ™® Íµ≠Í∞ÄÏù∏ Í≤ΩÏö∞
        if is_minor1 and is_minor2:
            return 0.7 if region1 == region2 else 0.4
        
        # ÌïòÎÇòÎäî Í∞úÎ≥Ñ, ÌïòÎÇòÎäî ÏÜåÍ∑úÎ™®Ïù∏ Í≤ΩÏö∞
        if (is_individual1 and is_minor2) or (is_minor1 and is_individual2):
            return 0.5 if region1 == region2 else 0.2
        
        # Í∏∞ÌÉÄ Í≤ΩÏö∞
        return 0.2

class HybridConfidenceCalculator:
    """ÌïòÏù¥Î∏åÎ¶¨Îìú Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞Í∏∞"""
    
    @staticmethod
    def calculate_weighted_confidence(decisions, similarity_scores):
        """Í∞ÄÏ§ë Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        if not decisions or not similarity_scores:
            return 0.0, 'Ï†ïÎ≥¥ÏóÜÏùå'
        
        # Í∞ÄÏû• ÎßéÏùÄ ÌåêÏ†ïÍµ¨Î∂Ñ Ï∞æÍ∏∞
        decision_counts = Counter(decisions)
        most_common_decision = decision_counts.most_common(1)[0][0]
        
        # Ìï¥Îãπ ÌåêÏ†ïÏùò Í∞ÄÏ§ë Ï†êÏàò Ìï©
        weighted_sum = sum(score for decision, score in zip(decisions, similarity_scores) 
                          if decision == most_common_decision)
        total_weight = sum(similarity_scores)
        
        if total_weight == 0:
            return 0.0, most_common_decision
        
        return weighted_sum / total_weight, most_common_decision
    
    @staticmethod
    def calculate_bayesian_confidence(positive_count, total_count, confidence_level=0.95):
        """Î≤†Ïù¥ÏßÄÏïà Ïã†Î¢∞Íµ¨Í∞Ñ Í≥ÑÏÇ∞"""
        if total_count == 0:
            return 0.0, (0.0, 0.0)
        
        # Î≤†ÌÉÄ Î∂ÑÌè¨ ÌååÎùºÎØ∏ÌÑ∞ (Î¨¥Ï†ïÎ≥¥ ÏÇ¨Ï†ÑÎ∂ÑÌè¨)
        alpha = 1 + positive_count
        beta = 1 + total_count - positive_count
        
        # Î≤†Ïù¥ÏßÄÏïà Ï∂îÏ†ïÍ∞í
        posterior_mean = alpha / (alpha + beta)
        
        # Ïã†Î¢∞Íµ¨Í∞Ñ
        alpha_level = 1 - confidence_level
        lower = stats.beta.ppf(alpha_level/2, alpha, beta)
        upper = stats.beta.ppf(1 - alpha_level/2, alpha, beta)
        
        return posterior_mean, (lower, upper)
    
    @classmethod
    def hybrid_confidence(cls, decisions, similarity_scores):
        """ÌïòÏù¥Î∏åÎ¶¨Îìú Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        if not decisions:
            return {
                'confidence': 0.0,
                'credible_interval': (0.0, 0.0),
                'predicted_decision': 'Ï†ïÎ≥¥ÏóÜÏùå',
                'sample_size': 0,
                'avg_similarity': 0.0,
                'interpretation': 'Ïú†ÏÇ¨ÏÇ¨Î°ÄÍ∞Ä ÏóÜÏäµÎãàÎã§.',
                'grade': 'Ïã†Î¢∞Î∂àÍ∞Ä'
            }
        
        # 1. Í∞ÄÏ§ë Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
        weighted_conf, predicted_decision = cls.calculate_weighted_confidence(decisions, similarity_scores)
        
        # 2. Î≤†Ïù¥ÏßÄÏïà Ïã†Î¢∞Íµ¨Í∞Ñ Í≥ÑÏÇ∞
        positive_count = sum(1 for d in decisions if d == predicted_decision)
        bayesian_mean, (lower, upper) = cls.calculate_bayesian_confidence(positive_count, len(decisions))
        
        # 3. ÌëúÎ≥∏ ÌÅ¨Í∏∞ Î∞è ÌíàÏßà Î≥¥Ï†ï
        sample_size = len(decisions)
        avg_similarity = np.mean(similarity_scores) if similarity_scores else 0.0
        
        # ÌëúÎ≥∏ ÌÅ¨Í∏∞ Î≥¥Ï†ï (Îçî Î≥¥ÏàòÏ†ÅÏúºÎ°ú)
        if sample_size >= 10:
            sample_factor = 1.0
        elif sample_size >= 5:
            sample_factor = 0.9
        else:
            sample_factor = 0.8
        
        # Ïú†ÏÇ¨ÎèÑ ÌíàÏßà Î≥¥Ï†ï
        if avg_similarity >= 0.7:
            quality_factor = 1.1
        elif avg_similarity >= 0.5:
            quality_factor = 1.0
        else:
            quality_factor = 0.9
        
        # ÏµúÏ¢Ö Ïã†Î¢∞ÎèÑ (Í∞ÄÏ§ë Î∞©Ïãù + Î≥¥Ï†ï)
        final_confidence = min(0.95, weighted_conf * sample_factor * quality_factor)
        
        # Ïã†Î¢∞ÎèÑ Îì±Í∏â
        if final_confidence >= 0.8 and sample_size >= 5:
            grade = 'ÎÜíÏùå'
        elif final_confidence >= 0.6 and sample_size >= 3:
            grade = 'Î≥¥ÌÜµ'
        else:
            grade = 'ÎÇÆÏùå'
        
        # Ìï¥ÏÑù Î©îÏãúÏßÄ
        interpretation = f"{final_confidence:.1%} (95% Íµ¨Í∞Ñ: {lower:.1%}-{upper:.1%})"
        
        return {
            'confidence': final_confidence,
            'credible_interval': (lower, upper),
            'predicted_decision': predicted_decision,
            'sample_size': sample_size,
            'avg_similarity': avg_similarity,
            'interpretation': interpretation,
            'grade': grade,
            'bayesian_mean': bayesian_mean,
            'weighted_confidence': weighted_conf
        }

class ImprovedInsuranceSystem:
    def __init__(self):
        self.model = None
        self.kosimcse_model = None
        self.kosimcse_tokenizer = None
        self.text_vectorizer = None
        self.label_encoders = {}
        self.feature_importance = None
        
        # Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî
        self.country_processor = CountryProcessor()
        self.confidence_calculator = HybridConfidenceCalculator()
        
        # ÏµúÏ†ÅÌôîÎêú Í∞ÄÏ§ëÏπò (ÌôïÏû•Îêú Î≥ÄÏàò Ìè¨Ìï®)
        self.optimal_weights = {
            # ÌïµÏã¨ Î≥ÄÏàò (Í∞úÎ≥Ñ Í∞ÄÏ§ëÏπò)
            'text_similarity': 0.35,      # ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ
            'accident_type': 0.20,        # ÏÇ¨Í≥†Ïú†Ìòï
            'country_similarity': 0.12,   # Íµ≠Í∞Ä Ïú†ÏÇ¨ÎèÑ
            
            # Ïà´ÏûêÌòï Î≥ÄÏàò Í∑∏Î£π (15%)
            'amount_similarity': 0.08,    # Í∏àÏï° Ïú†ÏÇ¨ÎèÑ
            'coverage_rate': 0.05,        # Î∂ÄÎ≥¥Ïú®
            'payment_terms': 0.02,        # Í≤∞Ï†úÏ°∞Í±¥
            
            # Î≤îÏ£ºÌòï Î≥ÄÏàò Í∑∏Î£π (18%)
            'insurance_type': 0.05,       # Î≥¥ÌóòÏ¢ÖÎ™©
            'product_category': 0.08,     # ÏÉÅÌíàÎ∂ÑÎ•ò
            'payment_method': 0.04,       # Í≤∞Ï†úÎ∞©Î≤ï
            'future_outlook': 0.01        # Ìñ•ÌõÑÏ†ÑÎßù
        }
        
        # Ï∫êÏãú Í¥ÄÎ¶¨
        self.embeddings_cache = {}
        self.similarity_cache = {}
        
        # Îü∞ÌÉÄÏûÑ Í∞ÄÏ§ëÏπò Ïò§Î≤ÑÎùºÏù¥Îìú(ÏÑ∏ÏÖò Ï§ë ÏùºÏãú Ï†ÅÏö©)
        self.runtime_weight_overrides = None
    
    @st.cache_resource
    def load_kosimcse_model(_self):
        """KoSimCSE Î™®Îç∏ Î°úÎìú"""
        try:
            # Lazy import to avoid hard dependency at module import time
            from transformers import AutoModel, AutoTokenizer
            model_name = "BM-K/KoSimCSE-roberta-multitask"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)
            model.eval()
            return model, tokenizer
        except Exception as e:
            st.error(f"AI Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
            return None, None
    
    def initialize_ai_model(self):
        """AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî"""
        if self.kosimcse_model is None:
            with st.spinner("AI Î™®Îç∏ÏùÑ Î°úÎìúÌïòÎäî Ï§ë..."):
                self.kosimcse_model, self.kosimcse_tokenizer = self.load_kosimcse_model()
        return self.kosimcse_model is not None
    
    def preprocess_text(self, text):
        """ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨"""
        if pd.isna(text) or text == '':
            return ""
        
        text = str(text).strip()
        
        # ÏùòÎØ∏ÏóÜÎäî ÌÖçÏä§Ìä∏ ÌïÑÌÑ∞ÎßÅ
        meaningless_patterns = [
            r'^ÏÑ§Î™ÖÏóÜÏùå$', r'^Ï≤®Î∂ÄÌååÏùºÏ∞∏Í≥†$', r'^Ìï¥ÎãπÏóÜÏùå$', r'^-$',
            r'^ÏóÜÏùå$', r'^Í∏∞ÌÉÄ$', r'^ÎØ∏ÏÉÅ$'
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return ""
        
        return text
    
    def get_text_embeddings(self, texts, batch_size=4):
        """ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© ÏÉùÏÑ±"""
        if not self.initialize_ai_model():
            return None
        
        processed_texts = [self.preprocess_text(text) for text in texts]
        valid_texts = [text for text in processed_texts if text]
        
        if not valid_texts:
            return None
        
        embeddings = []
        
        try:
            # Lazy import torch to avoid ImportError if not installed
            import torch
            for i in range(0, len(valid_texts), batch_size):
                batch_texts = valid_texts[i:i + batch_size]
                
                inputs = self.kosimcse_tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=256,
                    return_tensors="pt"
                )
                
                with torch.no_grad():
                    outputs = self.kosimcse_model(**inputs)
                    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                    embeddings.extend(batch_embeddings)
            
            return np.array(embeddings)
            
        except Exception as e:
            st.error(f"ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return None
    
    def smart_candidate_filtering(self, query_case, candidates_df, max_candidates=150):
        """Ïä§ÎßàÌä∏ ÌõÑÎ≥¥ ÌïÑÌÑ∞ÎßÅ"""
        query_country = query_case.get('ÏàòÏûÖÍµ≠', '')
        
        # 1. ÏøºÎ¶¨ Íµ≠Í∞ÄÍ∞Ä Í∞úÎ≥Ñ Ïú†ÏßÄ Íµ≠Í∞ÄÏù∏ Í≤ΩÏö∞
        if self.country_processor.is_individual_country(query_country):
            # Í∞ôÏùÄ Íµ≠Í∞Ä Ïö∞ÏÑ†
            same_country = candidates_df[candidates_df['ÏàòÏûÖÍµ≠'] == query_country]
            if len(same_country) >= 20:
                return same_country.sample(n=min(max_candidates, len(same_country)), random_state=42)
            
            # Í∞ôÏùÄ ÏßÄÏó≠ÏúºÎ°ú ÌôïÏû•
            query_region = self.country_processor.get_country_region(query_country)
            same_region = candidates_df[candidates_df['ÏàòÏûÖÍµ≠'].apply(
                lambda x: self.country_processor.get_country_region(x) == query_region
            )]
            if len(same_region) >= 50:
                return same_region.sample(n=min(max_candidates, len(same_region)), random_state=42)
        
        # 2. ÏøºÎ¶¨ Íµ≠Í∞ÄÍ∞Ä ÏÜåÍ∑úÎ™® Íµ≠Í∞ÄÏù∏ Í≤ΩÏö∞
        elif self.country_processor.is_minor_country(query_country):
            query_region = self.country_processor.get_country_region(query_country)
            
            # Í∞ôÏùÄ ÏßÄÏó≠Ïùò ÏÜåÍ∑úÎ™® Íµ≠Í∞ÄÎì§ Ïö∞ÏÑ†
            same_region_minor = candidates_df[candidates_df['ÏàòÏûÖÍµ≠'].apply(
                lambda x: (self.country_processor.get_country_region(x) == query_region and 
                          self.country_processor.is_minor_country(x))
            )]
            
            if len(same_region_minor) >= 10:
                return same_region_minor.sample(n=min(max_candidates//2, len(same_region_minor)), random_state=42)
            
            # Í∞ôÏùÄ ÏßÄÏó≠ Ï†ÑÏ≤¥Î°ú ÌôïÏû•
            same_region_all = candidates_df[candidates_df['ÏàòÏûÖÍµ≠'].apply(
                lambda x: self.country_processor.get_country_region(x) == query_region
            )]
            
            if len(same_region_all) >= 20:
                return same_region_all.sample(n=min(max_candidates, len(same_region_all)), random_state=42)
        
        # 3. Ï†ÑÏ≤¥ Í≤ÄÏÉâ (Î¨¥ÏûëÏúÑ ÏÉòÌîåÎßÅ)
        return candidates_df.sample(n=min(max_candidates, len(candidates_df)), random_state=42)
    
    def calculate_similarity_scores(self, query_case, candidates_df):
        """Í∞úÏÑ†Îêú Ïú†ÏÇ¨ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞"""
        
        # 0) Îç∞Ïù¥ÌÑ∞ ÎàÑÏ∂ú Î∞©ÏßÄ: ÎèôÏùº ÏºÄÏù¥Ïä§/Ï§ëÎ≥µ ÌÖçÏä§Ìä∏ ÌõÑÎ≥¥ Ï†úÍ±∞
        safe_candidates = candidates_df.copy()
        try:
            # ÎèôÏùº ÏÇ¨Í≥†Î≤àÌò∏/Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏ Î∞∞Ï†ú
            for key in ['ÏÇ¨Í≥†Î≤àÌò∏', 'Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏']:
                if key in safe_candidates.columns and key in query_case:
                    safe_candidates = safe_candidates[safe_candidates[key] != query_case.get(key)]
            # ÎèôÏùº ÌÖçÏä§Ìä∏(Ï†ÑÏ≤òÎ¶¨ ÌõÑ) ÏôÑÏ†Ñ ÏùºÏπò + Ï£ºÏöî Î©îÌÉÄ ÎèôÏùºÏãú Î∞∞Ï†ú
            q_text_norm = self.preprocess_text(query_case.get('ÏÇ¨Í≥†ÏÑ§Î™Ö', ''))
            if q_text_norm:
                def _norm_text(x):
                    return self.preprocess_text(x)
                txt_eq = safe_candidates['ÏÇ¨Í≥†ÏÑ§Î™Ö'].apply(_norm_text) == q_text_norm
                meta_eq = True
                if 'ÏàòÏûÖÍµ≠' in safe_candidates.columns and 'ÏàòÏûÖÍµ≠' in query_case:
                    meta_eq = meta_eq & (safe_candidates['ÏàòÏûÖÍµ≠'] == query_case.get('ÏàòÏûÖÍµ≠'))
                if 'Î≥¥ÌóòÏ¢ÖÎ™©' in safe_candidates.columns and 'Î≥¥ÌóòÏ¢ÖÎ™©' in query_case:
                    meta_eq = meta_eq & (safe_candidates['Î≥¥ÌóòÏ¢ÖÎ™©'] == query_case.get('Î≥¥ÌóòÏ¢ÖÎ™©'))
                dup_mask = txt_eq & meta_eq
                if dup_mask.any():
                    safe_candidates = safe_candidates[~dup_mask]
        except Exception:
            pass

        # Ïä§ÎßàÌä∏ ÌïÑÌÑ∞ÎßÅÏúºÎ°ú ÌõÑÎ≥¥ Ïàò Ï†úÌïú
        filtered_candidates = self.smart_candidate_filtering(query_case, safe_candidates)
        
        # Î©¥Ï±Ö ÏÇ¨Î°ÄÏôÄ ÎπÑÎ©¥Ï±Ö ÏÇ¨Î°Ä Î∂ÑÎ¶¨
        exemption_candidates = filtered_candidates[filtered_candidates['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö']
        non_exemption_candidates = filtered_candidates[filtered_candidates['ÌåêÏ†ïÍµ¨Î∂Ñ'] != 'Î©¥Ï±Ö']
        
        similarities = []
        
        # ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        query_text = query_case.get('ÏÇ¨Í≥†ÏÑ§Î™Ö', '')
        if query_text and len(query_text) > 10:
            candidate_texts = filtered_candidates['ÏÇ¨Í≥†ÏÑ§Î™Ö'].tolist()
            
            # AI Í∏∞Î∞ò Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            all_texts = [query_text] + candidate_texts
            embeddings = self.get_text_embeddings(all_texts)
            
            if embeddings is not None and len(embeddings) > 1:
                query_embedding = embeddings[0:1]
                candidate_embeddings = embeddings[1:]
                text_similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]
                
                # Î©¥Ï±Ö ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ïú†ÏÇ¨ÎèÑ Î≥¥Ï†ï Ï†ÅÏö©
                text_similarities = self._apply_exemption_keyword_boost(
                    query_text, candidate_texts, text_similarities, filtered_candidates
                )
            else:
                # Ìè¥Î∞±: Í∞ÑÎã®Ìïú ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ
                text_similarities = []
                query_words = set(query_text.lower().split())
                for candidate_text in candidate_texts:
                    if pd.notna(candidate_text):
                        candidate_words = set(str(candidate_text).lower().split())
                        if query_words and candidate_words:
                            jaccard_sim = len(query_words.intersection(candidate_words)) / len(query_words.union(candidate_words))
                            text_similarities.append(jaccard_sim)
                        else:
                            text_similarities.append(0.0)
                    else:
                        text_similarities.append(0.0)
                text_similarities = np.array(text_similarities)
                
                # Î©¥Ï±Ö ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ïú†ÏÇ¨ÎèÑ Î≥¥Ï†ï Ï†ÅÏö© (Ìè¥Î∞±ÏóêÎèÑ)
                text_similarities = self._apply_exemption_keyword_boost(
                    query_text, candidate_texts, text_similarities, filtered_candidates
                )
        else:
            text_similarities = np.zeros(len(filtered_candidates))
        
        # ÌÜµÌï© Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        for i, (idx, candidate) in enumerate(filtered_candidates.iterrows()):
            score = 0.0
            # Îü∞ÌÉÄÏûÑ Ïò§Î≤ÑÎùºÏù¥ÎìúÍ∞Ä ÏûàÏúºÎ©¥ Ïö∞ÏÑ† Ï†ÅÏö©
            weights = self.runtime_weight_overrides if self.runtime_weight_overrides else self.optimal_weights
            
            # 1. ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ
            if i < len(text_similarities):
                score += weights['text_similarity'] * text_similarities[i]
            
            # 2. ÏÇ¨Í≥†Ïú†Ìòï Ïú†ÏÇ¨ÎèÑ
            if query_case.get('ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö') == candidate.get('ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'):
                score += weights['accident_type']
            elif self._group_accident_type(query_case.get('ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö')) == self._group_accident_type(candidate.get('ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö')):
                score += weights['accident_type'] * 0.7
            
            # 3. Íµ≠Í∞Ä Ïú†ÏÇ¨ÎèÑ (Í∞úÏÑ†Îê®)
            country_sim = self.country_processor.calculate_country_similarity(
                query_case.get('ÏàòÏûÖÍµ≠'), candidate.get('ÏàòÏûÖÍµ≠')
            )
            score += weights['country_similarity'] * country_sim
            
            # 4. Í∏àÏï°ÎåÄ Ïú†ÏÇ¨ÎèÑ
            query_amount = query_case.get('ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°', 0)
            candidate_amount = candidate.get('ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°', 0)
            
            if query_amount > 0 and candidate_amount > 0:
                amount_ratio = min(query_amount, candidate_amount) / max(query_amount, candidate_amount)
                score += weights['amount_similarity'] * amount_ratio
            
            # 5. Î≥¥ÌóòÏ¢ÖÎ™© ÏùºÏπò
            if query_case.get('Î≥¥ÌóòÏ¢ÖÎ™©') == candidate.get('Î≥¥ÌóòÏ¢ÖÎ™©'):
                score += weights['insurance_type']
            
            # 6. ÏÉÅÌíàÎ∂ÑÎ•ò Ïú†ÏÇ¨ÎèÑ (Í∑∏Î£πÎ™Ö + ÏÉÅÏÑ∏Î∂ÑÎ•òÎ™Ö)
            if 'product_category' in weights:
                query_product = query_case.get('ÏÉÅÌíàÎ∂ÑÎ•òÎ™Ö', '')
                candidate_product = candidate.get('ÏÉÅÌíàÎ∂ÑÎ•òÎ™Ö', '')
                query_group = query_case.get('ÏÉÅÌíàÎ∂ÑÎ•òÍ∑∏Î£πÎ™Ö', '')
                candidate_group = candidate.get('ÏÉÅÌíàÎ∂ÑÎ•òÍ∑∏Î£πÎ™Ö', '')
                
                # ÏôÑÏ†Ñ ÏùºÏπò (ÏÉÅÏÑ∏Î∂ÑÎ•òÎ™Ö)
                if query_product == candidate_product:
                    score += weights['product_category']
                # Í∑∏Î£πÎ™Ö ÏùºÏπò
                elif query_group == candidate_group:
                    score += weights['product_category'] * 0.8
                # Í∑∏Î£πÌôîÎêú ÏÉÅÌíàÎ∂ÑÎ•ò ÏùºÏπò
                elif self._group_product_category(query_product) == self._group_product_category(candidate_product):
                    score += weights['product_category'] * 0.6
            
            # 7. Î∂ÄÎ≥¥Ïú® Ïú†ÏÇ¨ÎèÑ
            if 'coverage_rate' in weights:
                query_coverage = query_case.get('Î∂ÄÎ≥¥Ïú®', 0)
                candidate_coverage = candidate.get('Î∂ÄÎ≥¥Ïú®', 0)
                
                coverage_sim = self._calculate_coverage_similarity(query_coverage, candidate_coverage)
                score += weights['coverage_rate'] * coverage_sim
            
            # 8. Í≤∞Ï†úÎ∞©Î≤ï Ïú†ÏÇ¨ÎèÑ
            if 'payment_method' in weights:
                query_payment = query_case.get('Í≤∞Ï†úÎ∞©Î≤ï', '')
                candidate_payment = candidate.get('Í≤∞Ï†úÎ∞©Î≤ï', '')
                
                if query_payment == candidate_payment:
                    score += weights['payment_method']
                elif self._group_payment_method(query_payment) == self._group_payment_method(candidate_payment):
                    score += weights['payment_method'] * 0.8
            
            # 9. Í≤∞Ï†úÏ°∞Í±¥ Ïú†ÏÇ¨ÎèÑ
            if 'payment_terms' in weights:
                query_terms = query_case.get('Í≤∞Ï†úÏ°∞Í±¥', '')
                candidate_terms = candidate.get('Í≤∞Ï†úÏ°∞Í±¥', '')
                
                terms_sim = self._calculate_payment_terms_similarity(query_terms, candidate_terms)
                score += weights['payment_terms'] * terms_sim
            
            # 10. Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù Ïú†ÏÇ¨ÎèÑ
            if 'future_outlook' in weights:
                query_outlook = query_case.get('Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù', '')
                candidate_outlook = candidate.get('Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù', '')
                
                outlook_sim = self._calculate_future_outlook_similarity(query_outlook, candidate_outlook)
                score += weights['future_outlook'] * outlook_sim
            
            similarities.append((
                score, 
                text_similarities[i] if i < len(text_similarities) else 0.0, 
                country_sim,
                candidate
            ))
        
        # Î©¥Ï±Ö ÏÇ¨Î°ÄÏôÄ ÎπÑÎ©¥Ï±Ö ÏÇ¨Î°Ä Î∂ÑÎ¶¨
        exemption_similarities = [(score, text_sim, country_sim, candidate) 
                                for score, text_sim, country_sim, candidate in similarities 
                                if candidate['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö']
        
        non_exemption_similarities = [(score, text_sim, country_sim, candidate) 
                                    for score, text_sim, country_sim, candidate in similarities 
                                    if candidate['ÌåêÏ†ïÍµ¨Î∂Ñ'] != 'Î©¥Ï±Ö']
        
        # Í∞ÅÍ∞Å Ï†ïÎ†¨
        exemption_similarities.sort(key=lambda x: x[0], reverse=True)
        non_exemption_similarities.sort(key=lambda x: x[0], reverse=True)
        
        # Í∞ïÏ†ú Î©¥Ï±Ö Ìè¨Ìï® Í≤∞Í≥º Íµ¨ÏÑ±
        final_results = []
        
        # 1. Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î©¥Ï±Ö ÏÇ¨Î°Ä 1Í±¥ (ÏûàÎã§Î©¥ Î¨¥Ï°∞Í±¥ Ìè¨Ìï®)
        if exemption_similarities:
            final_results.append(exemption_similarities[0])
            print(f"üõ°Ô∏è Î©¥Ï±Ö Í≤ΩÍ≥†: Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î©¥Ï±Ö ÏÇ¨Î°Ä Ìè¨Ìï® (Ïú†ÏÇ¨ÎèÑ: {exemption_similarities[0][0]:.3f})")
        
        # 2. ÎÇòÎ®∏ÏßÄÎäî Ï†ÑÏ≤¥ÏóêÏÑú ÏÉÅÏúÑ ÏàúÏúºÎ°ú (Î©¥Ï±Ö Ï†úÏô∏ÌïòÍ≥†)
        remaining_slots = max(0, len(similarities) - len(final_results))
        if remaining_slots > 0:
            final_results.extend(non_exemption_similarities[:remaining_slots])
        
        return final_results
    
    def _group_accident_type(self, accident_type):
        """ÏÇ¨Í≥†Ïú†Ìòï Í∑∏Î£πÌôî"""
        if pd.isna(accident_type):
            return 'Í∏∞ÌÉÄ'
        
        if 'Ïã†Ïö©ÏúÑÌóò' in accident_type:
            if 'ÏßÄÍ∏âÏßÄÏ≤¥' in accident_type:
                return 'Ïã†Ïö©ÏúÑÌóò-ÏßÄÍ∏âÏßÄÏ≤¥'
            elif 'ÌååÏÇ∞' in accident_type:
                return 'Ïã†Ïö©ÏúÑÌóò-ÌååÏÇ∞'
            elif 'ÏßÄÍ∏âÎ∂àÎä•' in accident_type or 'ÏßÄÍ∏âÍ±∞Ï†à' in accident_type:
                return 'Ïã†Ïö©ÏúÑÌóò-ÏßÄÍ∏âÎ∂àÎä•/Í±∞Ï†à'
            else:
                return 'Ïã†Ïö©ÏúÑÌóò-Í∏∞ÌÉÄ'
        elif 'ÎπÑÏÉÅÏúÑÌóò' in accident_type:
            return 'ÎπÑÏÉÅÏúÑÌóò'
        elif 'Í≤ÄÏó≠ÏúÑÌóò' in accident_type:
            return 'Í≤ÄÏó≠ÏúÑÌóò'
        else:
            return 'Í∏∞ÌÉÄÏúÑÌóò'
    
    def _group_product_category(self, product_name):
        """ÏÉÅÌíàÎ∂ÑÎ•ò Í∑∏Î£πÌôî"""
        if pd.isna(product_name):
            return 'Í∏∞ÌÉÄ'
        
        product_name = str(product_name).lower()
        
        # ÏùòÎ•ò Î∞è ÏßÅÎ¨ºÎ•ò
        if any(word in product_name for word in ['ÏùòÎ•ò', 'ÏßÅÎ¨º', 'ÏÑ¨Ïú†', 'Ìå®ÏÖò', 'textile', 'clothing']):
            return 'ÏùòÎ•ò_ÏßÅÎ¨ºÎ•ò'
        # Ï†ÑÏûêÏ†úÌíà
        elif any(word in product_name for word in ['Ï†ÑÏûê', 'Î∞òÎèÑÏ≤¥', 'Ïª¥Ìì®ÌÑ∞', 'electronics', 'semiconductor']):
            return 'Ï†ÑÏûêÏ†úÌíà'
        # ÎÜçÏàòÏÇ∞Î¨º
        elif any(word in product_name for word in ['ÎÜçÏÇ∞Î¨º', 'ÏàòÏÇ∞Î¨º', 'ÏãùÌíà', 'agriculture', 'food']):
            return 'ÎÜçÏàòÏÇ∞Î¨º'
        # ÏûêÎèôÏ∞® Î∞è Î∂ÄÌíà
        elif any(word in product_name for word in ['ÏûêÎèôÏ∞®', 'Î∂ÄÌíà', 'auto', 'parts']):
            return 'ÏûêÎèôÏ∞®_Î∂ÄÌíà'
        # ÌôîÌïôÏ†úÌíà
        elif any(word in product_name for word in ['ÌôîÌïô', 'ÌîåÎùºÏä§Ìã±', 'chemical', 'plastic']):
            return 'ÌôîÌïôÏ†úÌíà'
        else:
            return 'Í∏∞ÌÉÄÏ†úÌíà'
    
    def _group_payment_method(self, payment_method):
        """Í≤∞Ï†úÎ∞©Î≤ï Í∑∏Î£πÌôî"""
        if pd.isna(payment_method):
            return 'Í∏∞ÌÉÄ'
        
        payment_method = str(payment_method).upper()
        
        # Ïã†Ïö©ÎèÑÎ≥Ñ Í∑∏Î£πÌôî
        if any(method in payment_method for method in ['L/C', 'LC', 'Ïã†Ïö©Ïû•']):
            return 'L/C'
        elif any(method in payment_method for method in ['D/P', 'DP', 'ÎèÑÏ∞©ÏßÄÏßÄÍ∏â']):
            return 'D/P'
        elif any(method in payment_method for method in ['D/A', 'DA', 'ÎèÑÏ∞©ÏßÄÏù∏Ïàò']):
            return 'D/A'
        elif any(method in payment_method for method in ['NET', 'OPEN', 'Î¨¥Ïã†Ïö©Ïû•']):
            return 'NET'
        else:
            return 'Í∏∞ÌÉÄÍ≤∞Ï†ú'
    
    def _calculate_coverage_similarity(self, query_rate, candidate_rate):
        """Î∂ÄÎ≥¥Ïú® Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        if pd.isna(query_rate) or pd.isna(candidate_rate):
            return 0.5  # Ï§ëÍ∞ÑÍ∞í
        
        # ÎπÑÏú® Ï∞®Ïù¥ Í≥ÑÏÇ∞
        rate_diff = abs(query_rate - candidate_rate) / 100
        
        # Ï∞®Ïù¥Í∞Ä ÌÅ¥ÏàòÎ°ù ÎÇÆÏùÄ Ïú†ÏÇ¨ÎèÑ
        return max(0.1, 1.0 - rate_diff)
    
    def _calculate_payment_terms_similarity(self, query_terms, candidate_terms):
        """Í≤∞Ï†úÏ°∞Í±¥ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        if pd.isna(query_terms) or pd.isna(candidate_terms):
            return 0.5
        
        query_terms = str(query_terms).lower()
        candidate_terms = str(candidate_terms).lower()
        
        # ÏôÑÏ†Ñ ÏùºÏπò
        if query_terms == candidate_terms:
            return 1.0
        
        # Ï°∞Í±¥ Ïú†ÌòïÎ≥Ñ Í∑∏Î£πÌôî
        if 'days' in query_terms and 'days' in candidate_terms:
            return 0.8
        elif 'sight' in query_terms and 'sight' in candidate_terms:
            return 0.8
        elif 'invoice' in query_terms and 'invoice' in candidate_terms:
            return 0.7
        
        return 0.3
    
    def _calculate_future_outlook_similarity(self, query_outlook, candidate_outlook):
        """Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        if pd.isna(query_outlook) or pd.isna(candidate_outlook):
            return 0.5
        
        query_outlook = str(query_outlook).lower()
        candidate_outlook = str(candidate_outlook).lower()
        
        # ÏôÑÏ†Ñ ÏùºÏπò
        if query_outlook == candidate_outlook:
            return 1.0
        
        # Í∏çÏ†ïÏ†Å vs Î∂ÄÏ†ïÏ†Å Í∑∏Î£πÌôî
        positive_terms = ['ÏßÄÍ∏âÏòàÏ†ï', 'ÌöåÏàòÏòàÏ†ï', 'Í∏çÏ†ïÏ†Å', 'positive']
        negative_terms = ['ÌöåÏàòÎ∂àÍ∞Ä', 'ÏßÄÍ∏âÎ∂àÍ∞Ä', 'Î∂ÄÏ†ïÏ†Å', 'negative']
        unknown_terms = ['ÌåêÎã®Î∂àÍ∞Ä', 'ÎØ∏ÏÉÅ', 'unknown']
        
        query_group = None
        candidate_group = None
        
        if any(term in query_outlook for term in positive_terms):
            query_group = 'positive'
        elif any(term in query_outlook for term in negative_terms):
            query_group = 'negative'
        elif any(term in query_outlook for term in unknown_terms):
            query_group = 'unknown'
        
        if any(term in candidate_outlook for term in positive_terms):
            candidate_group = 'positive'
        elif any(term in candidate_outlook for term in negative_terms):
            candidate_group = 'negative'
        elif any(term in candidate_outlook for term in unknown_terms):
            candidate_group = 'unknown'
        
        if query_group == candidate_group:
            return 0.8
        elif query_group == 'unknown' or candidate_group == 'unknown':
            return 0.5
        else:
            return 0.2
    
    def _apply_exemption_keyword_boost(self, query_text, candidate_texts, similarities, candidates_df):
        """Î©¥Ï±Ö Í¥ÄÎ†® ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ïú†ÏÇ¨ÎèÑ Î≥¥Ï†ï (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        
        # Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Î©¥Ï±Ö Í¥ÄÎ†® ÌïµÏã¨ Ìå®ÌÑ¥ (Îß•ÎùΩÏùÑ Í≥†Î†§)
        exemption_patterns = {
            'Í≥†ÏùòÍ≥ºÏã§': [
                ('RÍ∏â.*Í∏∞Îì±Î°ù', 0.15),  # Ï†ïÍ∑úÏãù Ìå®ÌÑ¥Í≥º Í∞ÄÏ§ëÏπò
                ('Ïã†Ïö©Ï°∞Ìöå.*ÏÜåÌôÄ', 0.12),
                ('Í≥†Ïùò.*ÏúÑÎ∞ò', 0.10),
                ('Í≥ºÏã§.*Î∞úÏÉù', 0.08)
            ],
            'Ïó∞ÏÜçÏàòÏ∂ú': [
                ('Ïù¥Ï†Ñ.*ÎØ∏ÏàòÍ∏à.*ÌöåÏàò', 0.20),
                ('Ïó∞ÏÜç.*ÏàòÏ∂ú.*ÏúÑÎ∞ò', 0.18),
                ('Í≤ΩÌï©ÏàòÏ∂úÏûê.*Ï°¥Ïû¨', 0.10),
                ('ÎèôÏùº.*ÏàòÏûÖÏûê.*Í±∞Îûò', 0.08)
            ],
            'Î≥¥ÏÉÅÌïúÎèÑÏ¥àÍ≥º': [
                ('Î≥¥ÏÉÅÌïúÎèÑ.*Ï¥àÍ≥º', 0.25),
                ('ÌïúÎèÑ.*Ï¥àÍ≥º', 0.20),
                ('Ï±ÖÏûÑÌïúÎèÑ.*Î∂ÄÏ°±', 0.15)
            ],
            'Î≥¥ÌóòÍ¥ÄÍ≥ÑÏÑ±Î¶Ω': [
                ('ÌóàÏúÑ.*ÏÑúÎ•ò.*Ï†úÏ∂ú', 0.20),
                ('Î≥¥ÌóòÍ¥ÄÍ≥Ñ.*ÏÑ±Î¶Ω.*Î∂àÍ∞Ä', 0.18),
                ('Ïã†Ï≤≠.*Ï†ïÎ≥¥.*ÌóàÏúÑ', 0.15)
            ],
            'Ï£ºÏùòÏùòÎ¨¥Ìï¥ÌÉú': [
                ('ÌååÏÇ∞Ïã†Ï≤≠.*ÏÉÅÌÉú', 0.25),
                ('Ïû¨Î¨¥ÏïÖÌôî.*ÏßïÌõÑ.*Î¨¥Ïãú', 0.20),
                ('Ï£ºÏùòÏùòÎ¨¥.*Ìï¥ÌÉú', 0.18),
                ('Ï†ÅÏ†àÌïú.*Ï°∞Ïπò.*Î∂ÄÏû¨', 0.12)
            ]
        }
        
        boosted_similarities = similarities.copy()
        
        for i, (candidate_text, candidate_row) in enumerate(zip(candidate_texts, candidates_df.itertuples())):
            # Ïã§Ï†ú ÌåêÏ†ïÍµ¨Î∂ÑÏù¥ Î©¥Ï±ÖÏù∏ Í≤ΩÏö∞Îßå Ï≤òÎ¶¨
            if hasattr(candidate_row, 'ÌåêÏ†ïÍµ¨Î∂Ñ') and candidate_row.ÌåêÏ†ïÍµ¨Î∂Ñ == 'Î©¥Ï±Ö':
                
                total_boost = 0.0  # ÎàÑÏ†Å Î∂ÄÏä§Ìä∏ ÎåÄÏã† Ï¥ùÌï©ÏúºÎ°ú Í≥ÑÏÇ∞
                matched_patterns = []
                
                # Í∞Å Ìå®ÌÑ¥Î≥ÑÎ°ú Îß§Ïπ≠ ÌôïÏù∏
                for pattern_type, pattern_list in exemption_patterns.items():
                    for pattern, weight in pattern_list:
                        # ÏøºÎ¶¨ÏôÄ ÌõÑÎ≥¥ Î™®ÎëêÏóêÏÑú Ìå®ÌÑ¥ Í≤ÄÏÉâ
                        import re
                        query_match = re.search(pattern, query_text, re.IGNORECASE)
                        candidate_match = re.search(pattern, str(candidate_text), re.IGNORECASE)
                        
                        if query_match and candidate_match:
                            # ÌåêÏ†ïÏÇ¨Ïú†ÎèÑ Ìå®ÌÑ¥Í≥º ÏùºÏπòÌïòÎäîÏßÄ ÌôïÏù∏
                            candidate_reason = getattr(candidate_row, 'ÌåêÏ†ïÏÇ¨Ïú†', '')
                            if pattern_type in ['Í≥†ÏùòÍ≥ºÏã§'] and 'Í≥†Ïùò' in candidate_reason:
                                total_boost += weight
                                matched_patterns.append(f"{pattern_type}:{pattern}")
                            elif pattern_type in ['Ïó∞ÏÜçÏàòÏ∂ú'] and 'Ïó∞ÏÜç' in candidate_reason:
                                total_boost += weight
                                matched_patterns.append(f"{pattern_type}:{pattern}")
                            elif pattern_type in ['Î≥¥ÏÉÅÌïúÎèÑÏ¥àÍ≥º'] and 'Ï¥àÍ≥º' in candidate_reason:
                                total_boost += weight
                                matched_patterns.append(f"{pattern_type}:{pattern}")
                            elif pattern_type in ['Î≥¥ÌóòÍ¥ÄÍ≥ÑÏÑ±Î¶Ω'] and 'ÏÑ±Î¶Ω' in candidate_reason:
                                total_boost += weight
                                matched_patterns.append(f"{pattern_type}:{pattern}")
                            elif pattern_type in ['Ï£ºÏùòÏùòÎ¨¥Ìï¥ÌÉú'] and 'Ìï¥ÌÉú' in candidate_reason:
                                total_boost += weight
                                matched_patterns.append(f"{pattern_type}:{pattern}")
                
                # ÏµúÏ¢Ö Î∂ÄÏä§Ìä∏ Ï†ÅÏö© (ÏµúÎåÄ 10% Î∂ÄÏä§Ìä∏Î°ú Ï†úÌïú)
                if total_boost > 0:
                    final_boost = min(0.10, total_boost)  # ÏµúÎåÄ 10% Î∂ÄÏä§Ìä∏
                    boosted_similarities[i] = min(1.0, similarities[i] + final_boost)  # Í≥±ÌïòÍ∏∞ ÎåÄÏã† ÎçîÌïòÍ∏∞
                        
        return boosted_similarities
    
    def _analyze_judgment_reasons(self, decisions, reasons, similarity_scores):
        """ÌåêÏ†ïÏÇ¨Ïú† Î∂ÑÏÑù"""
        
        # ÌåêÏ†ïÍµ¨Î∂ÑÎ≥Ñ ÏÇ¨Ïú† Î∂ÑÏÑù
        decision_reasons = {}
        for decision, reason, sim_score in zip(decisions, reasons, similarity_scores):
            if decision not in decision_reasons:
                decision_reasons[decision] = []
            decision_reasons[decision].append({
                'reason': reason,
                'similarity': sim_score
            })
        
        # ÏòàÏÉÅ ÌåêÏ†ïÏÇ¨Ïú† ÎèÑÏ∂ú
        predicted_reasons = {}
        
        for decision, reason_list in decision_reasons.items():
            # Ïú†ÏÇ¨ÎèÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Ï†ÅÏö©
            reason_weights = {}
            for item in reason_list:
                reason = item['reason']
                weight = item['similarity']
                
                if reason in reason_weights:
                    reason_weights[reason] += weight
                else:
                    reason_weights[reason] = weight
            
            # Í∞ÄÏ§ëÏπò ÏàúÏúºÎ°ú Ï†ïÎ†¨
            sorted_reasons = sorted(reason_weights.items(), key=lambda x: x[1], reverse=True)
            predicted_reasons[decision] = sorted_reasons
        
        return {
            'decision_reasons': decision_reasons,
            'predicted_reasons': predicted_reasons,
            'top_reasons': self._get_top_reasons_by_decision(predicted_reasons)
        }
    
    def _get_top_reasons_by_decision(self, predicted_reasons):
        """ÌåêÏ†ïÍµ¨Î∂ÑÎ≥Ñ ÏÉÅÏúÑ ÏÇ¨Ïú† ÏöîÏïΩ"""
        top_reasons = {}
        
        for decision, reasons in predicted_reasons.items():
            if reasons:
                # ÏÉÅÏúÑ 3Í∞ú ÏÇ¨Ïú†
                top_3 = reasons[:3]
                total_weight = sum([weight for _, weight in reasons])
                
                top_reasons[decision] = {
                    'reasons': [(reason, weight/total_weight) for reason, weight in top_3],
                    'total_cases': len(reasons)
                }
        
        return top_reasons

@st.cache_data
def load_data():
    """Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
    try:
        df = pd.read_csv('data/design.csv', encoding='cp949')
        
        # ÎÇ†Ïßú Ïª¨Îüº Ï≤òÎ¶¨
        date_columns = ['ÌåêÏ†ïÏùº', 'ÌåêÏ†ïÍ≤∞Ïû¨Ïùº', 'ÏÇ¨Í≥†Ï†ëÏàòÏùºÏûê', 'Î≥¥ÌóòÍ∏àÏ≤≠Íµ¨Ïùº']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        # Í∏àÏï° Ïª¨ÎüºÏùÑ Ïà´ÏûêÎ°ú Î≥ÄÌôò
        amount_columns = ['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°', 'ÏõêÌôîÌåêÏ†ïÍ∏àÏï°', ]
        for col in amount_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df
    except Exception as e:
        st.error(f"Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïò§Î•ò: {e}")
        return None

def create_country_analysis_tab(df, country_processor):
    """Íµ≠Í∞Ä Î∂ÑÏÑù ÌÉ≠"""
    st.subheader("üåç Íµ≠Í∞Ä Ï≤òÎ¶¨ Î∂ÑÏÑù")
    
    # Íµ≠Í∞Ä Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©
    df_processed = df.copy()
    df_processed['ÏàòÏûÖÍµ≠_processed'] = df_processed['ÏàòÏûÖÍµ≠'].apply(country_processor.preprocess_country)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**üìä Ï†ÑÏ≤òÎ¶¨ Ï†ÑÌõÑ ÎπÑÍµê**")
        
        before_count = df['ÏàòÏûÖÍµ≠'].nunique()
        after_count = df_processed['ÏàòÏûÖÍµ≠_processed'].nunique()
        
        st.metric("Ï†ÑÏ≤òÎ¶¨ Ï†Ñ Íµ≠Í∞Ä Ïàò", f"{before_count}Í∞ú")
        st.metric("Ï†ÑÏ≤òÎ¶¨ ÌõÑ Ïπ¥ÌÖåÍ≥†Î¶¨ Ïàò", f"{after_count}Í∞ú", f"-{before_count-after_count}Í∞ú")
        
        # Ï†ÑÏ≤òÎ¶¨ ÌõÑ Î∂ÑÌè¨
        processed_counts = df_processed['ÏàòÏûÖÍµ≠_processed'].value_counts().head(15)
        fig1 = px.bar(
            x=processed_counts.values,
            y=processed_counts.index,
            orientation='h',
            title="Ï†ÑÏ≤òÎ¶¨ ÌõÑ Íµ≠Í∞Ä Ïπ¥ÌÖåÍ≥†Î¶¨ Î∂ÑÌè¨",
            color_discrete_sequence=['#007bff']
        )
        fig1.update_layout(height=500, showlegend=False)
        st.plotly_chart(fig1, use_container_width=True)
    
    with col2:
        st.write("**üîç ÏÜåÍ∑úÎ™® Íµ≠Í∞Ä ÏÉÅÏÑ∏ ÎÇ¥Ïó≠**")
        
        # Í∏∞ÌÉÄ Íµ≠Í∞ÄÏóê Ìè¨Ìï®Îêú Íµ≠Í∞ÄÎì§ ÌëúÏãú
        minor_countries = []
        for region, countries in country_processor.regions.items():
            minor_countries.extend(countries['Í∏∞ÌÉÄ'])
        
        st.write("**Í∏∞ÌÉÄÍµ≠Í∞ÄÏóê Ìè¨Ìï®Îêú 39Í∞úÍµ≠:**")
        for i, country in enumerate(minor_countries, 1):
            if country in df['ÏàòÏûÖÍµ≠'].values:
                count = (df['ÏàòÏûÖÍµ≠'] == country).sum()
                st.write(f"{i:2d}. {country} ({count}Í±¥)")
        
        # ÏßÄÏó≠Î≥Ñ Í∑∏Î£πÌôî Í≤∞Í≥º
        st.write("**ÏßÄÏó≠Î≥Ñ Í∑∏Î£πÌôî Í≤∞Í≥º:**")
        for region in country_processor.regions.keys():
            region_key = f"{region}_Í∏∞ÌÉÄ"
            if region_key in df_processed['ÏàòÏûÖÍµ≠_processed'].values:
                count = (df_processed['ÏàòÏûÖÍµ≠_processed'] == region_key).sum()
                st.write(f"‚Ä¢ {region_key}: {count}Í±¥")

def create_similarity_search_interface(system, df):
    """Í∞úÏÑ†Îêú Ïú†ÏÇ¨ÏÇ¨Î°Ä Í≤ÄÏÉâ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
    st.subheader("üîç Ïú†ÏÇ¨ÏÇ¨Î°Ä Í≤ÄÏÉâ")
    
    # AI Î™®Îç∏ ÏÉÅÌÉú
    model_ready = system.initialize_ai_model()
    if model_ready:
        st.success("‚úÖ AI ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Î™®Îç∏ Ï§ÄÎπÑ ÏôÑÎ£å")
    else:
        st.warning("‚ö†Ô∏è AI Î™®Îç∏ ÏÇ¨Ïö© Î∂àÍ∞Ä - Í∏∞Î≥∏ Í≤ÄÏÉâ Î™®Îìú")
    
    
    # Í∞ÄÏ§ëÏπò Ï°∞Ï†ï (form Î∞ñÏóê Î∞∞Ïπò)
    with st.expander("üîß Í∞ÄÏ§ëÏπò Ï°∞Ï†ï (Í≥†Í∏â ÏÑ§Ï†ï)", expanded=False):
        st.write("**ÌòÑÏû¨ Í∞ÄÏ§ëÏπò ÏÑ§Ï†ï:**")
        
        # ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ
        text_weight = st.slider(
            "ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ Í∞ÄÏ§ëÏπò:",
            min_value=0.0,
            max_value=1.0,
            value=system.optimal_weights['text_similarity'],
            step=0.05,
            help="ÏÇ¨Í≥†ÏÑ§Î™Ö ÌÖçÏä§Ìä∏Ïùò Ï§ëÏöîÎèÑ"
        )
        
        # ÏÇ¨Í≥†Ïú†Ìòï
        accident_weight = st.slider(
            "ÏÇ¨Í≥†Ïú†Ìòï Í∞ÄÏ§ëÏπò:",
            min_value=0.0,
            max_value=1.0,
            value=system.optimal_weights['accident_type'],
            step=0.05,
            help="ÏÇ¨Í≥†Ïú†ÌòïÏùò Ï§ëÏöîÎèÑ"
        )
        
        # Íµ≠Í∞Ä
        country_weight = st.slider(
            "Íµ≠Í∞Ä Ïú†ÏÇ¨ÎèÑ Í∞ÄÏ§ëÏπò:",
            min_value=0.0,
            max_value=1.0,
            value=system.optimal_weights['country_similarity'],
            step=0.05,
            help="ÏàòÏûÖÍµ≠Ïùò Ï§ëÏöîÎèÑ"
        )
        
        # Í∏àÏï°
        amount_weight = st.slider(
            "Í∏àÏï° Ïú†ÏÇ¨ÎèÑ Í∞ÄÏ§ëÏπò:",
            min_value=0.0,
            max_value=1.0,
            value=system.optimal_weights['amount_similarity'],
            step=0.05,
            help="ÏÇ¨Í≥†Í∏àÏï°Ïùò Ï§ëÏöîÎèÑ"
        )
        
        # Î≥¥ÌóòÏ¢ÖÎ™©
        insurance_weight = st.slider(
            "Î≥¥ÌóòÏ¢ÖÎ™© Í∞ÄÏ§ëÏπò:",
            min_value=0.0,
            max_value=1.0,
            value=system.optimal_weights['insurance_type'],
            step=0.05,
            help="Î≥¥ÌóòÏ¢ÖÎ™©Ïùò Ï§ëÏöîÎèÑ"
        )
        
        # ÏÑ†ÌÉù ÎπÑÌôúÏÑ± Ï≤¥ÌÅ¨Î∞ïÏä§: ÌäπÏ†ï ÌîºÏ≤ò ÏòÅÌñ• Ï†úÏô∏
        st.write("**ÌîºÏ≤ò ÏÇ¨Ïö© Ïó¨Î∂Ä(ÏÑ†ÌÉù ÏïàÌï® Í∞ÄÎä•):**")
        use_text = st.checkbox("ÌÖçÏä§Ìä∏ ÏÇ¨Ïö©", value=True)
        use_accident = st.checkbox("ÏÇ¨Í≥†Ïú†Ìòï ÏÇ¨Ïö©", value=True)
        use_country = st.checkbox("Íµ≠Í∞Ä ÏÇ¨Ïö©", value=True)
        use_amount = st.checkbox("Í∏àÏï° ÏÇ¨Ïö©", value=True)
        use_insurance = st.checkbox("Î≥¥ÌóòÏ¢ÖÎ™© ÏÇ¨Ïö©", value=True)

        # ÏÇ¨Ïö© ÏïàÌï®Ïù¥Î©¥ Ìï¥Îãπ Í∞ÄÏ§ëÏπòÎ•º 0ÏúºÎ°ú Í∞ÑÏ£º
        text_weight = text_weight if use_text else 0.0
        accident_weight = accident_weight if use_accident else 0.0
        country_weight = country_weight if use_country else 0.0
        amount_weight = amount_weight if use_amount else 0.0
        insurance_weight = insurance_weight if use_insurance else 0.0

        # Í∞ÄÏ§ëÏπò Ìï©Í≥Ñ ÌôïÏù∏(Ï†ïÍ∑úÌôîÎäî ÌïòÏßÄ ÏïäÏùå: Ï†àÎåÄ Í∞ÄÏ§ëÏúºÎ°ú Ï≤òÎ¶¨)
        total_weight = text_weight + accident_weight + country_weight + amount_weight + insurance_weight
        st.info(f"Í∞ÄÏ§ëÏπò Ìï©Í≥Ñ: {total_weight:.2f} (Ï†àÎåÄ Í∞ÄÏ§ë)")
        
        # Í∞ÄÏ§ëÏπò Ï†ÅÏö© Î≤ÑÌäº
        if st.button("Í∞ÄÏ§ëÏπò Ï†ÅÏö©"):
            # Îü∞ÌÉÄÏûÑ Ïò§Î≤ÑÎùºÏù¥Îìú Î∞òÏòÅ(ÏÑ∏ÏÖò ÎèôÏïàÎßå Ï†ÅÏö©)
            system.runtime_weight_overrides = {
                'text_similarity': text_weight,
                'accident_type': accident_weight,
                'country_similarity': country_weight,
                'amount_similarity': amount_weight,
                'insurance_type': insurance_weight,
                # ÎÇòÎ®∏ÏßÄ Ìï≠Î™©ÏùÄ ÏõêÎûò ÏµúÏ†ÅÍ∞í Ïú†ÏßÄ(Ïò§Î≤ÑÎùºÏù¥ÎìúÏóêÏÑú Ï†úÍ≥µ Ïïà Ìï®)
                'product_category': system.optimal_weights.get('product_category', 0.0),
                'coverage_rate': system.optimal_weights.get('coverage_rate', 0.0),
                'payment_method': system.optimal_weights.get('payment_method', 0.0),
                'payment_terms': system.optimal_weights.get('payment_terms', 0.0),
                'future_outlook': system.optimal_weights.get('future_outlook', 0.0),
            }
            st.success("Í∞ÄÏ§ëÏπò(ÏÇ¨Ïö© ÏïàÌï® Ìè¨Ìï®)Í∞Ä Ï†ÅÏö©ÎêòÏóàÏäµÎãàÎã§!")
    
    # Í≤ÄÏÉâ Ìèº
    with st.form("improved_search_form"):
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.write("**üìã ÏÇ¨Í≥† Ï†ïÎ≥¥ ÏûÖÎ†•**")
            
            input_country = st.selectbox(
                "ÏàòÏûÖÍµ≠:",
                options=df['ÏàòÏûÖÍµ≠'].value_counts().head(30).index,
                help="ÏÇ¨Í≥†Í∞Ä Î∞úÏÉùÌïú ÏàòÏûÖÍµ≠ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî"
            )
            
            input_insurance = st.selectbox(
                "Î≥¥ÌóòÏ¢ÖÎ™©:",
                options=df['Î≥¥ÌóòÏ¢ÖÎ™©'].value_counts().head(10).index
            )
            
            input_accident_type = st.selectbox(
                "ÏÇ¨Í≥†Ïú†Ìòï:",
                options=df['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'].value_counts().head(10).index
            )
            
            input_amount = st.number_input(
                "ÏÇ¨Í≥†Í∏àÏï° (Ïõê):",
                min_value=0,
                value=50000000,
                step=1000000,
                format="%d"
            )
            
            input_coverage = st.slider(
                "Î∂ÄÎ≥¥Ïú® (%):",
                min_value=0,
                max_value=100,
                value=95,
                step=5,
                help="Î≥¥Ìóò Í∞ÄÏûÖ ÎπÑÏú®ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî"
            )
            
            input_description = st.text_area(
                "ÏÇ¨Í≥†ÏÑ§Î™Ö:",
                placeholder="ÏÇ¨Í≥†Ïùò Íµ¨Ï≤¥Ï†ÅÏù∏ ÏÉÅÌô©ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî...",
                height=120
            )
            
            # Ï∂îÍ∞Ä ÏûÖÎ†• ÌïÑÎìúÎì§
            # ÏÉÅÌíàÎ∂ÑÎ•ò ÏÑ†ÌÉù
            st.write("**üì¶ ÏÉÅÌíàÎ∂ÑÎ•ò ÏÑ†ÌÉù**")
            
            # ÏÉÅÌíàÎ∂ÑÎ•òÍ∑∏Î£πÎ™Ö ÏòµÏÖò (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò - ÏÉÅÏúÑ 10Í∞ú)
            product_group_options = [
                'ÏùòÎ•ò Î∞è ÏßÅÎ¨ºÎ•ò', 'Î¨¥Í∏∞ Î∞è Ïú†Í∏∞ÌôîÌïôÏ†úÌíà', 'Ï†ÑÍ∏∞ Î∞è Ï†ÑÏûêÏ†úÌíà', 'Í∏∞Í≥ÑÎ•ò',
                'Í∏àÏÜç, ÎπÑÍ∏àÏÜçÎ•ò', 'Í≥†Î¨¥, Í∞ÄÏ£Ω', 'Ïö¥ÏÜ°Ïû•ÎπÑ Î∞è Î∂ÄÌíà', 'Î™©Ïû¨ÏôÄ ÌéÑÌîÑ, ÏßÄÎ¨ºÎ•ò',
                'ÎÜçÏàòÏÇ∞Î¨º, ÏãùÎ£åÌíà', 'Ï†ïÎ∞ÄÍ∏∞Í∏∞, ÏãúÍ≥Ñ, ÏïÖÍ∏∞, Î¨¥Í∏∞Î•ò', 'Í∏∞ÌÉÄ'
            ]
            input_product_group = st.selectbox(
                "ÏÉÅÌíàÎ∂ÑÎ•òÍ∑∏Î£πÎ™Ö:",
                options=product_group_options,
                help="ÏÉÅÌíàÏùò ÎåÄÎ∂ÑÎ•òÎ•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî"
            )
            
            # ÏÉÅÌíàÎ∂ÑÎ•òÎ™Ö ÏòµÏÖò (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò - ÏÉÅÏúÑ 20Í∞ú)
            product_options = [
                'Í∞ïÎ†•ÏÇ¨(Ìè¥Î¶¨ÏóêÏä§ÌÖåÎ•¥Ïùò Í≤É)', 'Ïô∏Î∂ÄÌëúÎ©¥Ïù¥ ÌîåÎùºÏä§Ìã±Ïâ¨Ìä∏ ÎòêÎäî Î∞©ÏßÅÏö© ÏÑ¨Ïú†Ï†úÏùò Í≤É', 'Ïù∏Ï°∞ÏÑ¨Ïú†Ï†úÏùò Í≤É',
                'Ïù∏ÏáÑÌöåÎ°ú', '4. Ìè¥Î¶¨Ïπ¥Î≥¥ÎÑ§Ïù¥Ìä∏', 'Í¥ëÏ†ÑÏßÄ(ÌÉúÏñëÏ†ÑÏßÄ, Ìè¨ÌÜ†Îã§Ïù¥Ïò§Îìú, Ìè¨ÌÜ†Ïª§Ìîå Î∞è Ìè¨ÌÜ†Î¶¥Î†àÏù¥Î•º Ìè¨Ìï®ÌïúÎã§)',
                'Ïã†Ï∞®', 'Ï§ëÍ≥†Ï∞®', 'ÏùòÎ•ò Î∞è ÏßÅÎ¨ºÎ•ò', 'Ï†ÑÏûêÏ†úÌíà', 'ÎÜçÏàòÏÇ∞Î¨º', 'ÏûêÎèôÏ∞®Î∂ÄÌíà', 'ÌôîÌïôÏ†úÌíà',
                'Í∏∞ÌÉÄÏ†úÌíà', 'Í∏∞ÌÉÄ'
            ]
            input_product = st.selectbox(
                "ÏÉÅÌíàÎ∂ÑÎ•òÎ™Ö (ÏÉÅÏÑ∏):",
                options=product_options,
                help="ÏàòÏ∂ú ÏÉÅÌíàÏùò ÏÉÅÏÑ∏ Î∂ÑÎ•òÎ•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî"
            )
            
            # Í≤∞Ï†úÎ∞©Î≤ï ÏòµÏÖò (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)
            payment_method_options = [
                'O/A(T/T Ìè¨Ìï®)', 'D/A', 'NET', 'CAD', 'L/C Usance', 'COD', 'D/P', 'L/C',
                'Ïã†Ïö©Ïπ¥Îìú', 'Í∏∞ÏÑ±Í≥†Î∞©Ïãù', 'Ïã†Ïö©Ïû• Î¨∏Î©¥ÏÉÅ Tenor', 'ÏÑ†Í∏âÍ∏àÏßÄÍ∏âÏùº', 'Í∏∞ÌÉÄ'
            ]
            input_payment_method = st.selectbox(
                "Í≤∞Ï†úÎ∞©Î≤ï:",
                options=payment_method_options,
                help="Í±∞Îûò Í≤∞Ï†ú Î∞©Î≤ïÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî"
            )
            
            # Í≤∞Ï†úÏ°∞Í±¥ ÏòµÏÖò (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)
            payment_terms_options = [
                'Days From B/L Date', 'Days After B/L Date', 'Days After Invoice Date',
                'Days From Invoice Date', 'Days After Sight', 'ÏõîÎßê ÎßàÍ∞êÌõÑ', 'Days After Arrival',
                'After Delivery Date(NetÍ±∞ÎûòÎ°ú ÏàòÏûÖÏßÄÏù∏ÎèÑÏùº Í∏∞Ï§Ä Î≥¥ÌóòÍ∏∞ÏÇ∞)', 'Days From Nego Date',
                'After Finding Docs', 'At Sight', 'Days After Nego Date', 'Days From Arrival',
                'After Delivery Date(Íµ≠ÎÇ¥ÏàòÏ∂úÏùº Í∏∞Ï§Ä Î≥¥ÌóòÍ∏∞ÏÇ∞)', 'At', 'On Arrival Of Goods',
                'Îß§Ïõî 15ÏùºÏûê ÎßàÍ∞êÌõÑ', 'Í∏∞ÌÉÄ'
            ]
            input_payment_terms = st.selectbox(
                "Í≤∞Ï†úÏ°∞Í±¥:",
                options=payment_terms_options,
                help="Í≤∞Ï†ú Ï°∞Í±¥ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî"
            )
            
            # Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù ÏòµÏÖò (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)
            future_outlook_options = [
                'ÌåêÎã®Î∂àÍ∞Ä', 'Í≤∞Ï†úÎ∂àÎä•', 'Í∏∞ÌÉÄ', 'Ï†ÑÎß§Í∞ÄÎä•', 'ÏùºÎ∂ÄÍ≤∞Ï†úÍ∞ÄÎä•', 'Í≤∞Ï†úÏßÑÌñâÏ§ë'
            ]
            input_future_outlook = st.selectbox(
                "Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù:",
                options=future_outlook_options,
                help="Ìñ•ÌõÑ Í≤∞Ï†ú Ï†ÑÎßùÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî"
            )
        
        with col2:
            st.write("**üîß Í≤ÄÏÉâ ÏÑ§Ï†ï**")
            
            max_results = st.slider("ÏµúÎåÄ Í≤∞Í≥º Ïàò:", 3, 10, 5)
            
            st.write("**üìä Íµ≠Í∞Ä Ï≤òÎ¶¨ Ï†ïÎ≥¥**")
            
            # ÏûÖÎ†• Íµ≠Í∞ÄÏùò Ï≤òÎ¶¨ Í≤∞Í≥º ÎØ∏Î¶¨Î≥¥Í∏∞
            processed_country = system.country_processor.preprocess_country(input_country)
            is_individual = system.country_processor.is_individual_country(input_country)
            is_minor = system.country_processor.is_minor_country(input_country)
            region = system.country_processor.get_country_region(input_country)
            
            st.write(f"‚Ä¢ ÏûÖÎ†•Íµ≠Í∞Ä: **{input_country}**")
            st.write(f"‚Ä¢ Ï≤òÎ¶¨Í≤∞Í≥º: **{processed_country}**")
            st.write(f"‚Ä¢ ÏßÄÏó≠: **{region}**")
            st.write(f"‚Ä¢ Ïú†Ìòï: **{'Í∞úÎ≥ÑÍµ≠Í∞Ä' if is_individual else 'ÏÜåÍ∑úÎ™®Íµ≠Í∞Ä' if is_minor else 'Í∏∞ÌÉÄ'}**")
            
            # Í≤ÄÏÉâ ÌïÑÌÑ∞ ÏÑ§Ï†ï
            st.write("**üîç Í≤ÄÏÉâ ÌïÑÌÑ∞ ÏÑ§Ï†ï**")
            filter_col1, filter_col2, filter_col3 = st.columns(3)
            
            with filter_col1:
                use_country_filter = st.checkbox("ÏàòÏûÖÍµ≠ ÌïÑÌÑ∞ Ï†ÅÏö©", value=False, help="ÏÑ†ÌÉùÌïú ÏàòÏûÖÍµ≠Í≥º ÎèôÏùºÌïú ÏÇ¨Î°ÄÎßå Í≤ÄÏÉâ")
            with filter_col2:
                use_insurance_filter = st.checkbox("Î≥¥ÌóòÏ¢ÖÎ™© ÌïÑÌÑ∞ Ï†ÅÏö©", value=False, help="ÏÑ†ÌÉùÌïú Î≥¥ÌóòÏ¢ÖÎ™©Í≥º ÎèôÏùºÌïú ÏÇ¨Î°ÄÎßå Í≤ÄÏÉâ")
            with filter_col3:
                use_accident_filter = st.checkbox("ÏÇ¨Í≥†Ïú†Ìòï ÌïÑÌÑ∞ Ï†ÅÏö©", value=False, help="ÏÑ†ÌÉùÌïú ÏÇ¨Í≥†Ïú†ÌòïÍ≥º ÎèôÏùºÌïú ÏÇ¨Î°ÄÎßå Í≤ÄÏÉâ")
            
        

            
            st.write("**‚öñÔ∏è Í∏∞Î≥∏ Í∞ÄÏ§ëÏπò**")
            st.write("‚Ä¢ ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ: 35%")
            st.write("‚Ä¢ ÏÇ¨Í≥†Ïú†Ìòï: 20%")
            st.write("‚Ä¢ Íµ≠Í∞Ä Ïú†ÏÇ¨ÎèÑ: 12% ‚≠ê")
            st.write("‚Ä¢ Í∏àÏï°Ïú†ÏÇ¨ÎèÑ: 8%")
            st.write("‚Ä¢ Î≥¥ÌóòÏ¢ÖÎ™©: 5%")
            st.write("‚Ä¢ ÏÉÅÌíàÎ∂ÑÎ•ò: 8%")
            st.write("‚Ä¢ Î∂ÄÎ≥¥Ïú®: 5%")
            st.write("‚Ä¢ Í≤∞Ï†úÎ∞©Î≤ï: 4%")
            st.write("‚Ä¢ Í≤∞Ï†úÏ°∞Í±¥: 2%")
            st.write("‚Ä¢ Ìñ•ÌõÑÏ†ÑÎßù: 1%")

            
        submitted = st.form_submit_button("üîç Í≤ÄÏÉâ Ïã§Ìñâ", type="primary")
    
    if submitted:
        start_time = time.time()
        
        # ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± (ÌôïÏû•Îêú Î≥ÄÏàò Ìè¨Ìï®)
        case_data = {
            'ÏàòÏûÖÍµ≠': input_country,
            'Î≥¥ÌóòÏ¢ÖÎ™©': input_insurance,
            'ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö': input_accident_type,
            'ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°': input_amount,
            'ÏÇ¨Í≥†ÏÑ§Î™Ö': input_description,
            'Î∂ÄÎ≥¥Ïú®': input_coverage,
            'ÏÉÅÌíàÎ∂ÑÎ•òÎ™Ö': input_product,
            'ÏÉÅÌíàÎ∂ÑÎ•òÍ∑∏Î£πÎ™Ö': input_product_group,
            'Í≤∞Ï†úÎ∞©Î≤ï': input_payment_method,
            'Í≤∞Ï†úÏ°∞Í±¥': input_payment_terms,
            'Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù': input_future_outlook
        }
        
        with st.spinner(" Ïú†ÏÇ¨ÏÇ¨Î°Ä Í≤ÄÏÉâÌïòÎäî Ï§ë..."):
            # ÏùòÎØ∏ÏûàÎäî ÏÑ§Î™ÖÏù¥ ÏûàÎäî ÏÇ¨Î°Ä Ïö∞ÏÑ†
            search_df = df.copy()
            if input_description:
                meaningful_df = search_df[
                    (search_df['ÏÇ¨Í≥†ÏÑ§Î™Ö'].notna()) & 
                    (search_df['ÏÇ¨Í≥†ÏÑ§Î™Ö'].str.len() > 10) &
                    (~search_df['ÏÇ¨Í≥†ÏÑ§Î™Ö'].str.contains('ÏÑ§Î™ÖÏóÜÏùå|Ï≤®Î∂ÄÌååÏùºÏ∞∏Í≥†|Ìï¥ÎãπÏóÜÏùå', na=False, case=False))
                ]
                if len(meaningful_df) > 50:
                    search_df = meaningful_df
            
            # ÌïÑÌÑ∞ Ï†ÅÏö©
            if use_country_filter:
                search_df = search_df[search_df['ÏàòÏûÖÍµ≠'] == input_country]
                st.info(f"üîç ÏàòÏûÖÍµ≠ ÌïÑÌÑ∞ Ï†ÅÏö©: {input_country} ({len(search_df)}Í±¥)")
            
            if use_insurance_filter:
                search_df = search_df[search_df['Î≥¥ÌóòÏ¢ÖÎ™©'] == input_insurance]
                st.info(f"üîç Î≥¥ÌóòÏ¢ÖÎ™© ÌïÑÌÑ∞ Ï†ÅÏö©: {input_insurance} ({len(search_df)}Í±¥)")
            
            if use_accident_filter:
                search_df = search_df[search_df['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'] == input_accident_type]
                st.info(f"üîç ÏÇ¨Í≥†Ïú†Ìòï ÌïÑÌÑ∞ Ï†ÅÏö©: {input_accident_type} ({len(search_df)}Í±¥)")
            
            if len(search_df) == 0:
                st.error("‚ùå ÌïÑÌÑ∞ Ï°∞Í±¥Ïóê ÎßûÎäî ÏÇ¨Î°ÄÍ∞Ä ÏóÜÏäµÎãàÎã§. ÌïÑÌÑ∞Î•º Ï°∞Ï†ïÌï¥Ï£ºÏÑ∏Ïöî.")
                return
            
            # Í∞úÏÑ†Îêú Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            similarities = system.calculate_similarity_scores(case_data, search_df)
            # ÏÉÅÏúÑ 5Í±¥Îßå, Ïú†ÏÇ¨ÎèÑ 0.30 ÎØ∏Îßå Ï†úÏô∏
            min_score = 0.30
            top_similar = [r for r in similarities if r[0] >= min_score][:5]
            
            search_time = time.time() - start_time
        
        # ÌïòÏù¥Î∏åÎ¶¨Îìú Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
        if top_similar:
            decisions = [case[3]['ÌåêÏ†ïÍµ¨Î∂Ñ'] for case in top_similar]
            reasons = [case[3]['ÌåêÏ†ïÏÇ¨Ïú†'] for case in top_similar]
            similarity_scores = [case[0] for case in top_similar]
            
            confidence_result = system.confidence_calculator.hybrid_confidence(decisions, similarity_scores)
            
            # ÌåêÏ†ïÏÇ¨Ïú† Î∂ÑÏÑù
            reason_analysis = system._analyze_judgment_reasons(decisions, reasons, similarity_scores)
            
            # Ïã†Î¢∞ÎèÑ Í≤∞Í≥º ÌëúÏãú
            pred_decision = confidence_result['predicted_decision']
            confidence = confidence_result['confidence']
            grade = confidence_result['grade']
            
            if pred_decision == 'ÏßÄÍ∏â':
                box_class = "confidence-box success-box"
            elif pred_decision == 'Î©¥Ï±Ö':
                box_class = "confidence-box error-box"
            else:
                box_class = "confidence-box warning-box"
            
            st.markdown(f"""
            <div class="{box_class}">
              <!--  <h2>üéØ ÏòàÏÉÅ ÌåêÏ†ï: {pred_decision}</h2>
                <h3>ÌïòÏù¥Î∏åÎ¶¨Îìú Ïã†Î¢∞ÎèÑ: {confidence:.1%} ({grade})</h3>-->
                <p>{confidence_result['interpretation']}</p>
                <small>Í≤ÄÏÉâ ÏãúÍ∞Ñ: {search_time:.1f}Ï¥à | Ïú†ÏÇ¨ÏÇ¨Î°Ä: {confidence_result['sample_size']}Í∞ú | ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ: {confidence_result['avg_similarity']:.1%}</small>
            </div>
            """, unsafe_allow_html=True)
            
            # Ïã†Î¢∞ÎèÑ ÏÉÅÏÑ∏ Î∂ÑÏÑù
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.write("**üìä Í∞ÄÏ§ë vs Î≤†Ïù¥ÏßÄÏïà ÎπÑÍµê**")
                st.metric("Í∞ÄÏ§ë Ïã†Î¢∞ÎèÑ", f"{confidence_result['weighted_confidence']:.1%}")
                st.metric("Î≤†Ïù¥ÏßÄÏïà Ï∂îÏ†ï", f"{confidence_result['bayesian_mean']:.1%}")
                
            with col2:
                # ÌåêÏ†ïÍµ¨Î∂Ñ Î∂ÑÌè¨
                decision_counts = Counter(decisions)
                decision_df = pd.DataFrame(list(decision_counts.items()), columns=['ÌåêÏ†ïÍµ¨Î∂Ñ', 'Í±¥Ïàò'])
                fig_pred = px.pie(
                    decision_df,
                    values='Í±¥Ïàò',
                    names='ÌåêÏ†ïÍµ¨Î∂Ñ',
                    title="Ïú†ÏÇ¨ÏÇ¨Î°Ä ÌåêÏ†ïÍµ¨Î∂Ñ Î∂ÑÌè¨",
                    color_discrete_sequence=['#28a745', '#dc3545', '#ffc107']
                )
                st.plotly_chart(fig_pred, use_container_width=True)
            
            with col3:
                # Ïã†Î¢∞Íµ¨Í∞Ñ ÏãúÍ∞ÅÌôî
                lower, upper = confidence_result['credible_interval']
                
                fig_interval = go.Figure()
                fig_interval.add_trace(go.Scatter(
                    x=[confidence_result['bayesian_mean']],
                    y=['Î≤†Ïù¥ÏßÄÏïà Ï∂îÏ†ï'],
                    mode='markers',
                    marker=dict(size=15, color='blue'),
                    name='Ï∂îÏ†ïÍ∞í'
                ))
                fig_interval.add_shape(
                    type="line",
                    x0=lower, x1=upper,
                    y0=0, y1=0,
                    line=dict(color="red", width=4),
                )
                fig_interval.update_layout(
                    title="95% Ïã†Î¢∞Íµ¨Í∞Ñ",
                    xaxis_title="ÌôïÎ•†",
                    yaxis=dict(showticklabels=False),
                    height=200
                )
                st.plotly_chart(fig_interval, use_container_width=True)
            
            # ÌåêÏ†ïÏÇ¨Ïú† Î∂ÑÏÑù Ï∂îÍ∞Ä
            st.subheader("üéØ ÌåêÏ†ïÏÇ¨Ïú† Î∂ÑÏÑù")
            
            top_reasons = reason_analysis['top_reasons']
            
            if pred_decision in top_reasons:
                decision_info = top_reasons[pred_decision]
                st.write(f"**{pred_decision} ÌåêÏ†ïÏùò Ï£ºÏöî ÏÇ¨Ïú† (ÏÉÅÏúÑ 3Í∞ú):**")
                
                for i, (reason, prob) in enumerate(decision_info['reasons']):
                    emoji = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â"
                    st.write(f"{emoji} **{reason}** ")
                    # ({prob:.1%}) Ïù¥Í±¥ ÏùºÎã® Ï†úÏô∏
            else:
                st.info("Ìï¥Îãπ ÌåêÏ†ïÍµ¨Î∂ÑÏùò ÏÇ¨Ïú† Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§.")
            
            # Î™®Îì† ÌåêÏ†ïÍµ¨Î∂ÑÎ≥Ñ ÏÇ¨Ïú† ÏöîÏïΩ
            if len(top_reasons) > 1:
                st.write("**üìä ÌåêÏ†ïÍµ¨Î∂ÑÎ≥Ñ Ï£ºÏöî ÏÇ¨Ïú† ÎπÑÍµê**")
                
                reason_comparison = []
                for decision, info in top_reasons.items():
                    for reason, prob in info['reasons'][:2]:  # ÏÉÅÏúÑ 2Í∞úÎßå
                        reason_comparison.append({
                            'ÌåêÏ†ïÍµ¨Î∂Ñ': decision,
                            'ÌåêÏ†ïÏÇ¨Ïú†': reason,
                            'Í∞ÄÏ§ëÌôïÎ•†': prob,
                            'ÏÇ¨Î°ÄÏàò': info['total_cases']
                        })
                
                if reason_comparison:
                    reason_df = pd.DataFrame(reason_comparison)
                    
                    # ÌåêÏ†ïÏÇ¨Ïú†Î≥Ñ ÎßâÎåÄ Ï∞®Ìä∏
                    fig_reasons = px.bar(
                        reason_df,
                        x='ÌåêÏ†ïÏÇ¨Ïú†',
                        y='Í∞ÄÏ§ëÌôïÎ•†',
                        color='ÌåêÏ†ïÍµ¨Î∂Ñ',
                        title="ÌåêÏ†ïÍµ¨Î∂ÑÎ≥Ñ Ï£ºÏöî ÏÇ¨Ïú†",
                        color_discrete_sequence=['#28a745', '#dc3545', '#ffc107']
                    )
                    fig_reasons.update_layout(
                        xaxis_title="ÌåêÏ†ïÏÇ¨Ïú†",
                        xaxis_tickangle=45,
                        yaxis_title="Í∞ÄÏ§ëÌôïÎ•†",
                        yaxis_tickformat='.1%'
                    )
                    st.plotly_chart(fig_reasons, use_container_width=True)
            
            # ÏÉÅÏÑ∏ Í≤∞Í≥º
            st.subheader("üìã Ïú†ÏÇ¨ÏÇ¨Î°Ä ÏÉÅÏÑ∏ Î∂ÑÏÑù")
            
            # Î©¥Ï±Ö ÏÇ¨Î°Ä Ìè¨Ìï® Ïó¨Î∂Ä ÌôïÏù∏ Î∞è Í≤ΩÍ≥† ÌëúÏãú
            exemption_cases = [case for _, _, _, case in top_similar if case['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö']
            if exemption_cases:
                st.warning(f"üõ°Ô∏è **Î©¥Ï±Ö Í≤ΩÍ≥†**: {len(exemption_cases)}Í±¥Ïùò Î©¥Ï±Ö ÏÇ¨Î°ÄÍ∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Î∞òÎìúÏãú ÌôïÏù∏ÌïòÏÑ∏Ïöî!")
                
                # Î©¥Ï±Ö Ïú†ÏÇ¨ÎèÑ Í∏∞Î∞ò Î∂ÑÏÑù
                st.subheader("üõ°Ô∏è Î©¥Ï±Ö Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù")
                exemption_scores = [(score, case) for score, _, _, case in top_similar if case['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö']
                
                if exemption_scores:
                    avg_exemption_score = sum(score for score, _ in exemption_scores) / len(exemption_scores)
                    max_exemption_score = max(score for score, _ in exemption_scores)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Î©¥Ï±Ö ÏÇ¨Î°Ä Ïàò", len(exemption_scores))
                    with col2:
                        st.metric("ÌèâÍ∑† Î©¥Ï±Ö Ïú†ÏÇ¨ÎèÑ", f"{avg_exemption_score:.3f}")
                    with col3:
                        st.metric("ÏµúÍ≥† Î©¥Ï±Ö Ïú†ÏÇ¨ÎèÑ", f"{max_exemption_score:.3f}")
                    
                    # Î©¥Ï±Ö ÏúÑÌóòÎèÑ ÌèâÍ∞Ä
                    if max_exemption_score > 0.8:
                        st.error("üö® **ÎÜíÏùÄ Î©¥Ï±Ö ÏúÑÌóò**: Îß§Ïö∞ Ïú†ÏÇ¨Ìïú Î©¥Ï±Ö ÏÇ¨Î°ÄÍ∞Ä Ï°¥Ïû¨Ìï©ÎãàÎã§!")
                    elif max_exemption_score > 0.6:
                        st.warning("‚ö†Ô∏è **Ï§ëÍ∞Ñ Î©¥Ï±Ö ÏúÑÌóò**: Ïú†ÏÇ¨Ìïú Î©¥Ï±Ö ÏÇ¨Î°ÄÍ∞Ä ÏûàÏäµÎãàÎã§.")
                    else:
                        st.info("‚ÑπÔ∏è **ÎÇÆÏùÄ Î©¥Ï±Ö ÏúÑÌóò**: Î©¥Ï±Ö ÏÇ¨Î°ÄÏôÄÏùò Ïú†ÏÇ¨ÎèÑÍ∞Ä ÎÇÆÏäµÎãàÎã§.")
            
            for i, (total_score, text_sim, country_sim, similar_case) in enumerate(top_similar):
                # Î©¥Ï±Ö ÏÇ¨Î°ÄÎäî ÌäπÎ≥Ñ ÌëúÏãú
                if similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö':
                    expander_title = f"üõ°Ô∏è #{i+1} Ï¢ÖÌï©Ïú†ÏÇ¨ÎèÑ {total_score*100:.1f}% - ‚ö†Ô∏è **{similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ']}** ({similar_case['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö']}) ‚ö†Ô∏è"
                    expanded = True  # Î©¥Ï±ÖÏùÄ Í∏∞Î≥∏ ÌéºÏ≥êÏßê
                else:
                    expander_title = f"#{i+1} Ï¢ÖÌï©Ïú†ÏÇ¨ÎèÑ {total_score*100:.1f}% - {similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ']} ({similar_case['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö']})"
                    expanded = False
                
                with st.expander(expander_title, expanded=expanded):
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**üìã ÏÇ¨Î°Ä Ï†ïÎ≥¥**")
                        st.write(f"‚Ä¢ Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏: `{similar_case['Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏']}`")
                        st.write(f"‚Ä¢ ÏÇ¨Í≥†Î≤àÌò∏: `{similar_case['ÏÇ¨Í≥†Î≤àÌò∏']}`")
                        st.write(f"‚Ä¢ ÏàòÏûÖÍµ≠: **{similar_case['ÏàòÏûÖÍµ≠']}**")
                        st.write(f"‚Ä¢ Î≥¥ÌóòÏ¢ÖÎ™©: {similar_case['Î≥¥ÌóòÏ¢ÖÎ™©']}")
                        
                        if pd.notna(similar_case['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°']):
                            amount_str = f"{similar_case['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°']:,.0f}Ïõê"
                            if similar_case['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'] >= 100000000:
                                amount_str += f" ({similar_case['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°']/100000000:.1f}ÏñµÏõê)"
                            st.write(f"‚Ä¢ ÏÇ¨Í≥†Í∏àÏï°: **{amount_str}**")
                    
                    with col2:
                        st.write("**‚öñÔ∏è ÌåêÏ†ï Ï†ïÎ≥¥**")
                        
                        if similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'ÏßÄÍ∏â':
                            st.success(f"ÌåêÏ†ïÍµ¨Î∂Ñ: {similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ']}")
                        elif similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö':
                            st.error(f"ÌåêÏ†ïÍµ¨Î∂Ñ: {similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ']}")
                        else:
                            st.warning(f"ÌåêÏ†ïÍµ¨Î∂Ñ: {similar_case['ÌåêÏ†ïÍµ¨Î∂Ñ']}")
                        
                        st.write(f"‚Ä¢ ÌåêÏ†ïÏÇ¨Ïú†: **{similar_case['ÌåêÏ†ïÏÇ¨Ïú†']}**")
                        st.write(f"‚Ä¢ ÌåêÏ†ïÌöåÏ∞®: {similar_case['ÌåêÏ†ïÌöåÏ∞®']}Ìöå")
                        st.write(f"‚Ä¢ ÏÇ¨Í≥†ÏßÑÌñâÏÉÅÌÉú: {similar_case['ÏÇ¨Í≥†ÏßÑÌñâÏÉÅÌÉú']}")
                    
                    # Ïú†ÏÇ¨ÎèÑ ÏÉÅÏÑ∏ Î∂ÑÏÑù
                    st.write("**üìä Ïú†ÏÇ¨ÎèÑ ÏÑ∏Î∂Ä Î∂ÑÏÑù**")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ", f"{text_sim*100:.1f}%")
                    with col2:
                        st.metric("Íµ≠Í∞Ä Ïú†ÏÇ¨ÎèÑ", f"{country_sim*100:.1f}%")
                    with col3:
                        st.metric("Ï¢ÖÌï© Ïú†ÏÇ¨ÎèÑ", f"{total_score*100:.1f}%")
                    
                    # ÏßÑÌñâÎ∞î
                    total_pct = min(total_score * 100, 100)
                    st.progress(float(total_pct) / 100)
                    
                    if pd.notna(similar_case['ÏÇ¨Í≥†ÏÑ§Î™Ö']) and len(str(similar_case['ÏÇ¨Í≥†ÏÑ§Î™Ö'])) > 10:
                        st.write("**üìù ÏÇ¨Í≥†ÏÑ§Î™Ö**")
                        st.markdown(f"> {similar_case['ÏÇ¨Í≥†ÏÑ§Î™Ö']}")

                    # Ï†ÑÎ¨∏Í∞ÄÏö© ÌïµÏã¨ Î≥ÄÏàò ÏöîÏïΩ
                    st.write("**üîé ÌïµÏã¨ Î≥ÄÏàò**")
                    st.write(f"- ÏÉÅÌíàÎ∂ÑÎ•òÍ∑∏Î£πÎ™Ö: {similar_case.get('ÏÉÅÌíàÎ∂ÑÎ•òÍ∑∏Î£πÎ™Ö','-')}")
                    st.write(f"- Í≤∞Ï†úÎ∞©Î≤ï/Ï°∞Í±¥: {similar_case.get('Í≤∞Ï†úÎ∞©Î≤ï','-')} / {similar_case.get('Í≤∞Ï†úÏ°∞Í±¥','-')}")
                    st.write(f"- Î∂ÄÎ≥¥Ïú®: {similar_case.get('Î∂ÄÎ≥¥Ïú®','-')}")
                    st.write(f"- Ìñ•ÌõÑÏ†ÑÎßù: {similar_case.get('Ìñ•ÌõÑÍ≤∞Ï†úÏ†ÑÎßù','-')}")
        else:
            st.warning("Ï°∞Í±¥Ïóê ÎßûÎäî Ïú†ÏÇ¨ÏÇ¨Î°ÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")

def create_exemption_reason_tab(df):
    """Î©¥Ï±ÖÏÇ¨Ïú†Î≥Ñ ÏÇ¨Î°ÄÏ°∞Ìöå ÌÉ≠"""
    
    st.markdown("""
    <div class="main-header">
        <h2>üõ°Ô∏è Î©¥Ï±ÖÏÇ¨Ïú†Î≥Ñ ÏÇ¨Î°ÄÏ°∞Ìöå</h2>
        <p>ÌäπÏ†ï Î©¥Ï±ÖÏÇ¨Ïú†Î°ú Ïã§Ï†ú Î©¥Ï±ÖÎêú ÏÇ¨Î°ÄÎì§ÏùÑ ÌÉêÏÉâÌïòÍ≥† Î∂ÑÏÑùÌï©ÎãàÎã§.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Î©¥Ï±Ö ÏÇ¨Î°ÄÎßå ÌïÑÌÑ∞ÎßÅ
    exemption_df = df[df['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö'].copy()
    
    if len(exemption_df) == 0:
        st.warning("Î©¥Ï±Ö ÏÇ¨Î°ÄÍ∞Ä ÏóÜÏäµÎãàÎã§.")
        return
    
    # Î©¥Ï±ÖÏÇ¨Ïú† ÌÜµÍ≥Ñ
    st.subheader("üìä Î©¥Ï±ÖÏÇ¨Ïú† ÌÜµÍ≥Ñ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Ï¥ù Î©¥Ï±Ö ÏÇ¨Î°Ä", len(exemption_df))
    
    with col2:
        unique_reasons = exemption_df['ÌåêÏ†ïÏÇ¨Ïú†'].nunique()
        st.metric("Î©¥Ï±ÖÏÇ¨Ïú† Ï¢ÖÎ•ò", unique_reasons)
    
    with col3:
        avg_amount = exemption_df['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'].mean()
        if pd.notna(avg_amount):
            st.metric("ÌèâÍ∑† ÏÇ¨Í≥†Í∏àÏï°", f"{avg_amount:,.0f}Ïõê")
        else:
            st.metric("ÌèâÍ∑† ÏÇ¨Í≥†Í∏àÏï°", "N/A")
    
    # Î©¥Ï±ÖÏÇ¨Ïú†Î≥Ñ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî
    st.subheader("üìà Î©¥Ï±ÖÏÇ¨Ïú†Î≥Ñ Î∂ÑÌè¨")
    
    reason_counts = exemption_df['ÌåêÏ†ïÏÇ¨Ïú†'].value_counts().head(15)
    
    fig = px.bar(
        x=reason_counts.values,
        y=reason_counts.index,
        orientation='h',
        title="ÏÉÅÏúÑ 15Í∞ú Î©¥Ï±ÖÏÇ¨Ïú†Î≥Ñ ÏÇ¨Î°Ä Ïàò",
        labels={'x': 'ÏÇ¨Î°Ä Ïàò', 'y': 'Î©¥Ï±ÖÏÇ¨Ïú†'},
        color=reason_counts.values,
        color_continuous_scale='Reds'
    )
    
    fig.update_layout(
        height=500,
        showlegend=False,
        xaxis_title="ÏÇ¨Î°Ä Ïàò",
        yaxis_title="Î©¥Ï±ÖÏÇ¨Ïú†"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Î©¥Ï±ÖÏÇ¨Ïú† ÏÑ†ÌÉù Î∞è ÏÇ¨Î°Ä Ï°∞Ìöå
    st.subheader("üîç Î©¥Ï±ÖÏÇ¨Ïú†Î≥Ñ ÏÇ¨Î°Ä Ï°∞Ìöå")
    
    # Î©¥Ï±ÖÏÇ¨Ïú† Î™©Î°ù (ÏÇ¨Î°Ä ÏàòÏôÄ Ìï®Íªò ÌëúÏãú)
    reason_options = []
    for reason, count in reason_counts.items():
        reason_options.append(f"{reason} ({count}Í±¥)")
    
    selected_reason_full = st.selectbox(
        "Î©¥Ï±ÖÏÇ¨Ïú†Î•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:",
        options=reason_options,
        help="ÏÇ¨Î°Ä ÏàòÍ∞Ä ÎßéÏùÄ ÏàúÏúºÎ°ú Ï†ïÎ†¨ÎêòÏñ¥ ÏûàÏäµÎãàÎã§."
    )
    
    if selected_reason_full:
        # ÏÑ†ÌÉùÎêú Î©¥Ï±ÖÏÇ¨Ïú†ÏóêÏÑú ÏÇ¨Î°Ä Ïàò Ï†úÍ±∞
        selected_reason = selected_reason_full.split(" (")[0]
        
        # Ìï¥Îãπ Î©¥Ï±ÖÏÇ¨Ïú†Ïùò ÏÇ¨Î°ÄÎì§ ÌïÑÌÑ∞ÎßÅ
        filtered_cases = exemption_df[exemption_df['ÌåêÏ†ïÏÇ¨Ïú†'] == selected_reason].copy()
        
        if len(filtered_cases) > 0:
            st.success(f"‚úÖ **{selected_reason}** Î©¥Ï±ÖÏÇ¨Ïú†Î°ú Ï¥ù **{len(filtered_cases)}Í±¥**Ïùò ÏÇ¨Î°ÄÎ•º Ï∞æÏïòÏäµÎãàÎã§.")

            # Í∏∞Î≥∏: Î©¥Ï±ÖÏÇ¨Ïú†ÎßåÏúºÎ°ú ÌïÑÌÑ∞ÎßÅÎêú Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©
            display_cases = filtered_cases.copy()

            # ÏÑ†ÌÉù: Í≥†Í∏â ÌïÑÌÑ∞ (Í∏∞Î≥∏ ÎπÑÌôúÏÑ±Ìôî)
            with st.expander("‚öôÔ∏è Í≥†Í∏â ÌïÑÌÑ∞ (ÏÑ†ÌÉù)", expanded=False):
                use_advanced_filters = st.checkbox("Í≥†Í∏â ÌïÑÌÑ∞ ÏÇ¨Ïö©", value=False)
                if use_advanced_filters:
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        countries = ['Ï†ÑÏ≤¥'] + sorted(display_cases['ÏàòÏûÖÍµ≠'].dropna().unique().tolist())
                        selected_country = st.selectbox("ÏàòÏûÖÍµ≠ ÌïÑÌÑ∞:", countries)
                    with col2:
                        insurance_types = ['Ï†ÑÏ≤¥'] + sorted(display_cases['Î≥¥ÌóòÏ¢ÖÎ™©'].dropna().unique().tolist())
                        selected_insurance = st.selectbox("Î≥¥ÌóòÏ¢ÖÎ™© ÌïÑÌÑ∞:", insurance_types)
                    with col3:
                        accident_types = ['Ï†ÑÏ≤¥'] + sorted(display_cases['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'].dropna().unique().tolist())
                        selected_accident = st.selectbox("ÏÇ¨Í≥†Ïú†Ìòï ÌïÑÌÑ∞:", accident_types)

                    if selected_country != 'Ï†ÑÏ≤¥':
                        display_cases = display_cases[display_cases['ÏàòÏûÖÍµ≠'] == selected_country]
                        st.info(f"üîç ÏàòÏûÖÍµ≠ ÌïÑÌÑ∞: {selected_country} ({len(display_cases)}Í±¥)")
                    if selected_insurance != 'Ï†ÑÏ≤¥':
                        display_cases = display_cases[display_cases['Î≥¥ÌóòÏ¢ÖÎ™©'] == selected_insurance]
                        st.info(f"üîç Î≥¥ÌóòÏ¢ÖÎ™© ÌïÑÌÑ∞: {selected_insurance} ({len(display_cases)}Í±¥)")
                    if selected_accident != 'Ï†ÑÏ≤¥':
                        display_cases = display_cases[display_cases['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'] == selected_accident]
                        st.info(f"üîç ÏÇ¨Í≥†Ïú†Ìòï ÌïÑÌÑ∞: {selected_accident} ({len(display_cases)}Í±¥)")

            # ÌÇ§ÏõåÎìú Í≤ÄÏÉâ(ÏÇ¨Í≥†ÏÑ§Î™Ö ÎÇ¥ Ìè¨Ìï® Í≤ÄÏÉâ)
            keyword = st.text_input("ÌÇ§ÏõåÎìú Í≤ÄÏÉâ(ÏÇ¨Í≥†ÏÑ§Î™Ö):", value="", help="ÏÇ¨Í≥†ÏÑ§Î™ÖÏóê Ìè¨Ìï®ÎêòÎäî ÌÇ§ÏõåÎìúÎ°ú Í∞ÑÎã® Í≤ÄÏÉâ")
            if keyword:
                mask = display_cases['ÏÇ¨Í≥†ÏÑ§Î™Ö'].astype(str).str.contains(keyword, case=False, na=False)
                display_cases = display_cases[mask]
                st.info(f"üîç ÌÇ§ÏõåÎìú '{keyword}' Í≤∞Í≥º: {len(display_cases)}Í±¥")
            
            if len(display_cases) > 0:
                # Ï†ïÎ†¨ ÏòµÏÖò
                sort_options = {
                    'ÏÇ¨Í≥†Í∏àÏï° (ÎÜíÏùÄÏàú)': 'ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°',
                    'ÏÇ¨Í≥†Í∏àÏï° (ÎÇÆÏùÄÏàú)': 'ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°',
                    'ÌåêÏ†ïÌöåÏ∞® (ÎÜíÏùÄÏàú)': 'ÌåêÏ†ïÌöåÏ∞®',
                    'ÌåêÏ†ïÌöåÏ∞® (ÎÇÆÏùÄÏàú)': 'ÌåêÏ†ïÌöåÏ∞®',
                    'ÏÇ¨Í≥†Ï†ëÏàòÏùºÏûê (ÏµúÏã†Ïàú)': 'ÏÇ¨Í≥†Ï†ëÏàòÏùºÏûê',
                    'ÏÇ¨Í≥†Ï†ëÏàòÏùºÏûê (Ïò§ÎûòÎêúÏàú)': 'ÏÇ¨Í≥†Ï†ëÏàòÏùºÏûê'
                }
                
                selected_sort = st.selectbox("Ï†ïÎ†¨ Í∏∞Ï§Ä:", list(sort_options.keys()))
                
                # Ï†ïÎ†¨ Ï†ÅÏö©
                if 'ÎÜíÏùÄÏàú' in selected_sort or 'ÏµúÏã†Ïàú' in selected_sort:
                    ascending = False
                else:
                    ascending = True
                
                display_cases = display_cases.sort_values(
                    by=sort_options[selected_sort], 
                    ascending=ascending,
                    na_position='last'
                )
                
                # ÌëúÏãúÌï† Ïª¨Îüº ÏÑ†ÌÉù
                st.write("**üìã ÏÇ¨Î°Ä Î™©Î°ù**")
                
                # ÌëúÏãúÌï† Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
                display_data = display_cases[[
                    'Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏', 'ÏÇ¨Í≥†Î≤àÌò∏', 'ÌåêÏ†ïÌöåÏ∞®', 'ÏàòÏûÖÍµ≠', 'Î≥¥ÌóòÏ¢ÖÎ™©', 
                    'ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö', 'ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°', 'ÏÇ¨Í≥†ÏÑ§Î™Ö', 'ÏÇ¨Í≥†ÏßÑÌñâÏÉÅÌÉú'
                ]].copy()
                
                # Í∏àÏï° Ìè¨Îß∑ÌåÖ
                display_data['ÏÇ¨Í≥†Í∏àÏï°'] = display_data['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'].apply(
                    lambda x: f"{x:,.0f}Ïõê" if pd.notna(x) else "N/A"
                )
                
                # ÏÇ¨Í≥†ÏÑ§Î™Ö ÏöîÏïΩ (50Ïûê Ï†úÌïú)
                display_data['ÏÇ¨Í≥†ÏÑ§Î™Ö_ÏöîÏïΩ'] = display_data['ÏÇ¨Í≥†ÏÑ§Î™Ö'].apply(
                    lambda x: str(x)[:50] + "..." if pd.notna(x) and len(str(x)) > 50 else str(x) if pd.notna(x) else "N/A"
                )
                
                # ÏµúÏ¢Ö ÌëúÏãú Ïª¨Îüº
                final_columns = [
                    'Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏', 'ÏÇ¨Í≥†Î≤àÌò∏', 'ÌåêÏ†ïÌöåÏ∞®', 'ÏàòÏûÖÍµ≠', 'Î≥¥ÌóòÏ¢ÖÎ™©',
                    'ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö', 'ÏÇ¨Í≥†Í∏àÏï°', 'ÏÇ¨Í≥†ÏÑ§Î™Ö_ÏöîÏïΩ', 'ÏÇ¨Í≥†ÏßÑÌñâÏÉÅÌÉú'
                ]
                
                # Ïª¨ÎüºÎ™Ö ÌïúÍ∏ÄÌôî
                column_mapping = {
                    'Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏': 'Î≥¥ÏÉÅÌååÏùºÎ≤àÌò∏',
                    'ÏÇ¨Í≥†Î≤àÌò∏': 'ÏÇ¨Í≥†Î≤àÌò∏', 
                    'ÌåêÏ†ïÌöåÏ∞®': 'ÌåêÏ†ïÌöåÏ∞®',
                    'ÏàòÏûÖÍµ≠': 'ÏàòÏûÖÍµ≠',
                    'Î≥¥ÌóòÏ¢ÖÎ™©': 'Î≥¥ÌóòÏ¢ÖÎ™©',
                    'ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö': 'ÏÇ¨Í≥†Ïú†Ìòï',
                    'ÏÇ¨Í≥†Í∏àÏï°': 'ÏÇ¨Í≥†Í∏àÏï°',
                    'ÏÇ¨Í≥†ÏÑ§Î™Ö_ÏöîÏïΩ': 'ÏÇ¨Í≥†ÏÑ§Î™Ö',
                    'ÏÇ¨Í≥†ÏßÑÌñâÏÉÅÌÉú': 'ÏßÑÌñâÏÉÅÌÉú'
                }
                
                display_data = display_data[final_columns].rename(columns=column_mapping)
                
                # ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò
                items_per_page = 20
                total_items = len(display_data)
                total_pages = (total_items + items_per_page - 1) // items_per_page
                
                if total_pages > 1:
                    current_page = st.selectbox(
                        f"ÌéòÏù¥ÏßÄ ÏÑ†ÌÉù (Ï¥ù {total_pages}ÌéòÏù¥ÏßÄ):",
                        range(1, total_pages + 1)
                    ) - 1
                else:
                    current_page = 0
                
                start_idx = current_page * items_per_page
                end_idx = min(start_idx + items_per_page, total_items)
                
                # ÌòÑÏû¨ ÌéòÏù¥ÏßÄ Îç∞Ïù¥ÌÑ∞ ÌëúÏãú
                current_data = display_data.iloc[start_idx:end_idx]
                
                st.dataframe(
                    current_data,
                    use_container_width=True,
                    hide_index=True
                )
                
                # ÌéòÏù¥ÏßÄ Ï†ïÎ≥¥
                if total_pages > 1:
                    st.write(f"üìÑ {start_idx + 1}~{end_idx} / {total_items}Í±¥ (ÌéòÏù¥ÏßÄ {current_page + 1}/{total_pages})")
                
                # Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ Í∏∞Îä• Ï∂îÍ∞Ä
                st.markdown("---")
                st.subheader("üîç Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ Í∏∞Îä•")
                
                # Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ Î∞©Ïãù ÏÑ†ÌÉù(Í∏∞Î≥∏: Î©¥Ï±ÖÏÇ¨Ïú†Îßå ÌïÑÌÑ∞Îêú ÌíÄÏóêÏÑú Í≤ÄÏÉâ)
                search_method = st.radio(
                    "Í≤ÄÏÉâ Î∞©ÏãùÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:",
                    [
                        "Î∞©Ïãù A: Ìï¥Îãπ Î©¥Ï±ÖÏÇ¨Ïú† ÏÇ¨Î°ÄÎì§(ÌòÑÏû¨ ÌëúÏãúÎêú Î™©Î°ù)ÏóêÏÑú Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ",
                        "Î∞©Ïãù B: Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ìï¥Îãπ Î©¥Ï±ÖÏÇ¨Ïú†ÏôÄ Ïú†ÏÇ¨Ìïú ÏÇ¨Î°Ä Í≤ÄÏÉâ", 
                        "Î∞©Ïãù C: Î≥µÌï© Ï°∞Í±¥ÏúºÎ°ú Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ"
                    ],
                    help="A Í∂åÏû•: Í≥ºÎèÑÌïú ÏÑ†ÌïÑÌÑ∞Î°ú 0Í±¥ Î∞©ÏßÄ"
                )
                
                # Í≤ÄÏÉâÏñ¥ ÏûÖÎ†•
                search_text = st.text_area(
                    "Í≤ÄÏÉâÌï† ÏÇ¨Í≥†ÏÑ§Î™ÖÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî:",
                    placeholder="Ïòà: ÏàòÏ∂ú ÏßÄÏó∞, ÏßÄÍ∏â Í±∞Ï†à, Í≥ÑÏïΩ ÏúÑÎ∞ò Îì±",
                    height=100,
                    help="ÏÇ¨Í≥†Ïùò Ï£ºÏöî ÎÇ¥Ïö©ÏùÑ ÏûêÏú†Î°≠Í≤å ÏûÖÎ†•ÌïòÏÑ∏Ïöî"
                )
                
                # Ï∂îÍ∞Ä Ï°∞Í±¥ (Î∞©Ïãù CÏö©)
                additional_conditions = {}
                if "Î∞©Ïãù C" in search_method:
                    st.write("**Ï∂îÍ∞Ä Í≤ÄÏÉâ Ï°∞Í±¥:**")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        additional_conditions['country'] = st.selectbox(
                            "ÏàòÏûÖÍµ≠ (ÏÑ†ÌÉùÏÇ¨Ìï≠):",
                            ['Ï†ÑÏ≤¥'] + sorted(df['ÏàòÏûÖÍµ≠'].unique().tolist())
                        )
                        additional_conditions['insurance'] = st.selectbox(
                            "Î≥¥ÌóòÏ¢ÖÎ™© (ÏÑ†ÌÉùÏÇ¨Ìï≠):",
                            ['Ï†ÑÏ≤¥'] + sorted(df['Î≥¥ÌóòÏ¢ÖÎ™©'].unique().tolist())
                        )
                    
                    with col2:
                        additional_conditions['accident_type'] = st.selectbox(
                            "ÏÇ¨Í≥†Ïú†Ìòï (ÏÑ†ÌÉùÏÇ¨Ìï≠):",
                            ['Ï†ÑÏ≤¥'] + sorted(df['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'].unique().tolist())
                        )
                        additional_conditions['amount_range'] = st.selectbox(
                            "ÏÇ¨Í≥†Í∏àÏï° Î≤îÏúÑ (ÏÑ†ÌÉùÏÇ¨Ìï≠):",
                            ['Ï†ÑÏ≤¥', '1000ÎßåÏõê Ïù¥Ìïò', '1000ÎßåÏõê-5000ÎßåÏõê', '5000ÎßåÏõê-1ÏñµÏõê', '1ÏñµÏõê Ïù¥ÏÉÅ']
                        )
                
                # Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ Ïã§Ìñâ
                if st.button("üîç Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ Ïã§Ìñâ", type="primary"):
                    if search_text.strip():
                        # ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÌôïÏù∏
                        if 'improved_system' not in st.session_state:
                            st.session_state.improved_system = ImprovedInsuranceSystem()
                        system = st.session_state.improved_system
                        
                        # Í≤ÄÏÉâ Î∞©ÏãùÏóê Îî∞Î•∏ ÌõÑÎ≥¥ Îç∞Ïù¥ÌÑ∞ ÏÑ†ÌÉù
                        if "Î∞©Ïãù A" in search_method:
                            # ÌòÑÏû¨ ÌëúÏãú Î™©Î°ù(Î©¥Ï±ÖÏÇ¨Ïú† + ÏÑ†ÌÉùÏ†Å Í≥†Í∏âÌïÑÌÑ∞ + ÌÇ§ÏõåÎìú)ÏóêÏÑú Í≤ÄÏÉâ
                            candidates_df = display_cases.copy()
                            st.info(f"üîç Î∞©Ïãù A: ÌòÑÏû¨ ÌëúÏãú {len(candidates_df)}Í±¥ÏóêÏÑú Í≤ÄÏÉâ")
                            
                        elif "Î∞©Ïãù B" in search_method:
                            # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ìï¥Îãπ Î©¥Ï±ÖÏÇ¨Ïú†ÏôÄ Ïú†ÏÇ¨Ìïú ÏÇ¨Î°Ä Í≤ÄÏÉâ
                            candidates_df = df.copy()
                            st.info(f"üîç Î∞©Ïãù B: Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ {len(candidates_df)}Í±¥ÏóêÏÑú '{selected_reason}' Í¥ÄÎ†® ÏÇ¨Î°Ä Í≤ÄÏÉâ")
                            
                        else:  # Î∞©Ïãù C
                            # Î≥µÌï© Ï°∞Í±¥ Ï†ÅÏö©
                            candidates_df = df.copy()
                            
                            # Ï∂îÍ∞Ä Ï°∞Í±¥ ÌïÑÌÑ∞ÎßÅ
                            if additional_conditions['country'] != 'Ï†ÑÏ≤¥':
                                candidates_df = candidates_df[candidates_df['ÏàòÏûÖÍµ≠'] == additional_conditions['country']]
                            
                            if additional_conditions['insurance'] != 'Ï†ÑÏ≤¥':
                                candidates_df = candidates_df[candidates_df['Î≥¥ÌóòÏ¢ÖÎ™©'] == additional_conditions['insurance']]
                            
                            if additional_conditions['accident_type'] != 'Ï†ÑÏ≤¥':
                                candidates_df = candidates_df[candidates_df['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'] == additional_conditions['accident_type']]
                            
                            if additional_conditions['amount_range'] != 'Ï†ÑÏ≤¥':
                                if additional_conditions['amount_range'] == '1000ÎßåÏõê Ïù¥Ìïò':
                                    candidates_df = candidates_df[candidates_df['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'] <= 10000000]
                                elif additional_conditions['amount_range'] == '1000ÎßåÏõê-5000ÎßåÏõê':
                                    candidates_df = candidates_df[(candidates_df['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'] > 10000000) & (candidates_df['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'] <= 50000000)]
                                elif additional_conditions['amount_range'] == '5000ÎßåÏõê-1ÏñµÏõê':
                                    candidates_df = candidates_df[(candidates_df['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'] > 50000000) & (candidates_df['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'] <= 100000000)]
                                elif additional_conditions['amount_range'] == '1ÏñµÏõê Ïù¥ÏÉÅ':
                                    candidates_df = candidates_df[candidates_df['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'] > 100000000]
                            
                            st.info(f"üîç Î∞©Ïãù C: Î≥µÌï© Ï°∞Í±¥ Ï†ÅÏö© ÌõÑ {len(candidates_df)}Í±¥ÏóêÏÑú Í≤ÄÏÉâ")
                        
                        # ÏøºÎ¶¨ ÏºÄÏù¥Ïä§ ÏÉùÏÑ±
                        query_case = {
                            'ÏÇ¨Í≥†ÏÑ§Î™Ö': search_text,
                            'ÌåêÏ†ïÏÇ¨Ïú†': selected_reason,
                            'ÏàòÏûÖÍµ≠': additional_conditions.get('country', 'Ï†ÑÏ≤¥'),
                            'Î≥¥ÌóòÏ¢ÖÎ™©': additional_conditions.get('insurance', 'Ï†ÑÏ≤¥'),
                            'ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö': additional_conditions.get('accident_type', 'Ï†ÑÏ≤¥')
                        }
                        
                        # Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                        with st.spinner("Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ï§ë..."):
                            try:
                                similarities = system.calculate_similarity_scores(query_case, candidates_df)

                                # ÏÉÅÏúÑ 5Í±¥Îßå, 0.30 ÎØ∏Îßå Ï†úÏô∏
                                min_score = 0.30
                                top_items = [r for r in similarities if r[0] >= min_score][:5] if similarities else []

                                if top_items:
                                    # Í≤∞Í≥º ÌëúÏãú
                                    st.success(f"‚úÖ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ ÏôÑÎ£å! ÏÉÅÏúÑ {len(top_items)}Í∞ú Í≤∞Í≥º (ÏûÑÍ≥ÑÏπò {min_score:.2f} Ï†ÅÏö©)")

                                    # Í≤∞Í≥º ÌÖåÏù¥Î∏î ÏÉùÏÑ±
                                    results_data = []
                                    for i, (score, text_sim, country_sim, case) in enumerate(top_items, 1):
                                        results_data.append({
                                            'ÏàúÏúÑ': i,
                                            'Ïú†ÏÇ¨ÎèÑ(%)': f"{score*100:.1f}%",
                                            'ÌåêÏ†ïÍµ¨Î∂Ñ': case['ÌåêÏ†ïÍµ¨Î∂Ñ'],
                                            'ÌåêÏ†ïÏÇ¨Ïú†': case['ÌåêÏ†ïÏÇ¨Ïú†'],
                                            'ÏàòÏûÖÍµ≠': case['ÏàòÏûÖÍµ≠'],
                                            'Î≥¥ÌóòÏ¢ÖÎ™©': case['Î≥¥ÌóòÏ¢ÖÎ™©'],
                                            'ÏÇ¨Í≥†Ïú†Ìòï': case['ÏÇ¨Í≥†Ïú†ÌòïÎ™Ö'],
                                            'ÏÇ¨Í≥†Í∏àÏï°': f"{case['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°']:,.0f}Ïõê" if pd.notna(case['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°']) else "N/A",
                                            'ÏÇ¨Í≥†ÏÑ§Î™Ö': str(case['ÏÇ¨Í≥†ÏÑ§Î™Ö'])[:100] + "..." if len(str(case['ÏÇ¨Í≥†ÏÑ§Î™Ö'])) > 100 else str(case['ÏÇ¨Í≥†ÏÑ§Î™Ö'])
                                        })
                                    
                                    # Í≤∞Í≥º ÌÖåÏù¥Î∏î ÌëúÏãú
                                    results_df = pd.DataFrame(results_data)
                                    st.dataframe(
                                        results_df,
                                        use_container_width=True,
                                        hide_index=True
                                    )
                                    
                                    # Í≤ÄÏÉâ Î∞©ÏãùÎ≥Ñ ÌÜµÍ≥Ñ
                                    st.subheader("üìä Í≤ÄÏÉâ Í≤∞Í≥º Î∂ÑÏÑù")
                                    
                                    col1, col2, col3 = st.columns(3)
                                    
                                    with col1:
                                        exemption_count = sum(1 for _, _, _, case in top_items if case['ÌåêÏ†ïÍµ¨Î∂Ñ'] == 'Î©¥Ï±Ö')
                                        st.metric("Î©¥Ï±Ö ÏÇ¨Î°Ä", exemption_count)
                                    
                                    with col2:
                                        avg_similarity = sum(score for score, _, _, _ in top_items) / len(top_items)
                                        st.metric("ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ", f"{avg_similarity*100:.1f}%")
                                    
                                    with col3:
                                        max_similarity = max(score for score, _, _, _ in top_items)
                                        st.metric("ÏµúÍ≥† Ïú†ÏÇ¨ÎèÑ", f"{max_similarity*100:.1f}%")
                                    
                                    # Í≤ÄÏÉâ Î∞©ÏãùÎ≥Ñ ÌäπÏßï ÏÑ§Î™Ö
                                    st.info(f"""
                                    **üîç {search_method}**
                                    - Í≤ÄÏÉâ ÎåÄÏÉÅ: {len(candidates_df)}Í±¥
                                    - Í≤ÄÏÉâ Í≤∞Í≥º: {len(top_items)}Í±¥
                                    - Ï£ºÏöî ÌäπÏßï: {'Ìï¥Îãπ Î©¥Ï±ÖÏÇ¨Ïú† ÏÇ¨Î°ÄÎì§ Ï§ëÏóêÏÑú Ïú†ÏÇ¨ÎèÑ Í∏∞Î∞ò Ï†ïÎ†¨' if 'Î∞©Ïãù A' in search_method else 'Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Î©¥Ï±ÖÏÇ¨Ïú† Í¥ÄÎ†®ÏÑ± Í≥†Î†§' if 'Î∞©Ïãù B' in search_method else 'Î≥µÌï© Ï°∞Í±¥ + Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ'}
                                    """)
                                    
                                else:
                                    st.warning("‚ùå Ïú†ÏÇ¨Ìïú ÏÇ¨Î°ÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                                    
                            except Exception as e:
                                st.error(f"‚ùå Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")
                    else:
                        st.warning("Í≤ÄÏÉâÌï† ÏÇ¨Í≥†ÏÑ§Î™ÖÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
                
                # ÏÉÅÏÑ∏ Î∂ÑÏÑù
                st.subheader("üìä ÏÑ†ÌÉùÎêú Î©¥Ï±ÖÏÇ¨Ïú† ÏÉÅÏÑ∏ Î∂ÑÏÑù")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # ÏàòÏûÖÍµ≠Î≥Ñ Î∂ÑÌè¨
                    country_dist = display_cases['ÏàòÏûÖÍµ≠'].value_counts().head(10)
                    fig_country = px.pie(
                        values=country_dist.values,
                        names=country_dist.index,
                        title=f"ÏàòÏûÖÍµ≠Î≥Ñ Î∂ÑÌè¨ (ÏÉÅÏúÑ 10Í∞ú)"
                    )
                    st.plotly_chart(fig_country, use_container_width=True)
                
                with col2:
                    # Î≥¥ÌóòÏ¢ÖÎ™©Î≥Ñ Î∂ÑÌè¨
                    insurance_dist = display_cases['Î≥¥ÌóòÏ¢ÖÎ™©'].value_counts().head(10)
                    fig_insurance = px.pie(
                        values=insurance_dist.values,
                        names=insurance_dist.index,
                        title=f"Î≥¥ÌóòÏ¢ÖÎ™©Î≥Ñ Î∂ÑÌè¨ (ÏÉÅÏúÑ 10Í∞ú)"
                    )
                    st.plotly_chart(fig_insurance, use_container_width=True)
                
                # ÏÇ¨Í≥†Í∏àÏï° Î∂ÑÌè¨
                if display_cases['ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°'].notna().any():
                    fig_amount = px.histogram(
                        display_cases,
                        x='ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°',
                        nbins=20,
                        title=f"ÏÇ¨Í≥†Í∏àÏï° Î∂ÑÌè¨",
                        labels={'ÏõêÌôîÏÇ¨Í≥†Í∏àÏï°': 'ÏÇ¨Í≥†Í∏àÏï° (Ïõê)', 'count': 'ÏÇ¨Î°Ä Ïàò'}
                    )
                    st.plotly_chart(fig_amount, use_container_width=True)
                
                # ÌåêÏ†ïÌöåÏ∞®Î≥Ñ Î∂ÑÌè¨
                fig_rounds = px.bar(
                    x=display_cases['ÌåêÏ†ïÌöåÏ∞®'].value_counts().index,
                    y=display_cases['ÌåêÏ†ïÌöåÏ∞®'].value_counts().values,
                    title=f"ÌåêÏ†ïÌöåÏ∞®Î≥Ñ Î∂ÑÌè¨",
                    labels={'x': 'ÌåêÏ†ïÌöåÏ∞®', 'y': 'ÏÇ¨Î°Ä Ïàò'}
                )
                st.plotly_chart(fig_rounds, use_container_width=True)
                
            else:
                st.warning("ÏÑ†ÌÉùÌïú ÌïÑÌÑ∞ Ï°∞Í±¥Ïóê ÎßûÎäî ÏÇ¨Î°ÄÍ∞Ä ÏóÜÏäµÎãàÎã§.")
        else:
            st.warning(f"'{selected_reason}' Î©¥Ï±ÖÏÇ¨Ïú†Î°ú Î©¥Ï±ÖÎêú ÏÇ¨Î°ÄÍ∞Ä ÏóÜÏäµÎãàÎã§.")
    else:
        st.info("Î©¥Ï±ÖÏÇ¨Ïú†Î•º ÏÑ†ÌÉùÌïòÎ©¥ Ìï¥Îãπ ÏÇ¨Ïú†Î°ú Î©¥Ï±ÖÎêú ÏÇ¨Î°ÄÎì§ÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.")

def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    df = load_data()
    if df is None:
        st.error("Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§.")
        return
    
    # Í∞úÏÑ†Îêú ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
    if 'improved_system' not in st.session_state:
        st.session_state.improved_system = ImprovedInsuranceSystem()
    
    system = st.session_state.improved_system
    
    # Î©îÏù∏ Ìó§Îçî
    st.markdown("""
    <div class="main-header">
        <h1>üìã Íµ≠Ïô∏ Ï≤≠Íµ¨ Ïã¨ÏÇ¨ Ïú†ÏÇ¨ÏÇ¨Î°Ä Í≤ÄÏÉâ </h1>
        <p>Ïú†ÏÇ¨ ÏÇ¨Í≥† ÏÇ¨Î°Ä Í≤ÄÏÉâ ÏãúÏä§ÌÖú</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ÌÉ≠ Íµ¨ÏÑ±
    tab1, tab2, tab3 = st.tabs([
        "üîç ÏÇ¨Î°Ä Í≤ÄÏÉâ",
        "üõ°Ô∏è Î©¥Ï±ÖÏÇ¨Ïú†Î≥Ñ ÏÇ¨Î°ÄÏ°∞Ìöå",
        "üåç Íµ≠Í∞Ä Î∂ÑÏÑù"        
    ])
    
    with tab1:
        create_similarity_search_interface(system, df)
    
    with tab2:
        create_exemption_reason_tab(df)
    
    with tab3:
        create_country_analysis_tab(df, system.country_processor)
        
    
    # Ìë∏ÌÑ∞
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #6c757d; padding: 1rem;">
        <p>üìã Î≥¥ÌóòÏÇ¨Í≥† Î∂ÑÏÑù ÏãúÏä§ÌÖú</p>
        <small>Í≥ÑÏ∏µÏ†Å Íµ≠Í∞Ä Ï≤òÎ¶¨ + Î≤†Ïù¥ÏßÄÏïà Ïã†Î¢∞ÎèÑ + Ïä§ÎßàÌä∏ ÌïÑÌÑ∞ÎßÅ</small>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()