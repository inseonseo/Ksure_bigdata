import torch
import pandas as pd
import numpy as np
from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
import pickle
import os
warnings.filterwarnings('ignore')

class MultimodalSimilaritySearch:
    def __init__(self, text_weight=0.6, categorical_weight=0.3, numerical_weight=0.1):
        """
        ë‹¤ì¤‘ ëª¨ë‹¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ëª¨ë¸
        
        Args:
            text_weight: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (0.6)
            categorical_weight: ë²”ì£¼í˜• ë³€ìˆ˜ ê°€ì¤‘ì¹˜ (0.3) 
            numerical_weight: ìˆ«ìí˜• ë³€ìˆ˜ ê°€ì¤‘ì¹˜ (0.1)
        """
        self.text_weight = text_weight
        self.categorical_weight = categorical_weight
        self.numerical_weight = numerical_weight
        
        # KoSimCSE ëª¨ë¸ ì´ˆê¸°í™”
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
        print("ğŸ“¥ KoSimCSE ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = AutoModel.from_pretrained('BM-K/KoSimCSE-roberta')
        self.tokenizer = AutoTokenizer.from_pretrained('BM-K/KoSimCSE-roberta')
        self.model.to(self.device)
        self.model.eval()
        print("âœ… KoSimCSE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        # ìºì‹œ ë° ì¸ì½”ë” ì´ˆê¸°í™”
        self.embedding_cache = {}
        self.label_encoders = {}
        self.scaler = StandardScaler()
        self.is_fitted = False
        
        # ë°ì´í„° ë¶„í•  ì €ì¥
        self.train_df = None
        self.valid_df = None
        self.test_df = None
        
        # ìºì‹œ ë©”ì‹œì§€ ì œì–´
        self.cache_message_shown = False
        
        # ì „ì²˜ë¦¬ ì„¤ì •
        self.meaningless_patterns = [
            'ì²¨ë¶€í™•ì¸', 'í™•ì¸', 'ì²¨ë¶€', 'ì°¸ì¡°', 'ë³„ì²¨', 
            'ìƒì„¸ë‚´ìš©ì€ ì²¨ë¶€íŒŒì¼ ì°¸ì¡°', 'ì²¨ë¶€íŒŒì¼ í™•ì¸',
            'ì²¨ë¶€íŒŒì¼', 'íŒŒì¼ì°¸ì¡°', 'ë³„ë„ì²¨ë¶€', 'ì²¨ë¶€ì„œë¥˜'
        ]
    
    def print_cache_status(self):
        """ìºì‹œ ìƒíƒœ ì¶œë ¥"""
        print(f"ğŸ” ìºì‹œ ìƒíƒœ: {len(self.embedding_cache)}ê°œ ì„ë² ë”© ì €ì¥ë¨")
    
    def reset_cache_message(self):
        """ìºì‹œ ë©”ì‹œì§€ ìƒíƒœ ë¦¬ì…‹"""
        self.cache_message_shown = False
    
    def save_embeddings(self, file_path='embeddings_cache.pkl'):
        """ì„ë² ë”© ìºì‹œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        cache_data = {
            'embedding_cache': self.embedding_cache,
            'label_encoders': self.label_encoders,
            'scaler': self.scaler,
            'is_fitted': self.is_fitted
        }
        with open(file_path, 'wb') as f:
            pickle.dump(cache_data, f)
        print(f"ğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ: {file_path}")
    
    def load_embeddings(self, file_path='embeddings_cache.pkl'):
        """ì„ë² ë”© ìºì‹œë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ"""
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                cache_data = pickle.load(f)
            
            self.embedding_cache = cache_data['embedding_cache']
            self.label_encoders = cache_data['label_encoders']
            self.scaler = cache_data['scaler']
            self.is_fitted = cache_data['is_fitted']
            
            print(f"ğŸ“‚ ì„ë² ë”© ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.embedding_cache)}ê°œ")
            return True
        else:
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return False
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text)
        
        # ì˜ë¯¸ì—†ëŠ” íŒ¨í„´ ì œê±°
        for pattern in self.meaningless_patterns:
            text = text.replace(pattern, '')
        
        # ê³µë°± ì •ë¦¬
        text = ' '.join(text.split())
        
        # ìµœì†Œ ê¸¸ì´ í•„í„°ë§ (10ì ë¯¸ë§Œì€ ë¹ˆ ë¬¸ìì—´ë¡œ)
        return text if len(text) >= 10 else ''
    
    def clean_categorical(self, df, columns, min_freq=0.01):
        """ë²”ì£¼í˜• ë°ì´í„° ì •ì œ í•¨ìˆ˜"""
        print("ğŸ·ï¸ ë²”ì£¼í˜• ë°ì´í„° ì •ì œ ì¤‘...")
        
        for col in columns:
            if col not in df.columns:
                print(f"   âš ï¸ {col} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
                
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            df[col] = df[col].fillna('Unknown')
            
            # ë¹ˆë„ ê³„ì‚°
            value_counts = df[col].value_counts(normalize=True)
            rare_values = value_counts[value_counts < min_freq].index
            
            # í¬ê·€ ê°’ë“¤ì„ 'Others'ë¡œ ë³€ê²½
            if len(rare_values) > 0:
                df[col] = df[col].replace(rare_values, 'Others')
                print(f"   âœ… {col}: {len(rare_values)}ê°œ í¬ê·€ê°’ì„ 'Others'ë¡œ ë³€ê²½")
            
            print(f"   ğŸ“Š {col}: {df[col].nunique()}ê°œ ê³ ìœ ê°’")
        
        return df
    
    def handle_outliers(self, df, numerical_columns):
        """ìˆ«ìí˜• ë°ì´í„° ì´ìƒì¹˜ ì²˜ë¦¬ í•¨ìˆ˜"""
        print("ğŸ”¢ ìˆ«ìí˜• ë°ì´í„° ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        for col in numerical_columns:
            if col not in df.columns:
                print(f"   âš ï¸ {col} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # IQR ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # ì´ìƒì¹˜ ê°œìˆ˜ í™•ì¸
            outliers_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            
            # ì´ìƒì¹˜ë¥¼ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´
            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
            
            if outliers_count > 0:
                print(f"   âœ… {col}: {outliers_count}ê°œ ì´ìƒì¹˜ ì²˜ë¦¬")
        
        return df
        
    def load_and_split_data(self, file_path, text_column='ì‚¬ê³ ì„¤ëª…', 
                           categorical_columns=None, numerical_columns=None,
                           train_size=0.7, valid_size=0.15, test_size=0.15, random_state=42):
        """
        ë°ì´í„° ë¡œë“œ ë° Train/Validation/Test 3ë‹¨ê³„ ë¶„í• 
        
        Args:
            file_path: CSV íŒŒì¼ ê²½ë¡œ
            text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            categorical_columns: ë²”ì£¼í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            numerical_columns: ìˆ«ìí˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            train_size: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (0.7)
            valid_size: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (0.15)
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (0.15)
            random_state: ëœë¤ ì‹œë“œ
        """
        print("ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(file_path, encoding='cp949')
        print(f"ğŸ“ˆ ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ")
        
        # ì»¬ëŸ¼ íƒ€ì… ì„¤ì •
        if categorical_columns is None:
            categorical_columns = ['ì‚¬ê³ ìœ í˜•ëª…', 'ì‚¬ê³ ì§„í–‰ìƒíƒœ', 'ì‚¬ê³ ë“±ë¡ìƒíƒœëª…', 'ì ‘ìˆ˜ì‚¬ê³ ìœ í˜•í™•ì •', 
                                 'ì‚¬ê³ í†µí™”ì½”ë“œ_ZZ067', 'ì‹¬ì‚¬í•­ëª©ëª…', 'ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…','ìƒí’ˆë¶„ë¥˜ëª…', 
                                 'ìƒí’ˆë¶„ë¥˜ê·¸ë£¹ëª…', 'ìƒí’ˆëª…', 'ìƒì„¸ì‚¬ê³ ë³´í—˜ìƒí’ˆ', 'í–¥í›„ê²°ì œì „ë§', 
                                 'ìˆ˜ì¶œìëª…', 'ìˆ˜ì…ìëª…', 'ìˆ˜ì¶œìêµ­ê°€', 'ìˆ˜ì…ìêµ­ê°€']
        
        if numerical_columns is None:
            numerical_columns = ['ì™¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡',
                               'ì™¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ë¯¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ì›í™”í•©ê³„íŒì •ê¸ˆì•¡', 
                               'ì™¸í™”íŒì •ê¸ˆì•¡', 'ë¯¸í™”íŒì •ê¸ˆì•¡', 'ì›í™”íŒì •ê¸ˆì•¡',
                               'ì™¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ê¸ˆì•¡', 
                               'ì™¸í™”ë³´í—˜ê°€ì•¡', 'ë¯¸í™”ë³´í—˜ê°€ì•¡', 'ì›í™”ë³´í—˜ê°€ì•¡', 
                               'ì™¸í™”ë³´í—˜ê¸ˆì•¡', 'ë¯¸í™”ë³´í—˜ê¸ˆì•¡', 'ì›í™”ë³´í—˜ê¸ˆì•¡']
        
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {text_column}")
        print(f"ğŸ·ï¸ ë²”ì£¼í˜• ì»¬ëŸ¼: {len(categorical_columns)}ê°œ")
        print(f"ğŸ”¢ ìˆ«ìí˜• ì»¬ëŸ¼: {len(numerical_columns)}ê°œ")
        
        # 1. í…ìŠ¤íŠ¸ ë°ì´í„° ì •ì œ
        print(f"\nğŸ“ í…ìŠ¤íŠ¸ ë°ì´í„° ì •ì œ ì¤‘...")
        df[text_column] = df[text_column].apply(self.clean_text)
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
        before_count = len(df)
        df = df[df[text_column] != '']
        after_count = len(df)
        print(f"   âœ… ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°: {before_count - after_count}ê°œ ì œê±°ë¨")
        
        # 2. ë²”ì£¼í˜• ë°ì´í„° ì •ì œ
        df = self.clean_categorical(df, categorical_columns)
        
        # 3. ìˆ«ìí˜• ë°ì´í„° ì´ìƒì¹˜ ì²˜ë¦¬
        df = self.handle_outliers(df, numerical_columns)
        
        # 4. ì¤‘ë³µ ì œê±° (ì‚¬ê³ ì„¤ëª… ê¸°ì¤€)
        before_count = len(df)
        df = df.drop_duplicates(subset=[text_column])
        after_count = len(df)
        print(f"   âœ… ì‚¬ê³ ì„¤ëª… ê¸°ì¤€ ì¤‘ë³µ ì œê±°: {before_count - after_count}ê°œ ì œê±°ë¨")
        
        # ì¶”ê°€ ì¤‘ë³µ ì œê±° (ëª¨ë“  ì»¬ëŸ¼ ê¸°ì¤€)
        before_count = len(df)
        df = df.drop_duplicates()
        after_count = len(df)
        print(f"   âœ… ì „ì²´ ì»¬ëŸ¼ ê¸°ì¤€ ì¤‘ë³µ ì œê±°: {before_count - after_count}ê°œ ì œê±°ë¨")
        
        print(f"\nğŸ“Š ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ë°ì´í„°")
        
        # 5. 3ë‹¨ê³„ ë¶„í• 
        print(f"\nğŸ“Š ë°ì´í„° 3ë‹¨ê³„ ë¶„í•  ì¤‘...")
        
        # 1ë‹¨ê³„: Train + (Validation + Test) ë¶„í• 
        train_df, temp_df = train_test_split(
            df, test_size=(valid_size + test_size), random_state=random_state
        )
        
        # 2ë‹¨ê³„: Validation + Test ë¶„í• 
        valid_ratio = valid_size / (valid_size + test_size)
        valid_df, test_df = train_test_split(
            temp_df, test_size=(1 - valid_ratio), random_state=random_state
        )
        
        print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ ({len(train_df)/len(df)*100:.1f}%)")
        print(f"ğŸ” ê²€ì¦ ë°ì´í„°: {len(valid_df)}ê°œ ({len(valid_df)/len(df)*100:.1f}%)")
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ ({len(test_df)/len(df)*100:.1f}%)")
        
        # 6. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        categorical_features = []
        for col in categorical_columns:
            if col in df.columns:
                le = LabelEncoder()
                # ì „ì²´ ë°ì´í„°ë¡œ fit
                le.fit(df[col].astype(str))
                
                # ê° ë¶„í•  ë°ì´í„°ì— transform ì ìš©
                train_df[f'{col}_encoded'] = le.transform(train_df[col].astype(str))
                valid_df[f'{col}_encoded'] = le.transform(valid_df[col].astype(str))
                test_df[f'{col}_encoded'] = le.transform(test_df[col].astype(str))
                
                self.label_encoders[col] = le
                categorical_features.append(f'{col}_encoded')
                print(f"   âœ… {col} ì¸ì½”ë”© ì™„ë£Œ (ê³ ìœ ê°’: {len(le.classes_)}ê°œ)")
        
        # 7. ìˆ«ìí˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        numerical_features = []
        if numerical_columns:
            for col in numerical_columns:
                if col in df.columns:
                    numerical_features.append(col)
            
            if numerical_features:
                # ì „ì²´ ë°ì´í„°ë¡œ fit
                self.scaler.fit(df[numerical_features])
                
                # ê° ë¶„í•  ë°ì´í„°ì— transform ì ìš©
                train_df[numerical_features] = self.scaler.transform(train_df[numerical_features])
                valid_df[numerical_features] = self.scaler.transform(valid_df[numerical_features])
                test_df[numerical_features] = self.scaler.transform(test_df[numerical_features])
                
                print(f"   âœ… ìˆ«ìí˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: {len(numerical_features)}ê°œ")
        
        # ë°ì´í„° ì €ì¥
        self.train_df = train_df
        self.valid_df = valid_df
        self.test_df = test_df
        self.text_column = text_column
        self.categorical_features = categorical_features
        self.numerical_features = numerical_features
        self.is_fitted = True
        
        print("âœ… ë°ì´í„° 3ë‹¨ê³„ ë¶„í•  ë° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        return train_df, valid_df, test_df
    
    def load_data_only(self, file_path, text_column='ì‚¬ê³ ì„¤ëª…',
                      categorical_columns=None, numerical_columns=None,
                      train_size=0.7, valid_size=0.15, test_size=0.15, random_state=42):
        """ìºì‹œê°€ ìˆì„ ë•Œ ë°ì´í„°ë§Œ ë¡œë“œ (ì„ë² ë”© ìƒì„± ì•ˆí•¨)"""
        print("ğŸ“Š ë°ì´í„°ë§Œ ë¡œë“œ ì¤‘... (ì„ë² ë”©ì€ ìºì‹œ ì‚¬ìš©)")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(file_path, encoding='cp949')
        print(f"ğŸ“ˆ ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ")
        
        # ì»¬ëŸ¼ íƒ€ì… ì„¤ì •
        if categorical_columns is None:
            categorical_columns = ['ì‚¬ê³ ìœ í˜•ëª…', 'ì‚¬ê³ ì§„í–‰ìƒíƒœ', 'ì‚¬ê³ ë“±ë¡ìƒíƒœëª…', 'ì ‘ìˆ˜ì‚¬ê³ ìœ í˜•í™•ì •',
                                  'ì‚¬ê³ í†µí™”ì½”ë“œ_ZZ067', 'ì‹¬ì‚¬í•­ëª©ëª…', 'ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…','ìƒí’ˆë¶„ë¥˜ëª…',
                                  'ìƒí’ˆë¶„ë¥˜ê·¸ë£¹ëª…', 'ìƒí’ˆëª…', 'ìƒì„¸ì‚¬ê³ ë³´í—˜ìƒí’ˆ', 'í–¥í›„ê²°ì œì „ë§',
                                  'ìˆ˜ì¶œìëª…', 'ìˆ˜ì…ìëª…', 'ìˆ˜ì¶œìêµ­ê°€', 'ìˆ˜ì…ìêµ­ê°€']

        if numerical_columns is None:
            numerical_columns = ['ì™¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡',
                                'ì™¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ë¯¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ì›í™”í•©ê³„íŒì •ê¸ˆì•¡',
                                'ì™¸í™”íŒì •ê¸ˆì•¡', 'ë¯¸í™”íŒì •ê¸ˆì•¡', 'ì›í™”íŒì •ê¸ˆì•¡',
                                'ì™¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ê¸ˆì•¡',
                                'ì™¸í™”ë³´í—˜ê°€ì•¡', 'ë¯¸í™”ë³´í—˜ê°€ì•¡', 'ì›í™”ë³´í—˜ê°€ì•¡',
                                'ì™¸í™”ë³´í—˜ê¸ˆì•¡', 'ë¯¸í™”ë³´í—˜ê¸ˆì•¡', 'ì›í™”ë³´í—˜ê¸ˆì•¡']
        
        # ì „ì²˜ë¦¬ (ì„ë² ë”© ìƒì„± ì—†ì´)
        df[text_column] = df[text_column].apply(self.clean_text)
        df = df[df[text_column] != '']
        df = self.clean_categorical(df, categorical_columns)
        df = self.handle_outliers(df, numerical_columns)
        df = df.drop_duplicates(subset=[text_column])
        df = df.drop_duplicates()
        
        print(f"ğŸ“Š ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ë°ì´í„°")
        
        # 3ë‹¨ê³„ ë¶„í• 
        train_df, temp_df = train_test_split(
            df, test_size=(valid_size + test_size), random_state=random_state
        )
        valid_ratio = valid_size / (valid_size + test_size)
        valid_df, test_df = train_test_split(
            temp_df, test_size=(1 - valid_ratio), random_state=random_state
        )
        
        print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ ({len(train_df)/len(df)*100:.1f}%)")
        print(f"ğŸ” ê²€ì¦ ë°ì´í„°: {len(valid_df)}ê°œ ({len(valid_df)/len(df)*100:.1f}%)")
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ ({len(test_df)/len(df)*100:.1f}%)")
        
        # ì¸ì½”ë”© ë° ìŠ¤ì¼€ì¼ë§ (ê¸°ì¡´ ì¸ì½”ë” ì‚¬ìš©)
        categorical_features = []
        for col in categorical_columns:
            if col in df.columns and col in self.label_encoders:
                train_df[f'{col}_encoded'] = self.label_encoders[col].transform(train_df[col].astype(str))
                valid_df[f'{col}_encoded'] = self.label_encoders[col].transform(valid_df[col].astype(str))
                test_df[f'{col}_encoded'] = self.label_encoders[col].transform(test_df[col].astype(str))
                categorical_features.append(f'{col}_encoded')
        
        numerical_features = []
        if numerical_columns:
            for col in numerical_columns:
                if col in df.columns:
                    numerical_features.append(col)
            
            if numerical_features:
                train_df[numerical_features] = self.scaler.transform(train_df[numerical_features])
                valid_df[numerical_features] = self.scaler.transform(valid_df[numerical_features])
                test_df[numerical_features] = self.scaler.transform(test_df[numerical_features])
        
        # ë°ì´í„° ì €ì¥
        self.train_df = train_df
        self.valid_df = valid_df
        self.test_df = test_df
        self.text_column = text_column
        self.categorical_features = categorical_features
        self.numerical_features = numerical_features
        
        print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (ì„ë² ë”©ì€ ìºì‹œ ì‚¬ìš©)")
        return train_df, valid_df, test_df
    
    def encode_text(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        inputs = self.tokenizer(
            [text], padding=True, truncation=True, max_length=128, return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            embeddings, _ = self.model(**inputs, return_dict=False)
            embedding = embeddings[0][0].cpu().numpy()
            self.embedding_cache[text] = embedding
            return embedding
    
    def encode_batch(self, texts, batch_size=16):
        """ë°°ì¹˜ë¡œ í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = []
        uncached_texts = []
        
        for text in texts:
            if text not in self.embedding_cache:
                uncached_texts.append(text)
            else:
                embeddings.append(self.embedding_cache[text])
        
        if uncached_texts:
            print(f"    ğŸ“¥ {len(uncached_texts)}ê°œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            total_batches = (len(uncached_texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(uncached_texts), batch_size):
                batch_texts = uncached_texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts, padding=True, truncation=True, max_length=128, return_tensors="pt"
                ).to(self.device)
                
                with torch.no_grad():
                    batch_embeddings, _ = self.model(**inputs, return_dict=False)
                    batch_embeddings = batch_embeddings[0].cpu().numpy()
                    
                    for text, embedding in zip(batch_texts, batch_embeddings):
                        self.embedding_cache[text] = embedding
                        embeddings.append(embedding)
                
                current_batch = i // batch_size + 1
                if current_batch % max(1, total_batches // 10) == 0 or current_batch == total_batches:
                    progress = min((i + batch_size) / len(uncached_texts) * 100, 100)
                    print(f"    ì„ë² ë”© ì§„í–‰ë¥ : {progress:.1f}% ({min(i + batch_size, len(uncached_texts))}/{len(uncached_texts)})")
        else:
            # ìºì‹œëœ í…ìŠ¤íŠ¸ë§Œ ìˆì„ ë•ŒëŠ” í•œ ë²ˆë§Œ ë©”ì‹œì§€ ì¶œë ¥
            if not self.cache_message_shown:
                print(f"    âœ… ìºì‹œëœ ì„ë² ë”© ì‚¬ìš© ({len(texts)}ê°œ)")
                self.cache_message_shown = True
        
        return np.array(embeddings)
    
    def calculate_text_similarity(self, query_text, target_texts):
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        query_embedding = self.encode_text(query_text)
        target_embeddings = self.encode_batch(target_texts)
        return cosine_similarity([query_embedding], target_embeddings)[0]
    
    def calculate_categorical_similarity(self, query_categorical, target_categorical):
        """ë²”ì£¼í˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)"""
        similarities = []
        for target in target_categorical:
            # ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ì •í™•íˆ ì¼ì¹˜í•˜ë©´ 1, ì•„ë‹ˆë©´ 0
            similarity = 1.0 if query_categorical == target else 0.0
            similarities.append(similarity)
        return np.array(similarities)
    
    def calculate_numerical_similarity(self, query_numerical, target_numerical):
        """ìˆ«ìí˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚° (ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜)"""
        if len(query_numerical) == 0:
            return np.ones(len(target_numerical))
        
        similarities = []
        for target in target_numerical:
            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 / (1 + ê±°ë¦¬))
            distance = np.sqrt(np.sum((query_numerical - target) ** 2))
            similarity = 1 / (1 + distance)
            similarities.append(similarity)
        return np.array(similarities)
    
    def find_similar_cases(self, query_data, search_df, top_k=5, verbose=True):
        """
        ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
        
        Args:
            query_data: ì¿¼ë¦¬ ë°ì´í„° (dict í˜•íƒœ)
                - text: í…ìŠ¤íŠ¸
                - categorical: ë²”ì£¼í˜• ë³€ìˆ˜ dict
                - numerical: ìˆ«ìí˜• ë³€ìˆ˜ dict
            search_df: ê²€ìƒ‰í•  ë°ì´í„°í”„ë ˆì„
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        """
        if not self.is_fitted:
            raise ValueError("ë¨¼ì € load_and_split_data()ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”!")
        
        if verbose:
            print(f"\nğŸ” ë‹¤ì¤‘ ëª¨ë‹¬ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (ìƒìœ„ {top_k}ê°œ)")
            print(f"ğŸ“ ì¿¼ë¦¬ í…ìŠ¤íŠ¸: {query_data['text'][:100]}...")
        
        # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        text_similarities = self.calculate_text_similarity(
            query_data['text'], 
            search_df[self.text_column].tolist()
        )
        
        # 2. ë²”ì£¼í˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚°
        categorical_similarities = np.ones(len(search_df))
        if query_data.get('categorical') and self.categorical_features:
            for col, value in query_data['categorical'].items():
                if col in self.label_encoders:
                    encoded_value = self.label_encoders[col].transform([str(value)])[0]
                    col_similarities = self.calculate_categorical_similarity(
                        encoded_value,
                        search_df[f'{col}_encoded'].values
                    )
                    categorical_similarities *= col_similarities
        
        # 3. ìˆ«ìí˜• ë³€ìˆ˜ ìœ ì‚¬ë„ ê³„ì‚°
        numerical_similarities = np.ones(len(search_df))
        if query_data.get('numerical') and self.numerical_features:
            query_numerical = []
            for col in self.numerical_features:
                if col in query_data['numerical']:
                    query_numerical.append(query_data['numerical'][col])
                else:
                    query_numerical.append(0)  # ê¸°ë³¸ê°’
            
            query_numerical = np.array(query_numerical)
            target_numerical = search_df[self.numerical_features].values
            numerical_similarities = self.calculate_numerical_similarity(query_numerical, target_numerical)
        
        # 4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
        final_similarities = (
            self.text_weight * text_similarities +
            self.categorical_weight * categorical_similarities +
            self.numerical_weight * numerical_similarities
        )
        
        # 5. ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
        top_indices = np.argsort(final_similarities)[::-1][:top_k]
        
        results = []
        for i, idx in enumerate(top_indices):
            similarity = final_similarities[idx]
            row = search_df.iloc[idx]
            
            result = {
                'rank': i + 1,
                'similarity': similarity,
                'text_similarity': text_similarities[idx],
                'categorical_similarity': categorical_similarities[idx],
                'numerical_similarity': numerical_similarities[idx],
                'data': row.to_dict()
            }
            results.append(result)
            
            if verbose:
                print(f"\n{i+1}. ì „ì²´ ìœ ì‚¬ë„: {similarity:.4f}")
                print(f"   ğŸ“ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_similarities[idx]:.4f}")
                print(f"   ğŸ·ï¸ ë²”ì£¼í˜• ìœ ì‚¬ë„: {categorical_similarities[idx]:.4f}")
                print(f"   ğŸ”¢ ìˆ«ìí˜• ìœ ì‚¬ë„: {numerical_similarities[idx]:.4f}")
                print(f"   ğŸ“‹ ì‚¬ê³ ìœ í˜•: {row.get('ì‚¬ê³ ìœ í˜•ëª…', 'N/A')}")
                print(f"   ğŸ“„ ì‚¬ê³ ì„¤ëª…: {row.get(self.text_column, 'N/A')}")
        
        return results
    
    def evaluate_on_validation(self, top_k=5):
        """ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€"""
        print(f"\nğŸ“Š ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ (ìƒìœ„ {top_k}ê°œ ê¸°ì¤€)")
        
        correct_count = 0
        total_similarities = []
        
        for i, valid_row in self.valid_df.iterrows():
            # ê²€ì¦ ì¿¼ë¦¬ ìƒì„±
            query_data = {
                'text': valid_row[self.text_column],
                'categorical': {},
                'numerical': {}
            }
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.label_encoders.keys():
                if col in valid_row:
                    query_data['categorical'][col] = valid_row[col]
            
            # ìˆ«ìí˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.numerical_features:
                if col in valid_row:
                    query_data['numerical'][col] = valid_row[col]
            
            # í•™ìŠµ ë°ì´í„°ì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
            results = self.find_similar_cases(query_data, self.train_df, top_k=top_k, verbose=False)
            
            # ì •í™•ë„ ê³„ì‚°
            valid_accident_type = valid_row.get('ì‚¬ê³ ìœ í˜•ëª…', '')
            for result in results:
                if result['data'].get('ì‚¬ê³ ìœ í˜•ëª…') == valid_accident_type:
                    correct_count += 1
                    break
            
            # í‰ê·  ìœ ì‚¬ë„
            total_similarities.extend([r['similarity'] for r in results])
        
        accuracy = correct_count / len(self.valid_df) * 100
        avg_similarity = np.mean(total_similarities)
        
        print(f"âœ… ê²€ì¦ ì •í™•ë„: {accuracy:.2f}%")
        print(f"ğŸ“ˆ ê²€ì¦ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
        
        return accuracy, avg_similarity
    
    def evaluate_on_test(self, top_k=5):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€"""
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ (ìƒìœ„ {top_k}ê°œ ê¸°ì¤€)")
        
        correct_count = 0
        total_similarities = []
        
        for i, test_row in self.test_df.iterrows():
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±
            query_data = {
                'text': test_row[self.text_column],
                'categorical': {},
                'numerical': {}
            }
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.label_encoders.keys():
                if col in test_row:
                    query_data['categorical'][col] = test_row[col]
            
            # ìˆ«ìí˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.numerical_features:
                if col in test_row:
                    query_data['numerical'][col] = test_row[col]
            
            # í•™ìŠµ ë°ì´í„°ì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
            results = self.find_similar_cases(query_data, self.train_df, top_k=top_k, verbose=False)
            
            # ì •í™•ë„ ê³„ì‚°
            test_accident_type = test_row.get('ì‚¬ê³ ìœ í˜•ëª…', '')
            for result in results:
                if result['data'].get('ì‚¬ê³ ìœ í˜•ëª…') == test_accident_type:
                    correct_count += 1
                    break
            
            # í‰ê·  ìœ ì‚¬ë„
            total_similarities.extend([r['similarity'] for r in results])
        
        accuracy = correct_count / len(self.test_df) * 100
        avg_similarity = np.mean(total_similarities)
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.2f}%")
        print(f"ğŸ“ˆ í…ŒìŠ¤íŠ¸ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
        
        return accuracy, avg_similarity
    
    def test_with_real_queries(self, top_k=5):
        """ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ ì„ íƒ (ì‚¬ê³ ìœ í˜•ë³„ 1ê°œì”©)
        test_samples = []
        for accident_type in self.test_df['ì‚¬ê³ ìœ í˜•ëª…'].unique():
            type_samples = self.test_df[self.test_df['ì‚¬ê³ ìœ í˜•ëª…'] == accident_type].head(1)
            test_samples.extend(type_samples['ì‚¬ê³ ì„¤ëª…'].tolist())
        
        test_queries = test_samples[:5]  # ìµœëŒ€ 5ê°œ ìƒ˜í”Œ
        
        for i, query_text in enumerate(test_queries, 1):
            print(f"\n--- ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i} ---")
            
            # í•´ë‹¹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì°¾ê¸°
            test_row = self.test_df[self.test_df[self.text_column] == query_text].iloc[0]
            
            # ì¿¼ë¦¬ ë°ì´í„° ìƒì„±
            query_data = {
                'text': query_text,
                'categorical': {},
                'numerical': {}
            }
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.label_encoders.keys():
                if col in test_row:
                    query_data['categorical'][col] = test_row[col]
            
            # ìˆ«ìí˜• ë³€ìˆ˜ ì¶”ê°€
            for col in self.numerical_features:
                if col in test_row:
                    query_data['numerical'][col] = test_row[col]
            
            print(f"ğŸ” ì¿¼ë¦¬: {query_text}")
            print(f"ğŸ“‹ ì‹¤ì œ ì‚¬ê³ ìœ í˜•: {test_row.get('ì‚¬ê³ ìœ í˜•ëª…', 'N/A')}")
            
            # í•™ìŠµ ë°ì´í„°ì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
            results = self.find_similar_cases(query_data, self.train_df, top_k=top_k)
            
            # ì •í™•ë„ í™•ì¸
            correct_found = False
            for result in results:
                if result['data'].get('ì‚¬ê³ ìœ í˜•ëª…') == test_row.get('ì‚¬ê³ ìœ í˜•ëª…'):
                    correct_found = True
                    break
            
            if correct_found:
                print(f"âœ… ì •í™•í•œ ì‚¬ê³ ìœ í˜• ë°œê²¬!")
            else:
                print(f"âŒ ì •í™•í•œ ì‚¬ê³ ìœ í˜• ë¯¸ë°œê²¬")

    def test_weight_combinations(self, top_k=3):
        """ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ì‹¤í—˜ 2: ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸ (ìƒìœ„ {top_k}ê°œ ê¸°ì¤€)")
        
        weight_combinations = [
            {
                'name': 'í…ìŠ¤íŠ¸ ì¤‘ì‹¬ (80%)',
                'weights': {'text': 0.8, 'categorical': 0.15, 'numerical': 0.05}
            },
            {
                'name': 'ë²”ì£¼í˜• ì¤‘ì‹¬ (50%)',
                'weights': {'text': 0.4, 'categorical': 0.5, 'numerical': 0.1}
            },
            {
                'name': 'ê· ë“± ë¶„ë°° (33%)',
                'weights': {'text': 0.33, 'categorical': 0.33, 'numerical': 0.34}
            },
            {
                'name': 'í…ìŠ¤íŠ¸+ë²”ì£¼í˜• (50%+40%)',
                'weights': {'text': 0.5, 'categorical': 0.4, 'numerical': 0.1}
            },
            {
                'name': 'í…ìŠ¤íŠ¸ë§Œ (100%)',
                'weights': {'text': 1.0, 'categorical': 0.0, 'numerical': 0.0}
            }
        ]
        
        results = []
        
        for combo in weight_combinations:
            print(f"\nğŸ“Š {combo['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ê°€ì¤‘ì¹˜ ì„ì‹œ ë³€ê²½
            original_weights = {
                'text': self.text_weight,
                'categorical': self.categorical_weight,
                'numerical': self.numerical_weight
            }
            
            self.text_weight = combo['weights']['text']
            self.categorical_weight = combo['weights']['categorical']
            self.numerical_weight = combo['weights']['numerical']
            
            # ì„±ëŠ¥ í‰ê°€
            accuracy, avg_similarity = self.evaluate_on_test(top_k=top_k)
            
            # ê²°ê³¼ ì €ì¥
            results.append({
                'name': combo['name'],
                'weights': combo['weights'],
                'accuracy': accuracy,
                'avg_similarity': avg_similarity
            })
            
            # ê°€ì¤‘ì¹˜ ë³µì›
            self.text_weight = original_weights['text']
            self.categorical_weight = original_weights['categorical']
            self.numerical_weight = original_weights['numerical']
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ¯ ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"{'ì¡°í•©':<25} {'ì •í™•ë„':<10} {'í‰ê· ìœ ì‚¬ë„':<12}")
        print("-" * 50)
        for result in results:
            print(f"{result['name']:<25} {result['accuracy']:<10.2f}% {result['avg_similarity']:<12.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ì¡°í•© ì°¾ê¸°
        best_result = max(results, key=lambda x: x['accuracy'])
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_result['name']} ({best_result['accuracy']:.2f}%)")
        
        return results

    def test_evaluation_criteria(self):
        """ë‹¤ì–‘í•œ í‰ê°€ ê¸°ì¤€ìœ¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ì‹¤í—˜ 3: í‰ê°€ ê¸°ì¤€ í…ŒìŠ¤íŠ¸")
        
        evaluation_criteria = [
            {
                'name': 'ìƒìœ„ 1ê°œë§Œ í‰ê°€',
                'top_k': 1,
                'success_condition': lambda results, target: results[0]['data'].get('ì‚¬ê³ ìœ í˜•ëª…') == target
            },
            {
                'name': 'ìƒìœ„ 3ê°œ ì¤‘ 1ê°œ ì´ìƒ',
                'top_k': 3,
                'success_condition': lambda results, target: any(r['data'].get('ì‚¬ê³ ìœ í˜•ëª…') == target for r in results)
            },
            {
                'name': 'ìƒìœ„ 3ê°œ ì¤‘ ê³¼ë°˜ìˆ˜ (2ê°œ ì´ìƒ)',
                'top_k': 3,
                'success_condition': lambda results, target: sum(1 for r in results if r['data'].get('ì‚¬ê³ ìœ í˜•ëª…') == target) >= 2
            },
            {
                'name': 'ìƒìœ„ 5ê°œ ì¤‘ 1ê°œ ì´ìƒ (ê¸°ì¡´)',
                'top_k': 5,
                'success_condition': lambda results, target: any(r['data'].get('ì‚¬ê³ ìœ í˜•ëª…') == target for r in results)
            }
        ]
        
        results = []
        
        for criteria in evaluation_criteria:
            print(f"\nğŸ“Š {criteria['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            correct_count = 0
            total_similarities = []
            
            for i, test_row in self.test_df.iterrows():
                # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±
                query_data = {
                    'text': test_row[self.text_column],
                    'categorical': {},
                    'numerical': {}
                }
                
                # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
                for col in self.label_encoders.keys():
                    if col in test_row:
                        query_data['categorical'][col] = test_row[col]
                
                # ìˆ«ìí˜• ë³€ìˆ˜ ì¶”ê°€
                for col in self.numerical_features:
                    if col in test_row:
                        query_data['numerical'][col] = test_row[col]
                
                # í•™ìŠµ ë°ì´í„°ì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
                search_results = self.find_similar_cases(query_data, self.train_df, top_k=criteria['top_k'], verbose=False)
                
                # ì •í™•ë„ ê³„ì‚°
                test_accident_type = test_row.get('ì‚¬ê³ ìœ í˜•ëª…', '')
                if criteria['success_condition'](search_results, test_accident_type):
                    correct_count += 1
                
                # í‰ê·  ìœ ì‚¬ë„
                total_similarities.extend([r['similarity'] for r in search_results])
            
            accuracy = correct_count / len(self.test_df) * 100
            avg_similarity = np.mean(total_similarities)
            
            results.append({
                'name': criteria['name'],
                'top_k': criteria['top_k'],
                'accuracy': accuracy,
                'avg_similarity': avg_similarity
            })
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ¯ í‰ê°€ ê¸°ì¤€ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"{'ê¸°ì¤€':<25} {'ìƒìœ„K':<8} {'ì •í™•ë„':<10} {'í‰ê· ìœ ì‚¬ë„':<12}")
        print("-" * 60)
        for result in results:
            print(f"{result['name']:<25} {result['top_k']:<8} {result['accuracy']:<10.2f}% {result['avg_similarity']:<12.4f}")
        
        return results

def main():
    # ë‹¤ì¤‘ ëª¨ë‹¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ëª¨ë¸ ì´ˆê¸°í™”
    multimodal_search = MultimodalSimilaritySearch(
        text_weight=0.6,      # í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜
        categorical_weight=0.3, # ë²”ì£¼í˜• ê°€ì¤‘ì¹˜
        numerical_weight=0.1   # ìˆ«ìí˜• ê°€ì¤‘ì¹˜
    )
    
    # ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹œë„
    cache_file = 'multimodal_embeddings_cache.pkl'
    if multimodal_search.load_embeddings(cache_file):
        print(f"âœ… ìºì‹œ íŒŒì¼ ë¡œë“œ ì„±ê³µ! ì„ë² ë”© ì¬ì‚¬ìš© ê°€ëŠ¥")
        # ë°ì´í„°ë§Œ ë¡œë“œ (ì„ë² ë”©ì€ ì´ë¯¸ ìºì‹œë¨)
        train_df, valid_df, test_df = multimodal_search.load_data_only(
            file_path='data/case.csv',
            text_column='ì‚¬ê³ ì„¤ëª…',
            # ë²”ì£¼í˜• ë³€ìˆ˜ (16ê°œ)
            categorical_columns=['ì‚¬ê³ ìœ í˜•ëª…', 'ì‚¬ê³ ì§„í–‰ìƒíƒœ', 'ì‚¬ê³ ë“±ë¡ìƒíƒœëª…', 'ì ‘ìˆ˜ì‚¬ê³ ìœ í˜•í™•ì •',
                                'ì‚¬ê³ í†µí™”ì½”ë“œ_ZZ067', 'ì‹¬ì‚¬í•­ëª©ëª…', 'ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…','ìƒí’ˆë¶„ë¥˜ëª…',
                                'ìƒí’ˆë¶„ë¥˜ê·¸ë£¹ëª…', 'ìƒí’ˆëª…', 'ìƒì„¸ì‚¬ê³ ë³´í—˜ìƒí’ˆ', 'í–¥í›„ê²°ì œì „ë§',
                                'ìˆ˜ì¶œìëª…', 'ìˆ˜ì…ìëª…', 'ìˆ˜ì¶œìêµ­ê°€', 'ìˆ˜ì…ìêµ­ê°€'],
            # ìˆ«ìí˜• ë³€ìˆ˜ (18ê°œ)
            numerical_columns=['ì™¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡',
                              'ì™¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ë¯¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ì›í™”í•©ê³„íŒì •ê¸ˆì•¡',
                              'ì™¸í™”íŒì •ê¸ˆì•¡', 'ë¯¸í™”íŒì •ê¸ˆì•¡', 'ì›í™”íŒì •ê¸ˆì•¡',
                              'ì™¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ê¸ˆì•¡',
                              'ì™¸í™”ë³´í—˜ê°€ì•¡', 'ë¯¸í™”ë³´í—˜ê°€ì•¡', 'ì›í™”ë³´í—˜ê°€ì•¡',
                              'ì™¸í™”ë³´í—˜ê¸ˆì•¡', 'ë¯¸í™”ë³´í—˜ê¸ˆì•¡', 'ì›í™”ë³´í—˜ê¸ˆì•¡']
        )
    else:
        print(f"âš ï¸ ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹¤í–‰í•©ë‹ˆë‹¤.")
        # ë°ì´í„° ë¡œë“œ ë° 3ë‹¨ê³„ ë¶„í•  (ì „ì²˜ë¦¬ í¬í•¨)
        train_df, valid_df, test_df = multimodal_search.load_and_split_data(
            file_path='data/case.csv',
        text_column='ì‚¬ê³ ì„¤ëª…',
            # ë²”ì£¼í˜• ë³€ìˆ˜ (16ê°œ)
            categorical_columns=['ì‚¬ê³ ìœ í˜•ëª…', 'ì‚¬ê³ ì§„í–‰ìƒíƒœ', 'ì‚¬ê³ ë“±ë¡ìƒíƒœëª…', 'ì ‘ìˆ˜ì‚¬ê³ ìœ í˜•í™•ì •',
                                'ì‚¬ê³ í†µí™”ì½”ë“œ_ZZ067', 'ì‹¬ì‚¬í•­ëª©ëª…', 'ìƒí’ˆì‹¬ì‚¬í•­ëª©ëª…','ìƒí’ˆë¶„ë¥˜ëª…',
                                'ìƒí’ˆë¶„ë¥˜ê·¸ë£¹ëª…', 'ìƒí’ˆëª…', 'ìƒì„¸ì‚¬ê³ ë³´í—˜ìƒí’ˆ', 'í–¥í›„ê²°ì œì „ë§',
                                'ìˆ˜ì¶œìëª…', 'ìˆ˜ì…ìëª…', 'ìˆ˜ì¶œìêµ­ê°€', 'ìˆ˜ì…ìêµ­ê°€'],
            # ìˆ«ìí˜• ë³€ìˆ˜ (18ê°œ)
            numerical_columns=['ì™¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ì ‘ìˆ˜ê¸ˆì•¡',
                              'ì™¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ë¯¸í™”í•©ê³„íŒì •ê¸ˆì•¡', 'ì›í™”í•©ê³„íŒì •ê¸ˆì•¡',
                              'ì™¸í™”íŒì •ê¸ˆì•¡', 'ë¯¸í™”íŒì •ê¸ˆì•¡', 'ì›í™”íŒì •ê¸ˆì•¡',
                              'ì™¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ë¯¸í™”ì‚¬ê³ ê¸ˆì•¡', 'ì›í™”ì‚¬ê³ ê¸ˆì•¡',
                              'ì™¸í™”ë³´í—˜ê°€ì•¡', 'ë¯¸í™”ë³´í—˜ê°€ì•¡', 'ì›í™”ë³´í—˜ê°€ì•¡',
                              'ì™¸í™”ë³´í—˜ê¸ˆì•¡', 'ë¯¸í™”ë³´í—˜ê¸ˆì•¡', 'ì›í™”ë³´í—˜ê¸ˆì•¡']
        )
    
    # 1ë‹¨ê³„: ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
    print(f"\nğŸ“Š 1ë‹¨ê³„: ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€")
    multimodal_search.reset_cache_message()  # ìºì‹œ ë©”ì‹œì§€ ë¦¬ì…‹
    valid_accuracy, valid_similarity = multimodal_search.evaluate_on_validation()
    
    # 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
    print(f"\nğŸ§ª 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€")
    multimodal_search.reset_cache_message()  # ìºì‹œ ë©”ì‹œì§€ ë¦¬ì…‹
    test_accuracy, test_similarity = multimodal_search.evaluate_on_test()
    
    # 3ë‹¨ê³„: ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” 3ë‹¨ê³„: ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
    multimodal_search.reset_cache_message()  # ìºì‹œ ë©”ì‹œì§€ ë¦¬ì…‹
    multimodal_search.test_with_real_queries()
    
    # 4ë‹¨ê³„: ìºì‹œ ìƒíƒœ ì¶œë ¥
    multimodal_search.print_cache_status()
    
    # 5ë‹¨ê³„: ìºì‹œ ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‚¬ìš©)
    multimodal_search.save_embeddings('multimodal_embeddings_cache.pkl')
    
    # 6ë‹¨ê³„: ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª 6ë‹¨ê³„: ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸")
    multimodal_search.test_weight_combinations()
    
    # 7ë‹¨ê³„: í‰ê°€ ê¸°ì¤€ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª 7ë‹¨ê³„: í‰ê°€ ê¸°ì¤€ í…ŒìŠ¤íŠ¸")
    multimodal_search.test_evaluation_criteria()
    
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"   ê²€ì¦ ì •í™•ë„: {valid_accuracy:.2f}%")
    print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.2f}%")
    print(f"   ê²€ì¦ í‰ê·  ìœ ì‚¬ë„: {valid_similarity:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ í‰ê·  ìœ ì‚¬ë„: {test_similarity:.4f}")
    print(f"   ğŸ“Š ë°ì´í„° í’ˆì§ˆ:")
    print(f"      - ê³ ìœ  ì‚¬ê³ ì„¤ëª…: {len(train_df[multimodal_search.text_column].unique())}ê°œ")
    print(f"      - ì¤‘ë³µ ì œê±° íš¨ê³¼: {len(train_df[multimodal_search.text_column].unique()) / len(train_df) * 100:.1f}% ê³ ìœ ë„")
    print(f"   ğŸ” ìºì‹œ íš¨ìœ¨ì„±:")
    print(f"      - ìºì‹œëœ ì„ë² ë”©: {len(multimodal_search.embedding_cache)}ê°œ")
    print(f"      - ìºì‹œ íŒŒì¼ ì €ì¥: multimodal_embeddings_cache.pkl")

if __name__ == "__main__":
    main() 