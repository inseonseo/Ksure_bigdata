#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
ì „ì²´ ë°ì´í„°ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ì—¬ KoSimCSE í…ŒìŠ¤íŠ¸
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder, StandardScaler
import torch
from transformers import AutoTokenizer, AutoModel
import time

class SampleCaseSearch:
    def __init__(self, data_path='data/testy.csv', sample_size=1000):
        """
        ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        
        Args:
            data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            sample_size: ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ 1000ê°œ)
        """
        self.data_path = data_path
        self.sample_size = sample_size
        self.data = None
        self.case_summary = None
        self.kosimcse_model = None
        self.kosimcse_tokenizer = None
        self.text_embeddings = None
        
        print(f"ğŸš€ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë°ì´í„° {sample_size}ê°œ)")
        self.load_and_preprocess_data()
    
    def load_kosimcse_model(self):
        """KoSimCSE ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ¤– KoSimCSE ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            model_path = 'BM-K/KoSimCSE-roberta'
            self.kosimcse_tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.kosimcse_model = AutoModel.from_pretrained(model_path)
            print("âœ… KoSimCSE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ KoSimCSE ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            self.kosimcse_model = None
            self.kosimcse_tokenizer = None
    
    def load_and_preprocess_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì „ì²´ ë°ì´í„° ë¡œë“œ
        full_data = pd.read_csv(self.data_path, encoding='cp949')
        print(f"   ì „ì²´ ë°ì´í„°: {len(full_data)}ê°œ")
        
        # ìƒ˜í”Œë§ (ëœë¤)
        self.data = full_data.sample(n=min(self.sample_size, len(full_data)), random_state=42)
        print(f"   ìƒ˜í”Œ ë°ì´í„°: {len(self.data)}ê°œ")
        
        # KoSimCSE ëª¨ë¸ ë¡œë“œ
        self.load_kosimcse_model()
        
        # ì‚¬ê±´ë³„ ìš”ì•½ ìƒì„±
        self.create_case_summary()
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        self.create_text_embeddings()
        
        print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(self.case_summary)}ê°œ ì‚¬ê±´")
    
    def create_case_summary(self):
        """ì‚¬ê±´ë³„ ìš”ì•½ ì •ë³´ ìƒì„±"""
        print("ğŸ”§ ì‚¬ê±´ë³„ ìš”ì•½ ì •ë³´ ìƒì„± ì¤‘...")
        
        if 'ë³´ìƒíŒŒì¼ë²ˆí˜¸' in self.data.columns and 'ì‚¬ê³ ë²ˆí˜¸' in self.data.columns:
            # ì‚¬ê±´ë³„ ê·¸ë£¹í•‘
            case_groups = self.data.groupby(['ë³´ìƒíŒŒì¼ë²ˆí˜¸', 'ì‚¬ê³ ë²ˆí˜¸']).agg({
                'ì‚¬ê³ ì ‘ìˆ˜ì¼ì': 'first' if 'ì‚¬ê³ ì ‘ìˆ˜ì¼ì' in self.data.columns else None,
                'ì‚¬ê³ ê¸ˆì•¡': 'first' if 'ì‚¬ê³ ê¸ˆì•¡' in self.data.columns else None,
                'ì‚¬ê³ ìœ í˜•ëª…': 'first' if 'ì‚¬ê³ ìœ í˜•ëª…' in self.data.columns else None,
                'ìˆ˜ì…êµ­': 'first' if 'ìˆ˜ì…êµ­' in self.data.columns else None,
                'ì‚¬ê³ ì„¤ëª…': 'first' if 'ì‚¬ê³ ì„¤ëª…' in self.data.columns else None,
                'ìˆ˜ì¶œì': 'first' if 'ìˆ˜ì¶œì' in self.data.columns else None,
                'ë³´í—˜ì¢…ëª©': 'first' if 'ë³´í—˜ì¢…ëª©' in self.data.columns else None,
                'íŒì •êµ¬ë¶„': list if 'íŒì •êµ¬ë¶„' in self.data.columns else None,
                'íŒì •ì‚¬ìœ ': list if 'íŒì •ì‚¬ìœ ' in self.data.columns else None,
            }).reset_index()
            
            case_groups = case_groups.dropna(axis=1, how='all')
        else:
            case_groups = self.data.copy()
            case_groups['ë³´ìƒíŒŒì¼ë²ˆí˜¸'] = 'default'
            case_groups['ì‚¬ê³ ë²ˆí˜¸'] = range(len(case_groups))
        
        self.case_summary = case_groups
        print(f"âœ… ì‚¬ê±´ë³„ ìš”ì•½ ì™„ë£Œ: {len(self.case_summary)}ê°œ ì‚¬ê±´")
    
    def create_text_embeddings(self):
        """KoSimCSE í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if self.kosimcse_model is None:
            print("âš ï¸ KoSimCSE ëª¨ë¸ì´ ì—†ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        print("ğŸ¤– KoSimCSE ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ê²°í•©
        text_columns = []
        for col in ['ì‚¬ê³ ì„¤ëª…', 'ìˆ˜ì¶œì']:
            if col in self.case_summary.columns:
                text_columns.append(col)
        
        if text_columns:
            self.case_summary['combined_text'] = self.case_summary[text_columns].fillna('').agg(' '.join, axis=1)
        else:
            self.case_summary['combined_text'] = ''
        
        # ì„ë² ë”© ìƒì„±
        texts = self.case_summary['combined_text'].fillna('').tolist()
        self.text_embeddings = self.encode_text_with_kosimcse(texts)
        
        if self.text_embeddings is not None:
            print(f"âœ… KoSimCSE ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.text_embeddings.shape}")
        else:
            print("âš ï¸ KoSimCSE ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
    
    def encode_text_with_kosimcse(self, texts):
        """KoSimCSEë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if self.kosimcse_model is None or self.kosimcse_tokenizer is None:
            return None
        
        try:
            batch_size = 16  # ìƒ˜í”Œìš©ìœ¼ë¡œ ì‘ì€ ë°°ì¹˜
            embeddings = []
            
            print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if i % (batch_size * 5) == 0:
                    progress = (i / len(texts)) * 100
                    print(f"   ì§„í–‰ë¥ : {progress:.1f}% ({i}/{len(texts)})")
                
                # í† í¬ë‚˜ì´ì§•
                inputs = self.kosimcse_tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=128,  # ìƒ˜í”Œìš©ìœ¼ë¡œ ë” ì§§ê²Œ
                    return_tensors="pt"
                )
                
                # ì„ë² ë”© ìƒì„±
                with torch.no_grad():
                    outputs = self.kosimcse_model(**inputs)
                    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                    embeddings.extend(batch_embeddings)
            
            print("âœ… KoSimCSE ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
            return np.array(embeddings)
            
        except Exception as e:
            print(f"âš ï¸ KoSimCSE ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None
    
    def search_similar_cases(self, query, top_k=3):
        """ìœ ì‚¬í•œ ì‚¬ê³  ì‚¬ë¡€ ê²€ìƒ‰"""
        print(f"ğŸ” ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘... (ìƒìœ„ {top_k}ê°œ)")
        
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±
        query_text = ' '.join([str(v) for v in query.values() if v])
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        if self.kosimcse_model is not None:
            query_embedding = self.encode_text_with_kosimcse([query_text])
            if query_embedding is not None:
                query_embedding = query_embedding[0]
            else:
                query_embedding = None
        else:
            query_embedding = None
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for idx, case in self.case_summary.iterrows():
            similarity = 0
            
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
            if query_embedding is not None and self.text_embeddings is not None:
                case_embedding = self.text_embeddings[idx]
                similarity = cosine_similarity([query_embedding], [case_embedding])[0][0]
                similarity = max(0, similarity)
            
            # ë²”ì£¼í˜• ìœ ì‚¬ë„ ì¶”ê°€
            categorical_similarity = 0
            if 'ì‚¬ê³ ìœ í˜•ëª…' in query and 'ì‚¬ê³ ìœ í˜•ëª…' in case:
                if query['ì‚¬ê³ ìœ í˜•ëª…'] == case['ì‚¬ê³ ìœ í˜•ëª…']:
                    categorical_similarity += 0.5
            
            if 'ìˆ˜ì…êµ­' in query and 'ìˆ˜ì…êµ­' in case:
                if query['ìˆ˜ì…êµ­'] == case['ìˆ˜ì…êµ­']:
                    categorical_similarity += 0.3
            
            if 'ë³´í—˜ì¢…ëª©' in query and 'ë³´í—˜ì¢…ëª©' in case:
                if query['ë³´í—˜ì¢…ëª©'] == case['ë³´í—˜ì¢…ëª©']:
                    categorical_similarity += 0.2
            
            # ìµœì¢… ìœ ì‚¬ë„ (í…ìŠ¤íŠ¸ 70%, ë²”ì£¼í˜• 30%)
            final_similarity = 0.7 * similarity + 0.3 * categorical_similarity
            similarities.append(final_similarity)
        
        # ìƒìœ„ kê°œ ì„ íƒ
        similarities = np.array(similarities)
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for i, idx in enumerate(top_indices):
            case = self.case_summary.iloc[idx]
            similarity = similarities[idx]
            
            result = {
                'rank': i + 1,
                'similarity': float(similarity),
                'case_id': f"{case['ë³´ìƒíŒŒì¼ë²ˆí˜¸']}_{case['ì‚¬ê³ ë²ˆí˜¸']}",
                'case_info': {
                    'ì‚¬ê³ ìœ í˜•ëª…': case.get('ì‚¬ê³ ìœ í˜•ëª…', 'N/A'),
                    'ìˆ˜ì…êµ­': case.get('ìˆ˜ì…êµ­', 'N/A'),
                    'ë³´í—˜ì¢…ëª©': case.get('ë³´í—˜ì¢…ëª©', 'N/A'),
                    'ì‚¬ê³ ì„¤ëª…': case.get('ì‚¬ê³ ì„¤ëª…', 'N/A')[:100] + '...' if len(str(case.get('ì‚¬ê³ ì„¤ëª…', ''))) > 100 else case.get('ì‚¬ê³ ì„¤ëª…', 'N/A')
                },
                'predicted_results': {
                    'íŒì •êµ¬ë¶„': case.get('íŒì •êµ¬ë¶„', ['N/A'])[-1] if isinstance(case.get('íŒì •êµ¬ë¶„'), list) and case.get('íŒì •êµ¬ë¶„') else 'N/A',
                    'íŒì •ì‚¬ìœ ': case.get('íŒì •ì‚¬ìœ ', ['N/A'])[-1] if isinstance(case.get('íŒì •ì‚¬ìœ '), list) and case.get('íŒì •ì‚¬ìœ ') else 'N/A'
                }
            }
            results.append(result)
        
        return results

def sample_test():
    print("ğŸš€ ìƒ˜í”Œ ë°ì´í„°ë¡œ KoSimCSE í…ŒìŠ¤íŠ¸")
    start_time = time.time()
    
    # ìƒ˜í”Œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (1000ê°œ ë°ì´í„°)
    search_engine = SampleCaseSearch(sample_size=1000)
    
    init_time = time.time() - start_time
    print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ: {init_time:.2f}ì´ˆ")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_query = {
        'ì‚¬ê³ ìœ í˜•ëª…': 'ì§€ê¸‰ê±°ì ˆ',
        'ìˆ˜ì…êµ­': 'ë¯¸êµ­',
        'ë³´í—˜ì¢…ëª©': 'ë‹¨ê¸°ìˆ˜ì¶œë³´í—˜',
        'ì‚¬ê³ ì„¤ëª…': 'ìˆ˜ì…ìê°€ í’ˆì§ˆ ë¬¸ì œë¥¼ ì´ìœ ë¡œ ì§€ê¸‰ì„ ê±°ì ˆí•¨. ì‹ ìƒ ì—…ì²´ì˜ ì²« ìˆ˜ì¶œ ê±´ì—ì„œ ë°œìƒí•œ ë¬¸ì œì…ë‹ˆë‹¤.',
        'ìˆ˜ì¶œì': 'ì‹ ìƒ ì „ìì œí’ˆ ìˆ˜ì¶œì—…ì²´'
    }
    
    print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {test_query}")
    
    search_start = time.time()
    results = search_engine.search_similar_cases(test_query, top_k=3)
    search_time = time.time() - search_start
    
    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {search_time:.2f}ì´ˆ")
    
    # ê²°ê³¼ ì¶œë ¥
    for i, result in enumerate(results):
        print(f"\nğŸ† {i+1}ìœ„ (ìœ ì‚¬ë„: {result['similarity']:.3f})")
        print(f"   ì‚¬ê±´ ID: {result['case_id']}")
        print(f"   ì‚¬ê³ ìœ í˜•: {result['case_info']['ì‚¬ê³ ìœ í˜•ëª…']}")
        print(f"   ìˆ˜ì…êµ­: {result['case_info']['ìˆ˜ì…êµ­']}")
        print(f"   ì˜ˆì¸¡ íŒì •êµ¬ë¶„: {result['predicted_results']['íŒì •êµ¬ë¶„']}")
        print(f"   ì˜ˆì¸¡ íŒì •ì‚¬ìœ : {result['predicted_results']['íŒì •ì‚¬ìœ ']}")
    
    total_time = time.time() - start_time
    print(f"\nâœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
    print("ğŸ’¡ ì´ì œ ì „ì²´ ë°ì´í„°ë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”!")

if __name__ == "__main__":
    sample_test() 