import torch
from transformers import AutoModel, AutoTokenizer

def cal_score(a, b):
    if len(a.shape) == 1: a = a.unsqueeze(0)
    if len(b.shape) == 1: b = b.unsqueeze(0)

    a_norm = a / a.norm(dim=1)[:, None]
    b_norm = b / b.norm(dim=1)[:, None]
    return torch.mm(a_norm, b_norm.transpose(0, 1)) * 100

print("ğŸ”§ KoSimCSE ëª¨ë¸ ë¡œë”© ì¤‘...")
model = AutoModel.from_pretrained('BM-K/KoSimCSE-roberta')
tokenizer = AutoTokenizer.from_pretrained('BM-K/KoSimCSE-roberta')
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

sentences = ['ì¹˜íƒ€ê°€ ë“¤íŒì„ ê°€ë¡œ ì§ˆëŸ¬ ë¨¹ì´ë¥¼ ì«“ëŠ”ë‹¤.',
             'ì¹˜íƒ€ í•œ ë§ˆë¦¬ê°€ ë¨¹ì´ ë’¤ì—ì„œ ë‹¬ë¦¬ê³  ìˆë‹¤.',
             'ì›ìˆ­ì´ í•œ ë§ˆë¦¬ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•œë‹¤.']

print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤:")
for i, sentence in enumerate(sentences):
    print(f"  {i+1}. {sentence}")

inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")
embeddings, _ = model(**inputs, return_dict=False)

score01 = cal_score(embeddings[0][0], embeddings[1][0])
score02 = cal_score(embeddings[0][0], embeddings[2][0])

print(f"\nğŸ¯ ìœ ì‚¬ë„ ì ìˆ˜:")
print(f"  ë¬¸ì¥ 1 vs ë¬¸ì¥ 2: {score01.item():.2f}")
print(f"  ë¬¸ì¥ 1 vs ë¬¸ì¥ 3: {score02.item():.2f}")
print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 