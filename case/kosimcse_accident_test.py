import torch
import pandas as pd
import numpy as np
from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

def cal_score(a, b):
    if len(a.shape) == 1: a = a.unsqueeze(0)
    if len(b.shape) == 1: b = b.unsqueeze(0)

    a_norm = a / a.norm(dim=1)[:, None]
    b_norm = b / b.norm(dim=1)[:, None]
    return torch.mm(a_norm, b_norm.transpose(0, 1)) * 100

class KoSimCSEAccidentTest:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
        print("ğŸ“¥ KoSimCSE ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = AutoModel.from_pretrained('BM-K/KoSimCSE-roberta')
        self.tokenizer = AutoTokenizer.from_pretrained('BM-K/KoSimCSE-roberta')
        self.model.to(self.device)
        self.model.eval()
        print("âœ… KoSimCSE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        self.embedding_cache = {}
    
    def load_data(self, file_path, test_size=0.2, random_state=42):
        """ë°ì´í„° ë¡œë“œ ë° train/test ë¶„í• """
        df = pd.read_csv(file_path, encoding='cp949')
        df = df.dropna(subset=['ì‚¬ê³ ì„¤ëª…'])
        df = df.drop_duplicates()
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ ì‚¬ê³ ")
        
        train_df, test_df = train_test_split(
            df, test_size=test_size, random_state=random_state
        )
        print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ ì‚¬ê³ ")
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ ì‚¬ê³ ")
        return train_df, test_df
    
    def encode_text(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        inputs = self.tokenizer(
            [text], padding=True, truncation=True, max_length=128, return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            embeddings, _ = self.model(**inputs, return_dict=False)
            embedding = embeddings[0][0].cpu().numpy()  # KoSimCSEëŠ” ì²« ë²ˆì§¸ í† í° ì‚¬ìš©
            self.embedding_cache[text] = embedding
            return embedding
    
    def encode_batch(self, texts, batch_size=16):
        """ë°°ì¹˜ë¡œ í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = []
        uncached_texts = []
        
        for text in texts:
            if text not in self.embedding_cache:
                uncached_texts.append(text)
            else:
                embeddings.append(self.embedding_cache[text])
        
        if uncached_texts:
            print(f"    ğŸ“¥ {len(uncached_texts)}ê°œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            total_batches = (len(uncached_texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(uncached_texts), batch_size):
                batch_texts = uncached_texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts, padding=True, truncation=True, max_length=128, return_tensors="pt"
                ).to(self.device)
                
                with torch.no_grad():
                    batch_embeddings, _ = self.model(**inputs, return_dict=False)
                    batch_embeddings = batch_embeddings[0].cpu().numpy()
                    
                    for text, embedding in zip(batch_texts, batch_embeddings):
                        self.embedding_cache[text] = embedding
                        embeddings.append(embedding)
                
                current_batch = i // batch_size + 1
                if current_batch % max(1, total_batches // 10) == 0 or current_batch == total_batches:
                    progress = min((i + batch_size) / len(uncached_texts) * 100, 100)
                    print(f"    ì„ë² ë”© ì§„í–‰ë¥ : {progress:.1f}% ({min(i + batch_size, len(uncached_texts))}/{len(uncached_texts)})")
        else:
            print(f"    âœ… ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ìºì‹œë˜ì–´ ìˆì–´ì„œ ì¦‰ì‹œ ì™„ë£Œ!")
        
        return np.array(embeddings)
    
    def test_single_query(self, query, train_df, top_k=5):
        """ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ìœ ì‚¬í•œ ì‚¬ê³  ì°¾ê¸°"""
        print(f"\nğŸ” ì¿¼ë¦¬: {query}")
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.encode_text(query)
        
        # í•™ìŠµ ë°ì´í„° ì„ë² ë”©
        train_embeddings = self.encode_batch(train_df['ì‚¬ê³ ì„¤ëª…'].tolist())
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity([query_embedding], train_embeddings)[0]
        
        # ìƒìœ„ Kê°œ ê²°ê³¼
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        print(f"\nğŸ¯ ìƒìœ„ {top_k}ê°œ ìœ ì‚¬ ì‚¬ê³ :")
        for i, idx in enumerate(top_indices, 1):
            similarity = similarities[idx]
            accident_type = train_df.iloc[idx]['ì‚¬ê³ ìœ í˜•ëª…']
            description = train_df.iloc[idx]['ì‚¬ê³ ì„¤ëª…']
            print(f"\n{i}. ìœ ì‚¬ë„: {similarity:.4f}")
            print(f"   ì‚¬ê³ ìœ í˜•: {accident_type}")
            print(f"   ì‚¬ê³ ì„¤ëª…: {description}")
    
    def evaluate_performance(self, test_df, train_df, top_k=5):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ìƒìœ„ {top_k}ê°œ ê¸°ì¤€)")
        
        test_embeddings = self.encode_batch(test_df['ì‚¬ê³ ì„¤ëª…'].tolist())
        train_embeddings = self.encode_batch(train_df['ì‚¬ê³ ì„¤ëª…'].tolist())
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(test_embeddings, train_embeddings)
        
        correct_count = 0
        total_similarities = []
        
        for i, test_row in test_df.iterrows():
            test_idx = test_df.index.get_loc(i)
            test_accident_type = test_row['ì‚¬ê³ ìœ í˜•ëª…']
            
            # ìƒìœ„ Kê°œ ì¸ë±ìŠ¤
            top_indices = np.argsort(similarities[test_idx])[::-1][:top_k]
            
            # ì •í™•ë„ ê³„ì‚°
            for idx in top_indices:
                if train_df.iloc[idx]['ì‚¬ê³ ìœ í˜•ëª…'] == test_accident_type:
                    correct_count += 1
                    break
            
            # í‰ê·  ìœ ì‚¬ë„
            top_similarities = similarities[test_idx][top_indices]
            total_similarities.extend(top_similarities)
        
        accuracy = correct_count / len(test_df) * 100
        avg_similarity = np.mean(total_similarities)
        
        print(f"âœ… ì •í™•ë„: {accuracy:.2f}%")
        print(f"ğŸ“ˆ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.4f}")
        
        return accuracy, avg_similarity

def main():
    # KoSimCSE í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = KoSimCSEAccidentTest()
    
    # ë°ì´í„° ë¡œë“œ
    train_df, test_df = tester.load_data('Data/casestudy.csv')
    
    # ì„±ëŠ¥ í‰ê°€
    accuracy, avg_similarity = tester.evaluate_performance(test_df, train_df)
    
    # ê°œë³„ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” ê°œë³„ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
    test_samples = []
    for accident_type in test_df['ì‚¬ê³ ìœ í˜•ëª…'].unique():
        type_samples = test_df[test_df['ì‚¬ê³ ìœ í˜•ëª…'] == accident_type].head(1)
        test_samples.extend(type_samples['ì‚¬ê³ ì„¤ëª…'].tolist())
    
    test_queries = test_samples[:3]  # ìµœëŒ€ 3ê°œ ìƒ˜í”Œ
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n--- í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i} ---")
        tester.test_single_query(query, train_df, top_k=3)

if __name__ == "__main__":
    main() 